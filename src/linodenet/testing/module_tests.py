r"""Checks for testing certain module properties."""

__all__ = [
    # ABCs & Protocols
    "ModuleTest",
    # Functions
    "is_forward_stable",
    "is_backward_stable",
    "check_zero_mean_unit_variance",
    "get_output",
]

from collections.abc import Callable
from typing import Protocol

import torch
from torch import Tensor, nn

from linodenet.constants import ATOL, ONE, RTOL, ZERO


class ModuleTest(Protocol):
    r"""Protocol for Module Testing."""

    def __call__(
        self,
        module: nn.Module,
        /,
        *,
        rtol: float = RTOL,
        atol: float = ATOL,
    ) -> bool:
        r"""Test the module."""
        ...


def get_output(func: Callable[..., Tensor], /, *inputs: Tensor) -> Tensor:
    batch_size = inputs[0].shape[0]
    assert all(x.shape[0] == batch_size for x in inputs)

    # run the forward pass
    try:
        output = func(*inputs)
    except Exception as exc:
        raise RuntimeError(f"Error in forward pass of {func}") from exc

    # make sure the output is valid
    if not isinstance(output, Tensor):
        raise TypeError(f"Expected a tensor, but got {type(output)}")

    if output.ndim <= 1 or output.shape[0] != batch_size:
        raise ValueError(f"Expected a batched output, but got {output.shape}")

    if not output.dtype.is_floating_point:
        raise TypeError(f"Expected a floating point output, but got {output.dtype}")

    # make sure output is finite
    if not torch.all(torch.isfinite(output)):
        raise ValueError("Output has NAN and or INF values!")

    return output


@torch.no_grad()
def check_zero_mean_unit_variance(
    values: Tensor,
    /,
    *,
    batch_dim: int = 0,
    rtol: float = RTOL,
    atol: float = ATOL,
) -> bool:
    r"""Check if a tensor has zero mean and unit variance."""
    # compute mean an stdv
    output_dims = tuple(k for k in range(values.ndim) if k != batch_dim)
    mean_values = values.mean(dim=output_dims)
    stdv_values = values.std(dim=output_dims)

    # check if mean is close to 0 and stdv is close to 1 **across all runs** using RMSE
    # we use RMSE instead of just regular norm since we want to test if the value is close **on average**
    # since the convergence rate via the law of large numbers is slow (often only O(1/âˆšn)),
    # It seems fine to use a test where sample size is discounted.
    mean_rmse = (mean_values - ZERO).abs().pow(2).mean().sqrt()
    stdv_rmse = (stdv_values - ONE).abs().pow(2).mean().sqrt()
    mean_valid = mean_rmse <= (rtol * ZERO + atol)
    stdv_valid = stdv_rmse <= (rtol * ONE + atol)

    return bool(mean_valid) and bool(stdv_valid)


@torch.no_grad()
def is_forward_stable(
    func: Callable[..., Tensor],
    input_shapes: list[tuple[int, ...]],
    *,
    num_runs: int = 100,
    rtol: float = 1e-3,
    atol: float = 1e-3,
) -> bool:
    r"""Check if the forward pass is stable.

    Assumptions:
      - The module supports batching.
      - The module takes a fixed size nu
      - The module returns a single tensor.

    The test works as follows:

    1. Compute the means Î¼ and standard deviations Ïƒ of the output for a large number of random inputs.
    2. For each output, consider the distance between the input distribution ð“(0, 1) and output distribution ð“(Î¼, ÏƒÂ²).
       We measure this distance in terms of some divergence measure (e.g. KL-divergence, Wasserstein distance, etc.).
    3. We test relative closeness via the formula:

    .. math:: dist(N(0, 1), N(Î¼, ÏƒÂ²)) â‰¤ rtolâ‹…mag(N(0, 1)) + atol

    where dist is some divergence measure and mag is measure of the magnitude of the distribution.

    More specifically, we consider the entropy:

     .. math:: H(p,q) - H(p) = d(p, q) â‰¤ rtolâ‹…H(q) + atol

    In the special case when p=N(Î¼, ÏƒÂ²) and q=N(0, 1) are univariate gaussian, we have:

    .. math:: d(N(Î¼, ÏƒÂ²), N(0, 1)) â‰¤ rtolâ‹…H(N(0, 1)) + atol \\
        âŸº Â½(Î¼Â² + ÏƒÂ² - 1 - \log(ÏƒÂ²)) â‰¤ rtolâ‹…Â½(1 + \log(2Ï€)) + atol



    Recall the following facts about the information content of normal distributions:

    1. (univariate entropy) H(N(Î¼, ÏƒÂ²)) = Â½\log(2Ï€eÏƒÂ²)
    2. (univariate KL) KL(pâ‚, pâ‚‚) = Â½(Ïƒâ‚Â²/Ïƒâ‚‚Â² + (Î¼â‚ - Î¼â‚‚)Â²/Ïƒâ‚‚Â² + \log(Ïƒâ‚‚Â²/Ïƒâ‚Â²) - 1)
       - if Ïƒâ‚Â² = Ïƒâ‚‚Â², then KL(pâ‚, pâ‚‚) = Â½(Î¼â‚ - Î¼â‚‚)Â²
            -> Test A: d(p,q)<Îµ is satisfied if and only if |Î¼â‚ - Î¼â‚‚| < Îµ
            -> Test B: d(p,q)<Î² H(q) + Î± is satisfied if and only if |Î¼â‚ - Î¼â‚‚| < Î²Ìƒ\log(Ïƒ) + Î±
               If Ïƒ â†’ 0, then the test becomes more difficult, and even potentially impossible.
               If Ïƒ â†’ âˆž, then the test becomes easier.
       - if Ïƒâ‚‚>>1, then KL(pâ‚, pâ‚‚) â‰ˆ O(\log(Ïƒâ‚‚))
    3. univariate Wasserstein distance: Wâ‚‚(pâ‚, pâ‚‚)Â² = |Î¼â‚ - Î¼â‚‚|Â² + |Ïƒâ‚ - Ïƒâ‚‚|Â²

    In particular, consider the case when we have two zero-centered normal distributions N(0, Ïƒâ‚Â²) and N(0, Ïƒâ‚‚Â²).
    If we increase the standard deviation of the reference distribution,
    then the KL-divergence increases as $O(\log(Ïƒâ‚‚))$, but also the entropy increases as $O(\log(Ïƒâ‚‚))$.

    Is this true generally? (I.e. does this make the test "entropy-stable"?)
    """
    # generate random N(0,1) inputs
    inputs = [torch.randn(num_runs, *shape) for shape in input_shapes]
    output = get_output(func, *inputs)
    return check_zero_mean_unit_variance(output, rtol=rtol, atol=atol)


@torch.no_grad()
def is_backward_stable(
    func: Callable[..., Tensor],
    input_shapes: list[tuple[int, ...]],
    *,
    rtol: float = 1e-3,
    atol: float = 1e-3,
    check_params: bool = False,
    num_runs: int = 100,
) -> bool:
    r"""Check if a function is backward stable.

    In this context, a function is called backward stable, if its vector jacobian product,
    i.e. the function $vâ†¦váµ€(âˆ‚f/âˆ‚x)$ is forward stable (at a given point $x$).

    To test backward stability, we randomly sample $xâˆ¼ð“(0,1)$ and $vâˆ¼ð“(0,1)$
    with the same shape as $f(x)$. Then we call `.backward()` on the scalar value `âŸ¨v, f(x)âŸ©`.
    We then check whether `x.grad` has zero mean and unit variance.
    """
    # generate random N(0,1) inputs
    inputs = [
        torch.randn(num_runs, *shape, requires_grad=True) for shape in input_shapes
    ]

    with torch.enable_grad():
        output = get_output(func, *inputs)
        v = torch.randn_like(output)
        loss = (v * output).sum()
        loss.backward()

    passed = True

    # check input gradients
    assert all(x.grad is not None for x in inputs)
    input_grads = (x.grad for x in inputs if x.grad is not None)
    passed &= all(
        check_zero_mean_unit_variance(grad, rtol=rtol, atol=atol)
        for grad in input_grads
    )

    if check_params:
        if not isinstance(func, nn.Module):
            raise TypeError(f"Expected a module, got {type(func)}")
        param_grads = (p.grad for p in func.parameters() if p.grad is not None)
        passed &= all(
            check_zero_mean_unit_variance(grad, rtol=rtol, atol=atol)
            for grad in param_grads
        )

    return passed


@torch.no_grad()
def assert_forward_stable(
    func: Callable[..., Tensor],
    input_shapes: list[tuple[int, ...]],
    *,
    num_runs: int = 100,
    rtol: float = 1e-3,
    atol: float = 1e-3,
) -> None:
    r"""Check if the forward pass is stable.

    Raises:
        AssertionError: If the forward pass is not stable.
    """
    # generate random N(0,1) inputs
    inputs = [torch.randn(num_runs, *shape) for shape in input_shapes]
    output = get_output(func, *inputs)
    assert check_zero_mean_unit_variance(output, rtol=rtol, atol=atol)


@torch.no_grad()
def assert_backward_stable(
    func: Callable[..., Tensor],
    input_shapes: list[tuple[int, ...]],
    *,
    rtol: float = 1e-3,
    atol: float = 1e-3,
    check_params: bool = False,
    num_runs: int = 100,
) -> None:
    r"""Check if a function is backward stable.

    Raises:
        AssertionError: If the function is not backward stable.
    """
    # generate random N(0,1) inputs
    inputs = [
        torch.randn(num_runs, *shape, requires_grad=True) for shape in input_shapes
    ]

    with torch.enable_grad():
        output = get_output(func, *inputs)
        v = torch.randn_like(output)
        loss = (v * output).sum()
        loss.backward()

    # check input gradients
    assert all(x.grad is not None for x in inputs)
    input_grads = (x.grad for x in inputs if x.grad is not None)

    for grad in input_grads:
        assert check_zero_mean_unit_variance(grad, rtol=rtol, atol=atol)

    if check_params:
        if not isinstance(func, nn.Module):
            raise TypeError(f"Expected a module, got {type(func)}")
        param_grads = (p.grad for p in func.parameters() if p.grad is not None)
        for grad in param_grads:
            assert check_zero_mean_unit_variance(grad, rtol=rtol, atol=atol)
