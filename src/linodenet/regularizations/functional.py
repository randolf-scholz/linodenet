r"""Regularizations for the Linear ODE Networks.

Notes
-----
Contains regularizations in functional form.
  - See `~linodenet.regularizations.modular` for modular implementations.
"""


__all__ = [
    # Functions
    "logdetexp",
    "symmetric",
    "skew_symmetric",
    "diagonal",
    "orthogonal",
    "normal",
]

from typing import Optional

import torch.linalg
from torch import Tensor, jit

from linodenet.projections import functional as projections


@jit.script
def logdetexp(x: Tensor, p: float = 1.0) -> Tensor:
    r"""Bias $\det(e^A)$ towards 1.

    .. Signature:: ``(..., n,n) -> ...``

    By Jacobi's formula

    .. math:: \det(e^A) = e^{ð—ð—‹(A)} âŸº \log(\det(e^A)) = ð—ð—‹(A) âŸº \log(\det(A)) = ð—ð—‹(\log(A))

    In particular, we can regularize the LinODE model by adding a regularization term of the form

    .. math:: |ð—ð—‹(A)|

    Parameters
    ----------
    x: Tensor
    p: float, default=1.0

    Returns
    -------
    Tensor
    """
    traces = torch.sum(torch.diagonal(x, dim1=-1, dim2=-2), dim=-1)
    return torch.abs(traces) ** p


@jit.script
def skew_symmetric(x: Tensor, p: Optional[float] = None) -> Tensor:
    r"""Bias the matrix towards being skew-symmetric.

    .. Signature:: ``(..., n,n) -> ...``

    Parameters
    ----------
    x: Tensor
    p: Optional[float]
        If `None` uses Frobenius norm

    Returns
    -------
    Tensor
    """
    r = x - projections.skewsymmetric(x)
    if p is None:
        return torch.linalg.matrix_norm(r)
    return torch.linalg.matrix_norm(r, p)


@jit.script
def symmetric(x: Tensor, p: Optional[float] = None) -> Tensor:
    r"""Bias the matrix towards being symmetric.

    .. Signature:: ``(..., n,n) -> ...``

    Parameters
    ----------
    x: Tensor
    p: Optional[float]
        If `None` uses Frobenius norm

    Returns
    -------
    Tensor
    """
    r = x - projections.symmetric(x)
    if p is None:
        return torch.linalg.matrix_norm(r)
    return torch.linalg.matrix_norm(r, p)


@jit.script
def orthogonal(x: Tensor, p: Optional[float] = None) -> Tensor:
    r"""Bias the matrix towards being orthogonal.

    .. Signature:: ``(..., n,n) -> ...``


    Note that, given $nÃ—n$ matrix $X$ with SVD $X=Uâ‹…Î£â‹…V^âŠ¤$ holds

    .. math::
          &(1) &  â€–  X - Î Xâ€–_F &= â€–   Î£ - ð•€ â€–_F
        \\&(1) &  â€–X^ð–³ X - ð•€â€–_F &= â€–Î£^ð–³ Î£ - ð•€â€–_F
        \\&(1) &  â€–X X^ð–³ - Xâ€–_F &= â€–Î£Î£^ð–³ - ð•€â€–_F

    Parameters
    ----------
    x: Tensor
    p: Optional[float]
        If `None` uses Frobenius norm

    Returns
    -------
    Tensor
    """
    r = x - projections.orthogonal(x)
    if p is None:
        return torch.linalg.matrix_norm(r)
    return torch.linalg.matrix_norm(r, p)


@jit.script
def normal(x: Tensor, p: Optional[float] = None) -> Tensor:
    r"""Bias the matrix towards being normal.

    .. Signature:: ``(..., n,n) -> ...``

    Parameters
    ----------
    x: Tensor
    p: Optional[float]
        If `None` uses Frobenius norm

    Returns
    -------
    Tensor
    """
    r = x - projections.normal(x)
    if p is None:
        return torch.linalg.matrix_norm(r)
    return torch.linalg.matrix_norm(r, p)


@jit.script
def diagonal(x: Tensor, p: Optional[float] = None) -> Tensor:
    r"""Bias the matrix towards being diagonal.

    .. Signature:: ``(..., n,n) -> ...``

    Parameters
    ----------
    x: Tensor
    p: Optional[float]
        If `None` uses Frobenius norm

    Returns
    -------
    Tensor
    """
    r = x - projections.diagonal(x)
    if p is None:
        return torch.linalg.matrix_norm(r)
    return torch.linalg.matrix_norm(r, p)
