r"""Regularizations for the Linear ODE Networks.

Notes
-----
Contains regularizations in functional form.
  - See :mod:`~linodenet.regularizations.modular` for modular implementations.
"""


__all__ = [
    # Functions
    "logdetexp",
    "symmetric",
    "skew_symmetric",
    "diagonal",
    "orthogonal",
    "normal",
]

from typing import Optional

import torch.linalg
from torch import Tensor, jit

from linodenet.projections import functional as projections


@jit.script
def logdetexp(x: Tensor, p: float = 1.0) -> Tensor:
    r"""Bias `\det(e^A)` towards 1.

    By Jacobi's formula

    .. math::
        \det(e^A) = e^{ùóçùóã(A)} ‚ü∫ \log(\det(e^A)) = ùóçùóã(A) ‚ü∫ \log(\det(A)) = ùóçùóã(\log(A))

    In particular, we can regularize the LinODE model by adding a regularization term of the form

    .. math::
        |ùóçùóã(A)|

    **Signature:** ``(..., n,n) ‚ü∂ (...,)``

    Parameters
    ----------
    x: Tensor
    p: float, default=1.0

    Returns
    -------
    Tensor
    """
    traces = torch.sum(torch.diagonal(x, dim1=-1, dim2=-2), dim=-1)
    return torch.abs(traces) ** p


@jit.script
def skew_symmetric(x: Tensor, p: Optional[float] = None) -> Tensor:
    r"""Bias the matrix towards being skew-symmetric.

    **Signature:** ``(..., n,n) ‚ü∂ (...,)``

    Parameters
    ----------
    x: Tensor
    p: Optional[float]
        If :obj:`None` uses Frobenius norm

    Returns
    -------
    Tensor
    """
    r = x - projections.skew_symmetric(x)
    if p is None:
        return torch.linalg.matrix_norm(r)
    return torch.linalg.matrix_norm(r, p)


@jit.script
def symmetric(x: Tensor, p: Optional[float] = None) -> Tensor:
    r"""Bias the matrix towards being symmetric.

    **Signature:** ``(..., n,n) ‚ü∂ (...,)``

    Parameters
    ----------
    x: Tensor
    p: Optional[float]
        If :obj:`None` uses Frobenius norm

    Returns
    -------
    Tensor
    """
    r = x - projections.symmetric(x)
    if p is None:
        return torch.linalg.matrix_norm(r)
    return torch.linalg.matrix_norm(r, p)


@jit.script
def orthogonal(x: Tensor, p: Optional[float] = None) -> Tensor:
    r"""Bias the matrix towards being orthogonal.

    Note that, given `n√ón` matrix `X` with SVD `X=U‚ãÖŒ£‚ãÖV^ùñ≥` holds

    .. math::
          &(1) &  ‚Äñ  X - Œ†X‚Äñ_F &= ‚Äñ   Œ£ - ùïÄ ‚Äñ_F
        \\&(1) &  ‚ÄñX^ùñ≥ X - ùïÄ‚Äñ_F &= ‚ÄñŒ£^ùñ≥ Œ£ - ùïÄ‚Äñ_F
        \\&(1) &  ‚ÄñX X^ùñ≥ - X‚Äñ_F &= ‚ÄñŒ£Œ£^ùñ≥ - ùïÄ‚Äñ_F

    **Signature:** ``(..., n,n) ‚ü∂ (...,)``

    Parameters
    ----------
    x: Tensor
    p: Optional[float]
        If :obj:`None` uses Frobenius norm

    Returns
    -------
    Tensor
    """
    r = x - projections.orthogonal(x)
    if p is None:
        return torch.linalg.matrix_norm(r)
    return torch.linalg.matrix_norm(r, p)


@jit.script
def normal(x: Tensor, p: Optional[float] = None) -> Tensor:
    r"""Bias the matrix towards being normal.

    **Signature:** ``(..., n,n) ‚ü∂ (...,)``

    Parameters
    ----------
    x: Tensor
    p: Optional[float]
        If :obj:`None` uses Frobenius norm

    Returns
    -------
    Tensor
    """
    r = x - projections.normal(x)
    if p is None:
        return torch.linalg.matrix_norm(r)
    return torch.linalg.matrix_norm(r, p)


@jit.script
def diagonal(x: Tensor, p: Optional[float] = None) -> Tensor:
    r"""Bias the matrix towards being diagonal.

    **Signature:** ``(..., n,n) ‚ü∂ (...,)``

    Parameters
    ----------
    x: Tensor
    p: Optional[float]
        If :obj:`None` uses Frobenius norm

    Returns
    -------
    Tensor
    """
    r = x - projections.diagonal(x)
    if p is None:
        return torch.linalg.matrix_norm(r)
    return torch.linalg.matrix_norm(r, p)
