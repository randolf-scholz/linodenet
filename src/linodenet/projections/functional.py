r"""Projections for the Linear ODE Networks.

Notes:
    Contains projections in functional form.
      - See `~linodenet.projections.modular` for modular implementations.
"""

__all__ = [
    # ABCs & Protocols
    "Projection",
    # Projections
    "banded",
    "contraction",
    "diagonal",
    "diagonally_dominant",
    "hamiltonian",
    "identity",
    "low_rank",
    "lower_triangular",
    "masked",
    "normal",
    "orthogonal",
    "skew_symmetric",
    "symmetric",
    "symplectic",
    "traceless",
    "upper_triangular",
]

import torch
from torch import BoolTensor, Tensor
from typing_extensions import Protocol, runtime_checkable

from linodenet.constants import TRUE


@runtime_checkable
class Projection(Protocol):
    r"""Protocol for Projection Components."""

    def __call__(self, x: Tensor, /) -> Tensor:
        r"""Forward pass of the projection.

        .. Signature: ``(..., d) -> (..., f)``.
        """
        ...


# region projections -------------------------------------------------------------------
# region matrix groups -----------------------------------------------------------------
def identity(x: Tensor) -> Tensor:
    r"""Return x as-is.

    .. Signature:: ``... -> ...``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2
    """
    return x


def symmetric(x: Tensor) -> Tensor:
    r"""Return the closest symmetric matrix to X.

    .. Signature:: ``(..., n, n) -> (..., n, n)``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2   s.t.   Y^âŠ¤ = Y

    One can show analytically that Y = Â½(X + X^âŠ¤) is the unique minimizer.
    """
    return (x + x.swapaxes(-1, -2)) / 2


def skew_symmetric(x: Tensor) -> Tensor:
    r"""Return the closest skew-symmetric matrix to X.

    .. Signature:: ``(..., n, n) -> (..., n, n)``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2   s.t.   Y^âŠ¤ = -Y

    One can show analytically that Y = Â½(X - X^âŠ¤) is the unique minimizer.
    """
    return (x - x.swapaxes(-1, -2)) / 2


def low_rank(x: Tensor, rank: int = 1) -> Tensor:
    r"""Return the closest low rank matrix to X.

    .. Signature:: ``(..., m, n) -> (..., m, n)``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2   s.t.   rank(Y) â‰¤ k

    One can show analytically that Y = Uâ‚–Î£â‚–Vâ‚–^ð–³ is the unique minimizer,
    where X=UÎ£V^ð–³ is the SVD of X.
    """
    U, S, Vh = torch.linalg.svd(x, full_matrices=False)
    return torch.einsum(
        "...ij, ...j, ...jk -> ...ik",
        U[..., :, :rank],
        S[..., :rank],
        Vh[..., :rank, :],
    )


def orthogonal(x: Tensor) -> Tensor:
    r"""Return the closest orthogonal matrix to X.

    .. Signature:: ``(..., n, n) -> (..., n, n)``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2   s.t.   Y^ð–³ Y = ð•€ = YY^ð–³

    One can show analytically that $Y = UV^ð–³$ is the unique minimizer,
    where $X=UÎ£V^ð–³$ is the SVD of $X$.

    References:
        https://math.stackexchange.com/q/2215359
    """
    U, _, Vh = torch.linalg.svd(x)
    return torch.einsum("...ij, ...jk -> ...ik", U, Vh)


def traceless(x: Tensor) -> Tensor:
    r"""Return the closest traceless matrix to X.

    .. Signature:: ``(..., n, n) -> (..., n, n)``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2   s.t.   tr(Y) = 0

    One can show analytically that Y = X - (1/n)tr(X)ð•€â‚™ is the unique minimizer.

    Note:
        Traceless matrices are also called *trace-free* or *trace-zero* matrices.
        They have the important property that $\det(\exp(X)) = 1$,
        which follows from the fact that $\det(\exp(X)) = \exp(\r(X))$.
    """
    n = x.shape[-1]
    trace = x.diagonal(dim1=-1, dim2=-2).sum(dim=-1)
    eye = torch.eye(n, dtype=x.dtype, device=x.device)
    return x - torch.einsum("..., mn -> ...mn", trace / n, eye)


def normal(x: Tensor) -> Tensor:
    r"""Return the closest normal matrix to X.

    .. Signature:: ``(..., n, n) -> (..., n, n)``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2   s.t.   Y^âŠ¤Y = YY^âŠ¤

    **The Lagrangian:**

    .. math:: â„’(Y, Î›) = Â½âˆ¥X-Yâˆ¥_F^2 + âŸ¨Î›, [Y, Y^âŠ¤]âŸ©

    **First order necessary KKT condition:**

    .. math::
            0 &= âˆ‡â„’(Y, Î›) = (Y-X) + Y(Î› + Î›^âŠ¤) - (Î› + Î›^âŠ¤)Y
        \\âŸº Y &= X + [Y, Î›]

    **Second order sufficient KKT condition:**

    .. math::
             âŸ¨âˆ‡h|SâŸ©=0     &âŸ¹ âŸ¨S|âˆ‡Â²â„’|SâŸ© â‰¥ 0
         \\âŸº âŸ¨[Y, Î›]|SâŸ©=0 &âŸ¹ âŸ¨S|ð•€âŠ—ð•€ + Î›âŠ—ð•€ âˆ’ ð•€âŠ—Î›|SâŸ© â‰¥ 0
         \\âŸº âŸ¨[Y, Î›]|SâŸ©=0 &âŸ¹ âŸ¨S|SâŸ© + âŸ¨[S, Î›]|SâŸ© â‰¥ 0
    """
    raise NotImplementedError("TODO: implement Fixpoint / Gradient based algorithm.")


def symplectic(x: Tensor) -> Tensor:
    r"""Return the closest symplectic matrix to X.

    .. Signature:: ``(..., 2n, 2n) -> (..., 2n, 2n)``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2   s.t.   Y^ð–³ J Y = J   where   J=[ðŸŽ, ð•€; -ð•€, ðŸŽ]

    Alternatively, the above is equivalent to

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2   s.t.   Y^ð–³ J Y = J   where   J= ð”»â‚Šâ‚-ð”»â‚‹â‚

    where $ð”»â‚–$ is the $2nÃ—2n$ matrix with ones on the k-th diagonal.
    """
    raise NotImplementedError("TODO: implement Fixpoint / Gradient based algorithm.")


def hamiltonian(x: Tensor) -> Tensor:
    r"""Return the closest hamiltonian matrix to X.

    .. Signature:: ``(..., 2n, 2n) -> (..., 2n, 2n)``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2   s.t.   (JY)^T = JA   where   J=[ðŸŽ, ð•€; -ð•€, ðŸŽ]

    Alternatively, the above is equivalent to

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2   s.t.   Y^ð–³ J Y = J   where   J= ð”»â‚Šâ‚-ð”»â‚‹â‚

    where $ð”»â‚–$ is the $2nÃ—2n$ matrix with ones on the k-th diagonal.

    Note:
        The Hamiltonian matrices are the skew-symmetric matrices
        with respect to the symplectic inner product.
        - The matrix exponential of a Hamiltonian matrix is symplectic.
    """
    raise NotImplementedError("TODO: implement Fixpoint / Gradient based algorithm.")


# endregion matrix groups --------------------------------------------------------------


# region masked projections ------------------------------------------------------------
def masked(x: Tensor, mask: BoolTensor = TRUE) -> Tensor:
    r"""Return the closest banded matrix to X.

    .. Signature:: ``(..., m, n) -> (..., m, n)``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2   s.t.   ð•„âŠ™Y = Y

    One can show analytically that the unique smallest norm minimizer is $Y = ð•„âŠ™X$.

    See Also:
        - `projections.masked`
        - `projections.diagonal`
        - `projections.lower_triangular`
        - `projections.upper_triangular`
        - `projections.banded`
    """
    zero = torch.tensor(0.0, dtype=x.dtype, device=x.device)
    mask_ = torch.as_tensor(mask, dtype=torch.bool, device=x.device)
    return torch.where(mask_, x, zero)


def diagonal(x: Tensor) -> Tensor:
    r"""Return the closest diagonal matrix to X.

    .. Signature:: ``(..., m, n) -> (..., m, n)``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2   s.t.   ð•€âŠ™Y = Y

    One can show analytically that the unique smallest norm minimizer is $Y = ð•€âŠ™X$.

    See Also:
        - `projections.masked`
        - `projections.diagonal`
        - `projections.lower_triangular`
        - `projections.upper_triangular`
        - `projections.banded`
    """
    eye = torch.eye(x.shape[-2], x.shape[-1], dtype=torch.bool, device=x.device)
    zero = torch.tensor(0.0, dtype=x.dtype, device=x.device)
    return torch.where(eye, x, zero)


def upper_triangular(x: Tensor, upper: int = 0) -> Tensor:
    r"""Return the closest upper triangular matrix to X.

    .. Signature:: ``(..., m, n) -> (..., m, n)``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2   s.t.   ð•ŒâŠ™Y = Y

    One can show analytically that the unique smallest norm minimizer is $Y = ð•ŒâŠ™X$.

    See Also:
        - `projections.masked`
        - `projections.diagonal`
        - `projections.lower_triangular`
        - `projections.upper_triangular`
        - `projections.banded`
    """
    return x.triu(diagonal=upper)


def lower_triangular(x: Tensor, lower: int = 0) -> Tensor:
    r"""Return the closest lower triangular matrix to X.

    .. Signature:: ``(..., m, n) -> (..., m, n)``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2   s.t.   ð•ƒâŠ™Y = Y

    One can show analytically that the unique smallest norm minimizer is $Y = ð•ƒâŠ™X$.

    See Also:
        - `projections.masked`
        - `projections.diagonal`
        - `projections.lower_triangular`
        - `projections.upper_triangular`
        - `projections.banded`
    """
    return x.tril(diagonal=lower)


def banded(x: Tensor, upper: int = 0, lower: int = 0) -> Tensor:
    r"""Return the closest banded matrix to X.

    .. Signature:: ``(..., m, n) -> (..., m, n)``

    .. math:: \min_Y Â½âˆ¥X-Yâˆ¥_F^2   s.t.   ð”¹âŠ™Y = Y

    One can show analytically that the unique smallest norm minimizer is $Y = ð”¹âŠ™X$.

    See Also:
        - `projections.masked`
        - `projections.diagonal`
        - `projections.lower_triangular`
        - `projections.upper_triangular`
        - `projections.banded`
    """
    x = x.triu(diagonal=upper)
    x = x.tril(diagonal=lower)
    return x


# endregion masked projections ---------------------------------------------------------


# region other projections -------------------------------------------------------------
def contraction(x: Tensor) -> Tensor:
    r"""Return the closest contraction matrix to X.

    .. Signature:: ``(..., m, n) -> (..., m, n)``

    .. math:: \min_Y âˆ¥X-Yâˆ¥â‚‚  s.t. â€–Yâ€–â‚‚ â‰¤ 1

    One can show analytically that the unique smallest norm minimizer is
    $Y = \min(1, Ïƒâ»Â¹) X$, where $Ïƒ = â€–Xâ€–â‚‚$ is the spectral norm of $X$.

    Proof:
        Apply SVD: $X = UÎ£V^ð–³$, then, the problem is equivalent to minimizing
        $âˆ¥UÎ£V^ð–³ - Yâ€–â‚‚ = âˆ¥Î£ - U^ð–³ Y Vâ€–â‚‚ = âˆ¥Î£ - Zâ€–â‚‚$ subject to $â€–Zâ€–â‚‚ â‰¤ 1$.
        Since $Î£$ is diagonal, one can show the problem is equivalent to minimizing
        $âˆ¥ð›” - ð³â€–â‚‚$ subject to $â€–ð³â€–_âˆž â‰¤ 1$, where $ð³ = \text{diag}(Z)$.
        Which is solved by $ð³ = \min(1, Ïƒâ‚â»Â¹) ð›”$.
    """
    sigma = torch.linalg.matrix_norm(x, ord=2, dim=(-2, -1))
    return x / sigma.clamp_min(1.0)


def diagonally_dominant(x: Tensor) -> Tensor:
    r"""Return the closest diagonally dominant matrix to X.

    .. Signature:: ``(..., n, n) -> (..., n, n)``

    .. math:: \min_Y âˆ¥X-Yâˆ¥_F  s.t. |Y_{ii}| â‰¥ âˆ‘_{jâ‰ i} |Y_{ij}| for all i = 1, â€¦, n

    References:
        Computing the nearest diagonally dominant matrix (Mendoza et al. 1998)
    """
    raise NotImplementedError


# endregion other projections ----------------------------------------------------------
# endregion projections ----------------------------------------------------------------
