{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T22:27:52.039990Z",
     "iopub.status.busy": "2022-09-18T22:27:52.039513Z",
     "iopub.status.idle": "2022-09-18T22:27:52.042776Z",
     "shell.execute_reply": "2022-09-18T22:27:52.042394Z",
     "shell.execute_reply.started": "2022-09-18T22:27:52.039975Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from contextlib import AbstractContextManager, ContextDecorator, contextmanager\n",
    "from math import sqrt\n",
    "from time import perf_counter\n",
    "\n",
    "import torch\n",
    "import torch.nn.utils.parametrize as P\n",
    "from torch import Tensor, jit, nn\n",
    "from torch.nn.utils.parametrize import register_parametrization\n",
    "from torch.optim import SGD, AdamW\n",
    "from tqdm.autonotebook import trange\n",
    "\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "USE_CACHED: bool = True\n",
    "batch, seq_len, k, l = 128, 512, 256, 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How and when to refresh the buffer?\n",
    "\n",
    "- ‚üπ When accessing the buffer it needs to be up to data\n",
    "    - The buffer changes whenever the parameters are updated.\n",
    "    \n",
    "- Either: Try to add context manager inside the outermost module\n",
    "    - Pro: keeps outside of model code clean\n",
    "    - Con: Inefficient when model is wrapped in another model.\n",
    "- Or: outside the model altogether\n",
    "\n",
    "```python\n",
    "with cache_parametrizations(model):  <- easier to understand!\n",
    "    # <- recompute buffers on __enter__?\n",
    "    y = model(x)\n",
    "    ...\n",
    "    \n",
    "    \n",
    "    # <- recompute buffers on __exit__?\n",
    "```\n",
    "\n",
    "\n",
    "```python    \n",
    "model.refresh_buffers()\n",
    "y = model(x)\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Idea:\n",
    "\n",
    "- Parametrizations should be cached in forward pass\n",
    "- An outer module might call an inner module many times\n",
    "    - The outer module itself might be nested in another module where it is called many times\n",
    "    - Etc. (recursion)\n",
    "- \n",
    " \n",
    "Questions:\n",
    "    How to write it without having to add wrapper code at all levels?\n",
    "    => Just have wrapper code on the outside!\n",
    "    => Use contextmanager that\n",
    "        - on `__enter__` causes module to recompute parametrization\n",
    "        - on `__exit__` causes module to recompute parametrization\n",
    "    But really, we only ought to recompute parametrizations if the model parameters change.\n",
    "    - Perform recomputation on `backward()` does not work, since after `optimizer.step` the values are incorrect.\n",
    "    - Perform recomputation on `forward()` does not work, if model parameters are used otherwise, e.g. for logging\n",
    "    \n",
    "What happens when we ask the module to recompute parametrizations?\n",
    "Options:\n",
    "\n",
    "- Add parametrizations as properties\n",
    "    - Pro: Can have complex code\n",
    "    - Con: @property not jit supported\n",
    "- Add parametrizations as buffers\n",
    "    - Pro: Built-in concept\n",
    "    - Con: Where to add the recomputation code?\n",
    "        - We have to add it manually unfortunately?\n",
    "        - Or attach another module.\n",
    "- Add custom \"register_parametrization\" code.\n",
    "    - Pro: DIY, all freedoms\n",
    "    - Con: Hard to support generically.\n",
    "- Use built-in parametrization:\n",
    "    - Does not work with JIT!\n",
    "\n",
    "\n",
    "```python\n",
    "class Foo(nn.Module):\n",
    "    def __init__(self, k: int, l: int) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.normal(0, 1 / sqrt(k), (k, l)))\n",
    "        self.activation = nn.Tanh()\n",
    "        self.kernel = register_parametrization(\")   \n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = torch.einsum(\"...k, kl -> ...l\", x, self.kernel)\n",
    "        return self.activation(x)  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T23:40:31.694388Z",
     "iopub.status.busy": "2022-09-18T23:40:31.694028Z",
     "iopub.status.idle": "2022-09-18T23:40:31.697770Z",
     "shell.execute_reply": "2022-09-18T23:40:31.697454Z",
     "shell.execute_reply.started": "2022-09-18T23:40:31.694369Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model._buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T22:27:54.648830Z",
     "iopub.status.busy": "2022-09-18T22:27:54.648492Z",
     "iopub.status.idle": "2022-09-18T22:27:54.650802Z",
     "shell.execute_reply": "2022-09-18T22:27:54.650469Z",
     "shell.execute_reply.started": "2022-09-18T22:27:54.648817Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def cache_parametrizations(model: nn.Module):\n",
    "\n",
    "    # _buffers\n",
    "\n",
    "    persistent_buffers = {\n",
    "        k: v\n",
    "        for k, v in self._buffers.items()\n",
    "        if k not in self._non_persistent_buffers_set\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T22:34:38.962823Z",
     "iopub.status.busy": "2022-09-18T22:34:38.962432Z",
     "iopub.status.idle": "2022-09-18T22:34:38.971008Z",
     "shell.execute_reply": "2022-09-18T22:34:38.970645Z",
     "shell.execute_reply.started": "2022-09-18T22:34:38.962809Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T19:56:35.337138Z",
     "iopub.status.busy": "2022-09-18T19:56:35.336828Z",
     "iopub.status.idle": "2022-09-18T19:56:35.339791Z",
     "shell.execute_reply": "2022-09-18T19:56:35.339334Z",
     "shell.execute_reply.started": "2022-09-18T19:56:35.337125Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def register_parametrization(obj: nn.Module, name: str, func: callable):\n",
    "    kernel = func()\n",
    "    obj.register_buffer(name, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T22:18:33.339878Z",
     "iopub.status.busy": "2022-09-18T22:18:33.339568Z",
     "iopub.status.idle": "2022-09-18T22:18:33.343099Z",
     "shell.execute_reply": "2022-09-18T22:18:33.342763Z",
     "shell.execute_reply.started": "2022-09-18T22:18:33.339864Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ScalarParam(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c = nn.Parameter(torch.randn(()))\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, A: Tensor) -> Tensor:\n",
    "        return A * self.c\n",
    "\n",
    "\n",
    "class MatrixParam(nn.Module):\n",
    "    def __init__(self, k: int, l: int):\n",
    "        super().__init__()\n",
    "        self.C = nn.Parameter(torch.normal(0, 1 / sqrt(k), (k, l)))\n",
    "\n",
    "    def forward(self, A: Tensor) -> Tensor:\n",
    "        return A * self.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T22:19:17.517578Z",
     "iopub.status.busy": "2022-09-18T22:19:17.517219Z",
     "iopub.status.idle": "2022-09-18T22:19:17.520696Z",
     "shell.execute_reply": "2022-09-18T22:19:17.520419Z",
     "shell.execute_reply.started": "2022-09-18T22:19:17.517566Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomCache(nn.Module):\n",
    "    \"\"\"Dense layer with custom parametrization code.\"\"\"\n",
    "\n",
    "    def __init__(self, k: int, l: int) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.normal(0, 1 / sqrt(k), (k, l)))\n",
    "        self.activation = nn.Tanh()\n",
    "        self.kernel_parametrization = MatrixParam(k, l)\n",
    "        self.register_buffer(\"kernel\", self._compute_kernel())\n",
    "        self._parametrizations = [\"kernel\"]\n",
    "        # register_parametrization(self, \"kernel\", self.kernel_parametrization, self.weight)\n",
    "\n",
    "    def _compute_kernel(self) -> None:\n",
    "        return self.kernel_parametrization(self.weight)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = torch.einsum(\"...k, kl -> ...l\", x, self.weight)\n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T22:36:49.374654Z",
     "iopub.status.busy": "2022-09-18T22:36:49.374369Z",
     "iopub.status.idle": "2022-09-18T22:36:49.387115Z",
     "shell.execute_reply": "2022-09-18T22:36:49.386775Z",
     "shell.execute_reply.started": "2022-09-18T22:36:49.374642Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = CustomCache(5, 6)\n",
    "scripted = jit.script(model)\n",
    "jit.save(scripted, \"model.pt\")\n",
    "loaded = jit.load(\"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T23:41:10.909241Z",
     "iopub.status.busy": "2022-09-18T23:41:10.908960Z",
     "iopub.status.idle": "2022-09-18T23:41:10.912438Z",
     "shell.execute_reply": "2022-09-18T23:41:10.912048Z",
     "shell.execute_reply.started": "2022-09-18T23:41:10.909228Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict(loaded._buffers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T22:35:09.224515Z",
     "iopub.status.busy": "2022-09-18T22:35:09.224160Z",
     "iopub.status.idle": "2022-09-18T22:35:09.227548Z",
     "shell.execute_reply": "2022-09-18T22:35:09.227285Z",
     "shell.execute_reply.started": "2022-09-18T22:35:09.224502Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T16:14:05.153848Z",
     "iopub.status.busy": "2022-09-18T16:14:05.153698Z",
     "iopub.status.idle": "2022-09-18T16:14:05.156330Z",
     "shell.execute_reply": "2022-09-18T16:14:05.156025Z",
     "shell.execute_reply.started": "2022-09-18T16:14:05.153837Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @contextmanager\n",
    "class refresh_cache(AbstractContextManager):\n",
    "    def __enter__(self):\n",
    "        global REFRESH_CACHED\n",
    "        REFRESH_CACHED = True\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        global REFRESH_CACHED\n",
    "        REFRESH_CACHED = True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T16:14:05.156838Z",
     "iopub.status.busy": "2022-09-18T16:14:05.156730Z",
     "iopub.status.idle": "2022-09-18T16:14:05.159797Z",
     "shell.execute_reply": "2022-09-18T16:14:05.159487Z",
     "shell.execute_reply.started": "2022-09-18T16:14:05.156828Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Iterate(nn.Module):\n",
    "    def __init__(self, mod: nn.Module, niter: int = 100) -> None:\n",
    "        super().__init__()\n",
    "        self.mod = mod\n",
    "        self.niter = niter\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        for k in range(self.niter):\n",
    "            x = self.mod(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T16:14:05.167381Z",
     "iopub.status.busy": "2022-09-18T16:14:05.167273Z",
     "iopub.status.idle": "2022-09-18T16:14:05.169934Z",
     "shell.execute_reply": "2022-09-18T16:14:05.169639Z",
     "shell.execute_reply.started": "2022-09-18T16:14:05.167370Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, k: int, l: int) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.normal(0, 1 / sqrt(k), (k, l)))\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = torch.einsum(\"...k, kl -> ...l\", x, self.weight)\n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T16:14:05.325369Z",
     "iopub.status.busy": "2022-09-18T16:14:05.324992Z",
     "iopub.status.idle": "2022-09-18T16:14:05.328298Z",
     "shell.execute_reply": "2022-09-18T16:14:05.327993Z",
     "shell.execute_reply.started": "2022-09-18T16:14:05.325357Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearParam(nn.Module):\n",
    "    def __init__(self, k: int, l: int) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.normal(0, 1, (k, l)))\n",
    "        self.param = MatrixParam(k, l)\n",
    "        self.register_buffer(\"kernel\", self.param(self.weight), persistent=True)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.register_buffer(\"_refresh_cached\", torch.tensor(True), persistent=True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        self.kernel = self.param(self.weight)\n",
    "        x = torch.einsum(\"...k, kl -> ...l\", x, self.kernel)\n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T16:14:05.599783Z",
     "iopub.status.busy": "2022-09-18T16:14:05.599391Z",
     "iopub.status.idle": "2022-09-18T16:14:05.602706Z",
     "shell.execute_reply": "2022-09-18T16:14:05.602401Z",
     "shell.execute_reply.started": "2022-09-18T16:14:05.599770Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearGlobal(nn.Module):\n",
    "    def __init__(self, k: int, l: int) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.normal(0, 1, (k, l)))\n",
    "        self.param = MatrixParam(k, l)\n",
    "        kernel = self.param(self.weight)\n",
    "        self.register_buffer(\"kernel\", kernel, persistent=True)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        global REFRESH_CACHED\n",
    "        if REFRESH_CACHED:\n",
    "            self.kernel = self.param(self.weight)\n",
    "            REFRESH_CACHED = False\n",
    "        x = torch.einsum(\"...k, kl -> ...l\", x, self.kernel)\n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T16:14:07.548979Z",
     "iopub.status.busy": "2022-09-18T16:14:07.548792Z",
     "iopub.status.idle": "2022-09-18T16:14:07.553398Z",
     "shell.execute_reply": "2022-09-18T16:14:07.553081Z",
     "shell.execute_reply.started": "2022-09-18T16:14:07.548964Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearCached(nn.Module):\n",
    "    def __init__(self, k: int, l: int) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.normal(0, 1, (k, k)))\n",
    "        self.param = MatrixParam(k, l)\n",
    "        self.activation = nn.Tanh()\n",
    "        # with torch.no_grad():\n",
    "        kernel = self.param(self.weight)\n",
    "        self.register_buffer(\"kernel\", kernel, persistent=True)\n",
    "        # self.use_cached = nn.Parameter(torch.tensor(False), requires_grad=False)\n",
    "        self.register_buffer(\"_refresh_cached\", torch.tensor(True), persistent=False)\n",
    "\n",
    "        # self._refresh_cached = False\n",
    "\n",
    "        def hook(module, grad_input, grad_output) -> None:\n",
    "            # print(\"RAN HOOK\")\n",
    "            # self._refresh_cached = torch.tensor(False)\n",
    "            self._refresh_cached = ~(self._refresh_cached ^ self._refresh_cached)\n",
    "\n",
    "        self.register_full_backward_hook(hook)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self._refresh_cached:\n",
    "            self.kernel = self.param(self.weight)\n",
    "            self._refresh_cached = ~self._refresh_cached  # set to False\n",
    "        x = torch.einsum(\"...k, kl -> ...l\", x, self.kernel)\n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T16:14:07.936414Z",
     "iopub.status.busy": "2022-09-18T16:14:07.936282Z",
     "iopub.status.idle": "2022-09-18T16:14:08.609738Z",
     "shell.execute_reply": "2022-09-18T16:14:08.609360Z",
     "shell.execute_reply.started": "2022-09-18T16:14:07.936401Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.randn(batch, seq_len, k, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T16:14:08.952422Z",
     "iopub.status.busy": "2022-09-18T16:14:08.952261Z",
     "iopub.status.idle": "2022-09-18T16:14:08.957438Z",
     "shell.execute_reply": "2022-09-18T16:14:08.957101Z",
     "shell.execute_reply.started": "2022-09-18T16:14:08.952409Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Iterate(LinearParam(k, k), niter=100)\n",
    "model = model.to(device=DEVICE)\n",
    "optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "REFRESH_CACHED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T16:22:33.584868Z",
     "iopub.status.busy": "2022-09-18T16:22:33.584494Z",
     "iopub.status.idle": "2022-09-18T16:22:33.587768Z",
     "shell.execute_reply": "2022-09-18T16:22:33.587378Z",
     "shell.execute_reply.started": "2022-09-18T16:22:33.584855Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(loaded.modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T16:15:29.823459Z",
     "iopub.status.busy": "2022-09-18T16:15:29.823244Z",
     "iopub.status.idle": "2022-09-18T16:15:29.835659Z",
     "shell.execute_reply": "2022-09-18T16:15:29.835365Z",
     "shell.execute_reply.started": "2022-09-18T16:15:29.823445Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "jitted = jit.script(model)\n",
    "jit.save(jitted, \"model.pt\")\n",
    "loaded = jit.load(\"model.pt\")\n",
    "dir(loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T11:45:04.932770Z",
     "iopub.status.busy": "2022-09-18T11:45:04.932290Z",
     "iopub.status.idle": "2022-09-18T11:45:07.089295Z",
     "shell.execute_reply": "2022-09-18T11:45:07.088928Z",
     "shell.execute_reply.started": "2022-09-18T11:45:04.932756Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for _ in (pbar := trange(12)):\n",
    "    with refresh_cache():\n",
    "        t = perf_counter()\n",
    "        # print(model.mod[0]._refresh_cached)\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        # print(model.mod[0]._refresh_cached)\n",
    "        y = model(x)\n",
    "        # print(model.mod[0]._refresh_cached)\n",
    "        r = y.norm()\n",
    "        print(r)\n",
    "        # print(model.mod[0]._refresh_cached)\n",
    "        r.backward()\n",
    "        # print(model.mod.weight)\n",
    "        # print(model.mod.weight.grad)\n",
    "        optimizer.step()\n",
    "        torch.cuda.synchronize()\n",
    "        # print(model.mod[0]._refresh_cached)\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        t = perf_counter() - t\n",
    "        pbar.set_postfix(t=f\"{int(t*1000)}ms\")\n",
    "        # print(model.mod[0]._refresh_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T11:44:46.313723Z",
     "iopub.status.busy": "2022-09-18T11:44:46.313617Z",
     "iopub.status.idle": "2022-09-18T11:44:46.802206Z",
     "shell.execute_reply": "2022-09-18T11:44:46.801810Z",
     "shell.execute_reply.started": "2022-09-18T11:44:46.313712Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-18T11:44:46.802576Z",
     "iopub.status.idle": "2022-09-18T11:44:46.802722Z",
     "shell.execute_reply": "2022-09-18T11:44:46.802656Z",
     "shell.execute_reply.started": "2022-09-18T11:44:46.802648Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Iterate(nn.Sequential(LinearParam(k, k), nn.Tanh()))\n",
    "model = model.to(device=DEVICE)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-18T11:44:46.803147Z",
     "iopub.status.idle": "2022-09-18T11:44:46.803285Z",
     "shell.execute_reply": "2022-09-18T11:44:46.803220Z",
     "shell.execute_reply.started": "2022-09-18T11:44:46.803213Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.zero_grad(set_to_none=True)\n",
    "y = model(x)\n",
    "r = y.norm()\n",
    "r.backward()\n",
    "model.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-18T11:44:46.803749Z",
     "iopub.status.idle": "2022-09-18T11:44:46.803919Z",
     "shell.execute_reply": "2022-09-18T11:44:46.803856Z",
     "shell.execute_reply.started": "2022-09-18T11:44:46.803849Z"
    }
   },
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-18T11:44:46.804424Z",
     "iopub.status.idle": "2022-09-18T11:44:46.804561Z",
     "shell.execute_reply": "2022-09-18T11:44:46.804498Z",
     "shell.execute_reply.started": "2022-09-18T11:44:46.804491Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Iterate(nn.Sequential(Linear(k, k), nn.Tanh()))\n",
    "param = MatrixParam(k, l)\n",
    "P.register_parametrization(model.mod[0], \"weight\", param)\n",
    "model = model.to(device=DEVICE)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-18T11:44:46.804956Z",
     "iopub.status.idle": "2022-09-18T11:44:46.805091Z",
     "shell.execute_reply": "2022-09-18T11:44:46.805027Z",
     "shell.execute_reply.started": "2022-09-18T11:44:46.805021Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with P.cached():\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    y = model(x)\n",
    "    r = y.norm()\n",
    "    r.backward()\n",
    "    model.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILTIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-18T11:44:46.805700Z",
     "iopub.status.idle": "2022-09-18T11:44:46.805896Z",
     "shell.execute_reply": "2022-09-18T11:44:46.805833Z",
     "shell.execute_reply.started": "2022-09-18T11:44:46.805826Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from contextlib import contextmanager\n",
    "from typing import Dict, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.modules.container import Module, ModuleDict, ModuleList\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "_cache_enabled = 0\n",
    "_cache: Dict[Tuple[int, str], Optional[Tensor]] = {}\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def cached():\n",
    "    r\"\"\"Context manager that enables the caching system within parametrizations\n",
    "    registered with :func:`register_parametrization`.\n",
    "\n",
    "    The value of the parametrized objects is computed and cached the first time\n",
    "    they are required when this context manager is active. The cached values are\n",
    "    discarded when leaving the context manager.\n",
    "\n",
    "    This is useful when using a parametrized parameter more than once in the forward pass.\n",
    "    An example of this is when parametrizing the recurrent kernel of an RNN or when\n",
    "    sharing weights.\n",
    "\n",
    "    The simplest way to activate the cache is by wrapping the forward pass of the neural network\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "        import torch.nn.utils.parametrize as P\n",
    "        ...\n",
    "        with P.cached():\n",
    "            output = model(inputs)\n",
    "\n",
    "    in training and evaluation. One may also wrap the parts of the modules that use\n",
    "    several times the parametrized tensors. For example, the loop of an RNN with a\n",
    "    parametrized recurrent kernel:\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "        with P.cached():\n",
    "            for x in xs:\n",
    "                out_rnn = self.rnn_cell(x, out_rnn)\n",
    "    \"\"\"\n",
    "    global _cache\n",
    "    global _cache_enabled\n",
    "    _cache_enabled += 1\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        _cache_enabled -= 1\n",
    "        if not _cache_enabled:\n",
    "            _cache = {}\n",
    "\n",
    "\n",
    "def _register_parameter_or_buffer(module, name, X):\n",
    "    if isinstance(X, Parameter):\n",
    "        module.register_parameter(name, X)\n",
    "    else:\n",
    "        module.register_buffer(name, X)\n",
    "\n",
    "\n",
    "class ParametrizationList(ModuleList):\n",
    "    r\"\"\"A sequential container that holds and manages the ``original`` or ``original0``, ``original1``, ...\n",
    "    parameters or buffers of a parametrized :class:`torch.nn.Module`.\n",
    "\n",
    "    It is the type of ``module.parametrizations[tensor_name]`` when ``module[tensor_name]``\n",
    "    has been parametrized with :func:`register_parametrization`.\n",
    "\n",
    "    If the first registered parmetrization has a ``right_inverse`` that returns one tensor or\n",
    "    does not have a ``right_inverse`` (in which case we assume that ``right_inverse`` is the identity),\n",
    "    it will hold the tensor under the name ``original``.\n",
    "    If it has a ``right_inverse`` that returns more than one tensor, these will be registered as\n",
    "    ``original0``, ``original1``, ...\n",
    "\n",
    "    .. warning::\n",
    "        This class is used internally by :func:`register_parametrization`. It is documented\n",
    "        here for completeness. It shall not be instantiated by the user.\n",
    "\n",
    "    Args:\n",
    "        modules (sequence): sequence of modules representing the parametrizations\n",
    "        original (Parameter or Tensor): parameter or buffer that is parametrized\n",
    "        unsafe (bool): a boolean flag that denotes whether the parametrization\n",
    "            may change the dtype and shape of the tensor. Default: `False`\n",
    "            Warning: the parametrization is not checked for consistency upon registration.\n",
    "            Enable this flag at your own risk.\n",
    "    \"\"\"\n",
    "    original: Tensor\n",
    "    unsafe: bool\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        modules: Sequence[Module],\n",
    "        original: Union[Tensor, Parameter],\n",
    "        unsafe: bool = False,\n",
    "    ) -> None:\n",
    "        # We require this because we need to treat differently the first parametrization\n",
    "        # This should never throw, unless this class is used from the outside\n",
    "        if len(modules) == 0:\n",
    "            raise ValueError(\"ParametrizationList requires one or more modules.\")\n",
    "\n",
    "        super().__init__(modules)\n",
    "        self.unsafe = unsafe\n",
    "\n",
    "        # In plain words:\n",
    "        # module.weight must keep its dtype and shape.\n",
    "        # Furthermore, if there is no right_inverse or the right_inverse returns a tensor,\n",
    "        # this should be of the same dtype as the original tensor\n",
    "        #\n",
    "        # We check that the following invariants hold:\n",
    "        #    X = module.weight\n",
    "        #    Y = param.right_inverse(X)\n",
    "        #    assert isinstance(Y, Tensor) or\n",
    "        #           (isinstance(Y, collections.abc.Sequence) and all(isinstance(t, Tensor) for t in Y))\n",
    "        #    Z = param(Y) if isisntance(Y, Tensor) else param(*Y)\n",
    "        #    # Consistency checks\n",
    "        #    assert X.dtype == Z.dtype and X.shape == Z.shape\n",
    "        #    # If it has one input, this allows to be able to use set_ to be able to\n",
    "        #    # move data to/from the original tensor without changing its id (which is what the\n",
    "        #    # optimiser uses to track parameters)\n",
    "        #    if isinstance(Y, Tensor)\n",
    "        #      assert X.dtype == Y.dtype\n",
    "        # Below we use original = X, new = Y\n",
    "\n",
    "        original_shape = original.shape\n",
    "        original_dtype = original.dtype\n",
    "\n",
    "        # Compute new\n",
    "        with torch.no_grad():\n",
    "            new = original\n",
    "            for module in reversed(self):  # type: ignore[call-overload]\n",
    "                if hasattr(module, \"right_inverse\"):\n",
    "                    try:\n",
    "                        new = module.right_inverse(new)\n",
    "                    except NotImplementedError:\n",
    "                        pass\n",
    "                # else, or if it throws, we assume that right_inverse is the identity\n",
    "\n",
    "        if not isinstance(new, Tensor) and not isinstance(\n",
    "            new, collections.abc.Sequence\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"'right_inverse' must return a Tensor or a Sequence of tensors (list, tuple...). \"\n",
    "                f\"Got {type(new).__name__}\"\n",
    "            )\n",
    "\n",
    "        # Set the number of original tensors\n",
    "        self.is_tensor = isinstance(new, Tensor)\n",
    "        self.ntensors = 1 if self.is_tensor else len(new)\n",
    "\n",
    "        # Register the tensor(s)\n",
    "        if self.is_tensor:\n",
    "            if original.dtype != new.dtype:\n",
    "                raise ValueError(\n",
    "                    \"When `right_inverse` outputs one tensor, it may not change the dtype.\\n\"\n",
    "                    f\"original.dtype: {original.dtype}\\n\"\n",
    "                    f\"right_inverse(original).dtype: {new.dtype}\"\n",
    "                )\n",
    "            # Set the original to original so that the user does not need to re-register the parameter\n",
    "            # manually in the optimiser\n",
    "            with torch.no_grad():\n",
    "                original.set_(new)  # type: ignore[call-overload]\n",
    "            _register_parameter_or_buffer(self, \"original\", original)\n",
    "        else:\n",
    "            for i, originali in enumerate(new):\n",
    "                if not isinstance(originali, Tensor):\n",
    "                    raise ValueError(\n",
    "                        \"'right_inverse' must return a Tensor or a Sequence of tensors \"\n",
    "                        \"(list, tuple...). \"\n",
    "                        f\"Got element {i} of the sequence with type {type(originali).__name__}.\"\n",
    "                    )\n",
    "\n",
    "                # If the original tensor was a Parameter that required grad, we expect the user to\n",
    "                # add the new parameters to the optimizer after registering the parametrization\n",
    "                # (this is documented)\n",
    "                if isinstance(original, Parameter):\n",
    "                    originali = Parameter(originali)\n",
    "                originali.requires_grad_(original.requires_grad)\n",
    "                _register_parameter_or_buffer(self, f\"original{i}\", originali)\n",
    "\n",
    "        if not self.unsafe:\n",
    "            # Consistency checks:\n",
    "            # Since f : A -> B, right_inverse : B -> A, Z and original should live in B\n",
    "            # Z = forward(right_inverse(original))\n",
    "            Z = self()\n",
    "            if not isinstance(Z, Tensor):\n",
    "                raise ValueError(\n",
    "                    f\"A parametrization must return a tensor. Got {type(Z).__name__}.\"\n",
    "                )\n",
    "            if Z.dtype != original_dtype:\n",
    "                raise ValueError(\n",
    "                    \"Registering a parametrization may not change the dtype of the tensor, unless `unsafe` flag is enabled.\\n\"\n",
    "                    f\"unparametrized dtype: {original_dtype}\\n\"\n",
    "                    f\"parametrized dtype: {Z.dtype}\"\n",
    "                )\n",
    "            if Z.shape != original_shape:\n",
    "                raise ValueError(\n",
    "                    \"Registering a parametrization may not change the shape of the tensor, unless `unsafe` flag is enabled.\\n\"\n",
    "                    f\"unparametrized shape: {original_shape}\\n\"\n",
    "                    f\"parametrized shape: {Z.shape}\"\n",
    "                )\n",
    "\n",
    "    def right_inverse(self, value: Tensor) -> None:\n",
    "        r\"\"\"Calls the methods ``right_inverse`` (see :func:`register_parametrization`)\n",
    "        of the parametrizations in the inverse order they were registered in.\n",
    "        Then, it stores the result in ``self.original`` if ``right_inverse`` outputs one tensor\n",
    "        or in ``self.original0``, ``self.original1``, ... if it outputs several.\n",
    "\n",
    "        Args:\n",
    "            value (Tensor): Value to which initialize the module\n",
    "        \"\"\"\n",
    "        # All the exceptions in this function should almost never throw.\n",
    "        # They could throw if, for example, right_inverse function returns a different\n",
    "        # dtype when given a different input, which should most likely be caused by a\n",
    "        # bug in the user's code\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # See https://github.com/pytorch/pytorch/issues/53103\n",
    "            for module in reversed(self):  # type: ignore[call-overload]\n",
    "                if hasattr(module, \"right_inverse\"):\n",
    "                    value = module.right_inverse(value)\n",
    "                else:\n",
    "                    raise RuntimeError(\n",
    "                        f\"parametrization {type(module).__name__} does not implement \"\n",
    "                        \"right_inverse.\"\n",
    "                    )\n",
    "            if self.is_tensor:\n",
    "                # These exceptions should only throw when a right_inverse function does not\n",
    "                # return the same dtype for every input, which should most likely be caused by a bug\n",
    "                if not isinstance(value, Tensor):\n",
    "                    raise ValueError(\n",
    "                        f\"`right_inverse` should return a tensor. Got {type(value).__name__}\"\n",
    "                    )\n",
    "                if value.dtype != self.original.dtype:\n",
    "                    raise ValueError(\n",
    "                        f\"The tensor returned by `right_inverse` has dtype {value.dtype} \"\n",
    "                        f\"while `original` has dtype {self.original.dtype}\"\n",
    "                    )\n",
    "                # We know that the result is going to have the same dtype\n",
    "                self.original.set_(value)  # type: ignore[call-overload]\n",
    "            else:\n",
    "                if not isinstance(value, collections.abc.Sequence):\n",
    "                    raise ValueError(\n",
    "                        \"'right_inverse' must return a sequence of tensors. \"\n",
    "                        f\"Got {type(value).__name__}.\"\n",
    "                    )\n",
    "                if len(value) != self.ntensors:\n",
    "                    raise ValueError(\n",
    "                        \"'right_inverse' must return a sequence of tensors of length \"\n",
    "                        f\"{self.ntensors}. Got a sequence of lenght {len(value)}.\"\n",
    "                    )\n",
    "                for i, tensor in enumerate(value):\n",
    "                    original_i = getattr(self, f\"original{i}\")\n",
    "                    if not isinstance(tensor, Tensor):\n",
    "                        raise ValueError(\n",
    "                            f\"`right_inverse` must return a sequence of tensors. \"\n",
    "                            f\"Got element {i} of type {type(tensor).__name__}\"\n",
    "                        )\n",
    "                    if original_i.dtype != tensor.dtype:\n",
    "                        raise ValueError(\n",
    "                            f\"Tensor {i} returned by `right_inverse` has dtype {tensor.dtype} \"\n",
    "                            f\"while `original{i}` has dtype {original_i.dtype}\"\n",
    "                        )\n",
    "                    original_i.set_(tensor)\n",
    "\n",
    "    def forward(self) -> Tensor:\n",
    "        # Unpack the originals for the first parametrization\n",
    "        if self.is_tensor:\n",
    "            x = self[0](self.original)\n",
    "        else:\n",
    "            originals = (getattr(self, f\"original{i}\") for i in range(self.ntensors))\n",
    "            x = self[0](*originals)\n",
    "        # It's not possible to call self[1:] here, so we have to be a bit more cryptic\n",
    "        # Also we want to skip all non-integer keys\n",
    "        curr_idx = 1\n",
    "        while hasattr(self, str(curr_idx)):\n",
    "            x = self[curr_idx](x)\n",
    "            curr_idx += 1\n",
    "        return x\n",
    "\n",
    "\n",
    "def _inject_new_class(module: Module) -> None:\n",
    "    r\"\"\"Sets up a module to be parametrized.\n",
    "\n",
    "    This works by substituting the class of the module by a class\n",
    "    that extends it to be able to inject a property\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): module into which to inject the property\n",
    "    \"\"\"\n",
    "    cls = module.__class__\n",
    "\n",
    "    def getstate(self):\n",
    "        raise RuntimeError(\n",
    "            \"Serialization of parametrized modules is only \"\n",
    "            \"supported through state_dict(). See:\\n\"\n",
    "            \"https://pytorch.org/tutorials/beginner/saving_loading_models.html\"\n",
    "            \"#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training\"\n",
    "        )\n",
    "\n",
    "    param_cls = type(\n",
    "        f\"Parametrized{cls.__name__}\",\n",
    "        (cls,),\n",
    "        {\n",
    "            \"__getstate__\": getstate,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    module.__class__ = param_cls\n",
    "\n",
    "\n",
    "def _inject_property(module: Module, tensor_name: str) -> None:\n",
    "    r\"\"\"Injects a property into module[tensor_name].\n",
    "\n",
    "    It assumes that the class in the module has already been modified from its\n",
    "    original one using _inject_new_class and that the tensor under :attr:`tensor_name`\n",
    "    has already been moved out\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): module into which to inject the property\n",
    "        tensor_name (str): name of the name of the property to create\n",
    "    \"\"\"\n",
    "    # We check the precondition.\n",
    "    # This should never fire if register_parametrization is correctly implemented\n",
    "    assert not hasattr(module, tensor_name)\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def get_cached_parametrization(parametrization) -> Tensor:\n",
    "        global _cache\n",
    "        key = (id(module), tensor_name)\n",
    "        tensor = _cache.get(key)\n",
    "        if tensor is None:\n",
    "            tensor = parametrization()\n",
    "            _cache[key] = tensor\n",
    "        return tensor\n",
    "\n",
    "    def get_parametrized(self) -> Tensor:\n",
    "        parametrization = self.parametrizations[tensor_name]\n",
    "        if _cache_enabled:\n",
    "            if torch.jit.is_scripting():\n",
    "                # Scripting\n",
    "                raise RuntimeError(\n",
    "                    \"Caching is not implemented for scripting. \"\n",
    "                    \"Either disable caching or avoid scripting.\"\n",
    "                )\n",
    "            elif torch._C._get_tracing_state() is not None:\n",
    "                # Tracing\n",
    "                raise RuntimeError(\n",
    "                    \"Cannot trace a model while caching parametrizations.\"\n",
    "                )\n",
    "            else:\n",
    "                return get_cached_parametrization(parametrization)\n",
    "        else:\n",
    "            # If caching is not active, this function just evaluates the parametrization\n",
    "            return parametrization()\n",
    "\n",
    "    def set_original(self, value: Tensor) -> None:\n",
    "        self.parametrizations[tensor_name].right_inverse(value)\n",
    "\n",
    "    setattr(module.__class__, tensor_name, property(get_parametrized, set_original))\n",
    "\n",
    "\n",
    "def register_parametrization(\n",
    "    module: Module,\n",
    "    tensor_name: str,\n",
    "    parametrization: Module,\n",
    "    *,\n",
    "    unsafe: bool = False,\n",
    ") -> Module:\n",
    "    r\"\"\"Adds a parametrization to a tensor in a module.\n",
    "\n",
    "    Assume that ``tensor_name=\"weight\"`` for simplicity. When accessing ``module.weight``,\n",
    "    the module will return the parametrized version ``parametrization(module.weight)``.\n",
    "    If the original tensor requires a gradient, the backward pass will differentiate\n",
    "    through :attr:`parametrization`, and the optimizer will update the tensor accordingly.\n",
    "\n",
    "    The first time that a module registers a parametrization, this function will add an attribute\n",
    "    ``parametrizations`` to the module of type :class:`~ParametrizationList`.\n",
    "\n",
    "    The list of parametrizations on the tensor ``weight`` will be accessible under\n",
    "    ``module.parametrizations.weight``.\n",
    "\n",
    "    The original tensor will be accessible under\n",
    "    ``module.parametrizations.weight.original``.\n",
    "\n",
    "    Parametrizations may be concatenated by registering several parametrizations\n",
    "    on the same attribute.\n",
    "\n",
    "    The training mode of a registered parametrization is updated on registration\n",
    "    to match the training mode of the host module\n",
    "\n",
    "    Parametrized parameters and buffers have an inbuilt caching system that can be activated\n",
    "    using the context manager :func:`cached`.\n",
    "\n",
    "    A :attr:`parametrization` may optionally implement a method with signature\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "        def right_inverse(self, X: Tensor) -> Union[Tensor, Sequence[Tensor]]\n",
    "\n",
    "    This method is called on the unparametrized tensor when the first parametrization\n",
    "    is registered to compute the initial value of the original tensor.\n",
    "    If this method is not implemented, the original tensor will be just the unparametrized tensor.\n",
    "\n",
    "    If all the parametrizations registered on a tensor implement `right_inverse` it is possible\n",
    "    to initialize a parametrized tensor by assigning to it, as shown in the example below.\n",
    "\n",
    "    It is possible for the first parametrization to depend on several inputs.\n",
    "    This may be implemented returning a tuple of tensors from ``right_inverse``\n",
    "    (see the example implementation of a ``RankOne`` parametrization below).\n",
    "\n",
    "    In this case, the unconstrained tensors are also located under ``module.parametrizations.weight``\n",
    "    with names ``original0``, ``original1``,...\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        If unsafe=False (default) both the forward and right_inverse methods will be called\n",
    "        once to perform a number of consistency checks.\n",
    "        If unsafe=True, then right_inverse will be called if the tensor is not parametrized,\n",
    "        and nothing will be called otherwise.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        In most situations, ``right_inverse`` will be a function such that\n",
    "        ``forward(right_inverse(X)) == X`` (see\n",
    "        `right inverse <https://en.wikipedia.org/wiki/Inverse_function#Right_inverses>`_).\n",
    "        Sometimes, when the parametrization is not surjective, it may be reasonable\n",
    "        to relax this.\n",
    "\n",
    "    .. warning::\n",
    "\n",
    "        If a parametrization depends on several inputs, :func:`~register_parametrization`\n",
    "        will register a number of new parameters. If such parametrization is registered\n",
    "        after the optimizer is created, these new parameters will need to be added manually\n",
    "        to the optimizer. See :meth:`torch.Optimizer.add_param_group`.\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): module on which to register the parametrization\n",
    "        tensor_name (str): name of the parameter or buffer on which to register\n",
    "            the parametrization\n",
    "        parametrization (nn.Module): the parametrization to register\n",
    "    Keyword args:\n",
    "        unsafe (bool): a boolean flag that denotes whether the parametrization\n",
    "            may change the dtype and shape of the tensor. Default: `False`\n",
    "            Warning: the parametrization is not checked for consistency upon registration.\n",
    "            Enable this flag at your own risk.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if the module does not have a parameter or a buffer named :attr:`tensor_name`\n",
    "\n",
    "    Examples:\n",
    "        >>> import torch\n",
    "        >>> import torch.nn as nn\n",
    "        >>> import torch.nn.utils.parametrize as P\n",
    "        >>>\n",
    "        >>> class Symmetric(nn.Module):\n",
    "        >>>     def forward(self, X):\n",
    "        >>>         return X.triu() + X.triu(1).T  # Return a symmetric matrix\n",
    "        >>>\n",
    "        >>>     def right_inverse(self, A):\n",
    "        >>>         return A.triu()\n",
    "        >>>\n",
    "        >>> m = nn.Linear(5, 5)\n",
    "        >>> P.register_parametrization(m, \"weight\", Symmetric())\n",
    "        >>> print(torch.allclose(m.weight, m.weight.T))  # m.weight is now symmetric\n",
    "        True\n",
    "        >>> A = torch.rand(5, 5)\n",
    "        >>> A = A + A.T   # A is now symmetric\n",
    "        >>> m.weight = A  # Initialize the weight to be the symmetric matrix A\n",
    "        >>> print(torch.allclose(m.weight, A))\n",
    "        True\n",
    "\n",
    "        >>> class RankOne(nn.Module):\n",
    "        >>>     def forward(self, x, y):\n",
    "        >>>         # Form a rank 1 matrix multiplying two vectors\n",
    "        >>>         return x.unsqueeze(-1) @ y.unsqueeze(-2)\n",
    "        >>>\n",
    "        >>>     def right_inverse(self, Z):\n",
    "        >>>         # Project Z onto the rank 1 matrices\n",
    "        >>>         U, S, Vh = torch.linalg.svd(Z, full_matrices=False)\n",
    "        >>>         # Return rescaled singular vectors\n",
    "        >>>         s0_sqrt = S[0].sqrt().unsqueeze(-1)\n",
    "        >>>         return U[..., :, 0] * s0_sqrt, Vh[..., 0, :] * s0_sqrt\n",
    "        >>>\n",
    "        >>> linear_rank_one = P.register_parametrization(nn.Linear(4, 4), \"weight\", RankOne())\n",
    "        >>> print(torch.linalg.matrix_rank(linear_rank_one.weight).item())\n",
    "        1\n",
    "\n",
    "    \"\"\"\n",
    "    parametrization.train(module.training)\n",
    "    if is_parametrized(module, tensor_name):\n",
    "        # Correctness checks.\n",
    "        # If A is the space of tensors with shape and dtype equal to module.weight\n",
    "        # we check that parametrization.forward and parametrization.right_inverse are\n",
    "        # functions from A to A\n",
    "        if not unsafe:\n",
    "            Y = getattr(module, tensor_name)\n",
    "            X = parametrization(Y)\n",
    "            if not isinstance(X, Tensor):\n",
    "                raise ValueError(\n",
    "                    f\"A parametrization must return a tensor. Got {type(X).__name__}.\"\n",
    "                )\n",
    "            if X.dtype != Y.dtype:\n",
    "                raise ValueError(\n",
    "                    \"Registering a parametrization may not change the dtype of the tensor, unless the `unsafe` flag is enabled.\\n\"\n",
    "                    f\"module.{tensor_name}.dtype: {Y.dtype}\\n\"\n",
    "                    f\"parametrization(module.{tensor_name}).dtype: {X.dtype}\"\n",
    "                )\n",
    "            if X.shape != Y.shape:\n",
    "                raise ValueError(\n",
    "                    \"Registering a parametrization may not change the shape of the tensor, unless the `unsafe` flag is enabled.\\n\"\n",
    "                    f\"module.{tensor_name}.shape: {Y.shape}\\n\"\n",
    "                    f\"parametrization(module.{tensor_name}).shape: {X.shape}\"\n",
    "                )\n",
    "            if hasattr(parametrization, \"right_inverse\"):\n",
    "                try:\n",
    "                    Z = parametrization.right_inverse(X)  # type: ignore[operator]\n",
    "                except NotImplementedError:\n",
    "                    pass\n",
    "                else:\n",
    "                    if not isinstance(Z, Tensor):\n",
    "                        raise ValueError(\n",
    "                            f\"parametrization.right_inverse must return a tensor. Got: {type(Z).__name__}\"\n",
    "                        )\n",
    "                    if Z.dtype != Y.dtype:\n",
    "                        raise ValueError(\n",
    "                            \"The tensor returned by parametrization.right_inverse must have the same dtype \"\n",
    "                            f\"as module.{tensor_name}, unless the `unsafe` flag is enabled.\\n\"\n",
    "                            f\"module.{tensor_name}.dtype: {Y.dtype}\\n\"\n",
    "                            f\"returned dtype: {Z.dtype}\"\n",
    "                        )\n",
    "                    if Z.shape != Y.shape:\n",
    "                        raise ValueError(\n",
    "                            \"The tensor returned by parametrization.right_inverse must have the same shape \"\n",
    "                            f\"as module.{tensor_name}, unless the `unsafe` flag is enabled.\\n\"\n",
    "                            f\"module.{tensor_name}.shape: {Y.shape}\\n\"\n",
    "                            f\"returned shape: {Z.shape}\"\n",
    "                        )\n",
    "            # else right_inverse is assumed to be the identity\n",
    "\n",
    "        # add the new parametrization to the parametrization list\n",
    "        assert isinstance(module.parametrizations, ModuleDict)  # Make mypy happy\n",
    "        module.parametrizations[tensor_name].append(parametrization)\n",
    "        # If unsafe was True in previous parametrization, keep it enabled\n",
    "        module.parametrizations[tensor_name].unsafe |= unsafe  # type: ignore[index, union-attr]\n",
    "    elif tensor_name in module._buffers or tensor_name in module._parameters:\n",
    "        # Set the parametrization mechanism\n",
    "        # Fetch the original buffer or parameter\n",
    "        original = getattr(module, tensor_name)\n",
    "        # We create this early to check for possible errors\n",
    "        parametrizations = ParametrizationList(\n",
    "            [parametrization], original, unsafe=unsafe\n",
    "        )\n",
    "        # Delete the previous parameter or buffer\n",
    "        delattr(module, tensor_name)\n",
    "        # If this is the first parametrization registered on the module,\n",
    "        # we prepare the module to inject the property\n",
    "        if not is_parametrized(module):\n",
    "            # Change the class\n",
    "            _inject_new_class(module)\n",
    "            # Inject a ``ModuleDict`` into the instance under module.parametrizations\n",
    "            module.parametrizations = ModuleDict()\n",
    "        # Add a property into the class\n",
    "        _inject_property(module, tensor_name)\n",
    "        # Add a ParametrizationList\n",
    "        assert isinstance(module.parametrizations, ModuleDict)  # Make mypy happy\n",
    "        module.parametrizations[tensor_name] = parametrizations\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Module '{module}' does not have a parameter, a buffer, or a \"\n",
    "            f\"parametrized element with name '{tensor_name}'\"\n",
    "        )\n",
    "    return module\n",
    "\n",
    "\n",
    "def is_parametrized(module: Module, tensor_name: Optional[str] = None) -> bool:\n",
    "    r\"\"\"Returns ``True`` if module has an active parametrization.\n",
    "\n",
    "    If the argument :attr:`tensor_name` is specified, returns ``True`` if\n",
    "    ``module[tensor_name]`` is parametrized.\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): module to query\n",
    "        name (str, optional): attribute in the module to query\n",
    "            Default: ``None``\n",
    "    \"\"\"\n",
    "    parametrizations = getattr(module, \"parametrizations\", None)\n",
    "    if parametrizations is None or not isinstance(parametrizations, ModuleDict):\n",
    "        return False\n",
    "    if tensor_name is None:\n",
    "        # Check that there is at least one parametrized buffer or Parameter\n",
    "        return len(parametrizations) > 0\n",
    "    else:\n",
    "        return tensor_name in parametrizations\n",
    "\n",
    "\n",
    "def remove_parametrizations(\n",
    "    module: Module, tensor_name: str, leave_parametrized: bool = True\n",
    ") -> Module:\n",
    "    r\"\"\"Removes the parametrizations on a tensor in a module.\n",
    "\n",
    "    - If ``leave_parametrized=True``, ``module[tensor_name]`` will be set to\n",
    "      its current output. In this case, the parametrization shall not change the ``dtype``\n",
    "      of the tensor.\n",
    "    - If ``leave_parametrized=False``, ``module[tensor_name]`` will be set to\n",
    "      the unparametrised tensor in ``module.parametrizations[tensor_name].original``.\n",
    "      This is only possible when the parametrization depends on just one tensor.\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): module from which remove the parametrization\n",
    "        tensor_name (str): name of the parametrization to be removed\n",
    "        leave_parametrized (bool, optional): leave the attribute :attr:`tensor_name` parametrized.\n",
    "            Default: ``True``\n",
    "\n",
    "    Returns:\n",
    "        Module: module\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if ``module[tensor_name]`` is not parametrized\n",
    "        ValueError: if ``leave_parametrized=False`` and the parametrization depends on several tensors\n",
    "    \"\"\"\n",
    "\n",
    "    if not is_parametrized(module, tensor_name):\n",
    "        raise ValueError(\n",
    "            f\"Module {module} does not have a parametrization on {tensor_name}\"\n",
    "        )\n",
    "\n",
    "    # Fetch the original tensor\n",
    "    assert isinstance(module.parametrizations, ModuleDict)  # Make mypy happy\n",
    "    parametrizations = module.parametrizations[tensor_name]\n",
    "    if parametrizations.is_tensor:\n",
    "        original = parametrizations.original\n",
    "        if leave_parametrized:\n",
    "            with torch.no_grad():\n",
    "                t = getattr(module, tensor_name)\n",
    "            # We know they have the same dtype because we have checked this when registering the\n",
    "            # parametrizations. As such, we can use set_\n",
    "            # We do this so that the parameter does not to change the id()\n",
    "            # This way the user does not need to update the optimizer\n",
    "            with torch.no_grad():\n",
    "                if type(original) is torch.Tensor:\n",
    "                    original.set_(t)\n",
    "                else:\n",
    "                    try:\n",
    "                        original.set_(t)\n",
    "                    except RuntimeError as e:\n",
    "                        # TODO: Fix this for tensor subclasses that are parameters:\n",
    "                        # RuntimeError: set_storage is not allowed on a Tensor created from .data or .detach().\n",
    "                        raise RuntimeError(\n",
    "                            \"Calling remove_parametrizations() with leave_parametrized=True \"\n",
    "                            \"for a parameter that is an instance of a tensor subclass requires \"\n",
    "                            \"set_() to be implemented correctly for the tensor subclass. Either \"\n",
    "                            \"set leave_parametrized=False or provide a working implementation for \"\n",
    "                            \"set_() in the tensor subclass.\"\n",
    "                        )\n",
    "    else:\n",
    "        if leave_parametrized:\n",
    "            # We cannot use no_grad because we need to know whether one or more\n",
    "            # original tensors required grad\n",
    "            t = getattr(module, tensor_name)\n",
    "            # We'll have to trust the user to add it to the optimizer\n",
    "            original = Parameter(t) if t.requires_grad else t\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Cannot leave unparametrized (`leave_parametrized=False`) a tensor \"\n",
    "                \"that is parametrized in terms of a sequence of tensors.\"\n",
    "            )\n",
    "\n",
    "    # Delete the property that manages the parametrization\n",
    "    delattr(module.__class__, tensor_name)\n",
    "    # Delete the ParametrizationList\n",
    "    del module.parametrizations[tensor_name]\n",
    "\n",
    "    # Restore the parameter / buffer into the main class\n",
    "    _register_parameter_or_buffer(module, tensor_name, original)\n",
    "\n",
    "    # Roll back the parametrized class if no other buffer or parameter\n",
    "    # is currently parametrized in this class\n",
    "    if not is_parametrized(module):\n",
    "        delattr(module, \"parametrizations\")\n",
    "        # Restore class\n",
    "        orig_cls = module.__class__.__bases__[0]\n",
    "        module.__class__ = orig_cls\n",
    "    return module\n",
    "\n",
    "\n",
    "def type_before_parametrizations(module: Module) -> type:\n",
    "    r\"\"\"Returns the module type before parametrizations were applied and if not,\n",
    "    then it returns the module type.\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): module to get type of\n",
    "    \"\"\"\n",
    "    if is_parametrized(module):\n",
    "        return module.__class__.__bases__[0]\n",
    "    else:\n",
    "        return type(module)\n",
    "\n",
    "\n",
    "def transfer_parametrizations_and_params(\n",
    "    from_module: Module, to_module: Module, tensor_name: Optional[str] = None\n",
    ") -> Module:\n",
    "    r\"\"\"Transfers parametrizations and the parameters they parametrize from from_module\n",
    "    to to_module. If tensor_name is specified, only transfers the specified parameter, otherwise\n",
    "    transfers all parametrized parameters. If those parameters do not exist in to_module, it will create them.\n",
    "    Does nothing if from_module is not parametrized.\n",
    "\n",
    "    Args:\n",
    "        from_module (nn.Module): module to transfer from\n",
    "        to_module (nn.Module): module to transfer to\n",
    "        tensor_name (str, optional): parameter to transfer\n",
    "\n",
    "    Returns:\n",
    "        Module: to_module\n",
    "    \"\"\"\n",
    "    if is_parametrized(from_module):\n",
    "        assert isinstance(from_module.parametrizations, ModuleDict)  # for mypy\n",
    "\n",
    "        # get list of all params or the single param to transfer\n",
    "        parameters_to_transfer: Union[list, ModuleDict] = (\n",
    "            from_module.parametrizations if tensor_name is None else [tensor_name]\n",
    "        )\n",
    "\n",
    "        assert hasattr(parameters_to_transfer, \"__iter__\")  # for mypy\n",
    "        for parameter_name in parameters_to_transfer:\n",
    "\n",
    "            # initialize the to-be-transfered param in to_module if it doesn't exist already\n",
    "            if not hasattr(to_module, parameter_name):\n",
    "                setattr(\n",
    "                    to_module,\n",
    "                    parameter_name,\n",
    "                    Parameter(getattr(from_module, parameter_name)),\n",
    "                )\n",
    "\n",
    "            # apply the params's parametrizations to to_module\n",
    "            for param_func in from_module.parametrizations[parameter_name]:\n",
    "                register_parametrization(to_module, parameter_name, param_func)\n",
    "            assert isinstance(to_module.parametrizations, ModuleDict)  # for mypy\n",
    "\n",
    "            # make values match, original values can be stored in either original or\n",
    "            # original0, original1..., need to check both cases\n",
    "            if hasattr(from_module.parametrizations[parameter_name], \"original\"):\n",
    "                to_module.parametrizations[\n",
    "                    parameter_name\n",
    "                ].original = from_module.parametrizations[parameter_name].original\n",
    "            else:\n",
    "                num = 0\n",
    "                orig_num = \"original\" + str(num)\n",
    "                # loop through each original# until all values have been set\n",
    "                while hasattr(from_module.parametrizations[parameter_name], orig_num):\n",
    "                    setattr(\n",
    "                        to_module.parametrizations[parameter_name],\n",
    "                        orig_num,\n",
    "                        getattr(from_module.parametrizations[parameter_name], orig_num),\n",
    "                    )\n",
    "                    num = num + 1\n",
    "                    orig_num = \"original\" + str(num)\n",
    "\n",
    "    return to_module"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
