{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invertible layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'  # always print last expr.\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import torch.utils.cpp_extension\n",
    "from torch import Tensor, dot, eye, jit, nn, outer\n",
    "from torch.linalg import matrix_norm, vector_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SpectralNorm(torch.autograd.Function):\n",
    "    r\"\"\"$‖A‖_2=λ_\\max(A^⊤A)$.\n",
    "\n",
    "    The spectral norm $∥A∥_2 ≔ \\sup_x ∥Ax∥_2 / ∥x∥_2$ can be shown to be equal to\n",
    "    $σ_{\\max}(A) = \\sqrt{λ_{\\max} (A^⊤A)}$, the largest singular value of $A$.\n",
    "\n",
    "    It can be computed efficiently via Power iteration.\n",
    "\n",
    "    One can show that the derivative is equal to:\n",
    "\n",
    "    .. math::  \\pdv{½∥A∥_2}{A} = uv^⊤\n",
    "\n",
    "    where $u,v$ are the left/right-singular vector corresponding to $σ_\\max$\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    - | `Spectral Normalization for Generative Adversarial Networks\n",
    "        <https://openreview.net/forum?id=B1QRgziT->`_\n",
    "      | Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida\n",
    "      | `International Conference on Learning Representations 2018\n",
    "        <https://iclr.cc/Conferences/2018>`_\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx: Any, *tensors: Tensor, **kwargs: Any) -> Tensor:\n",
    "        r\"\"\".. Signature:: ``(m, n) -> 1``.\"\"\"\n",
    "        A = tensors[0]\n",
    "        atol: float = kwargs[\"atol\"] if \"atol\" in kwargs else 1e-6\n",
    "        rtol: float = kwargs[\"rtol\"] if \"rtol\" in kwargs else 1e-6\n",
    "        maxiter: int = kwargs[\"maxiter\"] if \"maxiter\" in kwargs else 1000\n",
    "        m, n, *other = A.shape\n",
    "        assert not other, \"Expected 2D input.\"\n",
    "        # initialize u and v, median should be useful guess.\n",
    "        u = u_next = A.median(dim=1).values\n",
    "        v = v_next = A.median(dim=0).values\n",
    "        σ: Tensor = torch.einsum(\"ij, i, j ->\", A, u, v)\n",
    "\n",
    "        for _ in range(maxiter):\n",
    "            u = u_next / torch.norm(u_next)\n",
    "            v = v_next / torch.norm(v_next)\n",
    "            # choose optimal σ given u and v: σ = argmin ‖A - σuvᵀ‖²\n",
    "            σ = torch.einsum(\"ij, i, j ->\", A, u, v)  # u.T @ A @ v\n",
    "            # Residual: if Av = σu and Aᵀu = σv\n",
    "            u_next = A @ v\n",
    "            v_next = A.T @ u\n",
    "            σu = σ * u\n",
    "            σv = σ * v\n",
    "            ru = u_next - σ * u\n",
    "            rv = v_next - σ * v\n",
    "            if (\n",
    "                vector_norm(ru) <= rtol * vector_norm(σu) + atol\n",
    "                and vector_norm(rv) <= rtol * vector_norm(σv) + atol\n",
    "            ):\n",
    "                break\n",
    "\n",
    "        ctx.save_for_backward(u, v)\n",
    "        return σ\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx: Any, *grad_outputs: Tensor) -> Tensor:\n",
    "        u, v = ctx.saved_tensors\n",
    "        return torch.einsum(\"..., i, j -> ...ij\", grad_outputs[0], u, v)\n",
    "\n",
    "    vjp = backward\n",
    "\n",
    "    @staticmethod\n",
    "    def jvp(ctx: Any, *grad_inputs: Any) -> Any:\n",
    "        r\"\"\"Jacobian-vector product forward mode.\"\"\"\n",
    "        u, v = ctx.saved_tensors\n",
    "        return torch.einsum(\"...ij, i, j -> ...\", grad_inputs[0], u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m, n = 4, 3\n",
    "A = torch.nn.Parameter(torch.randn(m, n))\n",
    "\n",
    "U, S, V = torch.svd(A)\n",
    "u = U[:, 0]\n",
    "v = V[:, 0]\n",
    "s = S[0]\n",
    "\n",
    "u.retain_grad()\n",
    "v.retain_grad()\n",
    "s.retain_grad()\n",
    "\n",
    "assert torch.allclose(A @ v, s * u)\n",
    "assert torch.allclose(A.T @ u, s * v)\n",
    "assert torch.allclose(s, u.T @ A @ v)\n",
    "\n",
    "phi = torch.randn(m)\n",
    "psi = torch.randn(n) * 0.0\n",
    "xi = torch.cat([phi, psi])\n",
    "y = dot(phi, u) + dot(psi, v)\n",
    "y.backward()\n",
    "assert all(phi == u.grad)\n",
    "assert all(psi == v.grad)\n",
    "print(A.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "K = torch.cat(\n",
    "    [\n",
    "        torch.cat([s * torch.eye(m), -A], dim=-1),\n",
    "        torch.cat([-A.T, s * torch.eye(n)], dim=-1),\n",
    "    ],\n",
    "    dim=0,\n",
    ")\n",
    "sol = torch.linalg.lstsq(K.T, xi)[0]  # solve unstable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "du = sol[:4]\n",
    "dv = sol[4:]\n",
    "manual_grad = (eye(m) - outer(u, u)) @ outer(du, v) + outer(u, dv) @ (\n",
    "    eye(n) - outer(v, v)\n",
    ")\n",
    "assert torch.allclose(A.grad, manual_grad, atol=10**-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iResNet layer\n",
    "\n",
    "Any module with forward pass $y = f(x) = x + g(x)$, where $g = g_L ∘ g_{L-1} ∘ … ∘ g_1$, each layer is a contraction ($\\text{Lip}(g) < 1$).\n",
    "Then $x = y - g(x)$ is a fixed point equation that can be solved by fixed point iteration.\n",
    "\n",
    "$$ x' = y - g(x)$$\n",
    "\n",
    "\n",
    "Alternatively, we can solve with gradient descent. Question: is the fixed point iteration equivalent to some GD scheme? We have:\n",
    "\n",
    "\n",
    "$$ x' = y - g(x) = x - x + y - g(x) = x - (x+g(x) - y) = x - ∇_x ∫ (x + g(x) - y) dx $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## backward for iResNet inverse\n",
    "\n",
    "So, we calculated $x(y) = f^{-1}(y)$ via fixed point iteration in the inverse pass. What is the gradient? By the inverse function theorem: \n",
    "\n",
    "$$𝐃[f^{-1}](y) = \\big(𝐃[f](x)\\big)^{-1}$$\n",
    "\n",
    "making use of this fact, we can compute Vector-Jacobian-Products (VJP) as \n",
    "\n",
    "$$ [∆y ↦ ⟨ v ∣ \\big(𝐃[f](x)\\big)^{-1} ∆y⟩ =  [∆y ↦⟨  \\big(𝐃[f](x)\\big)^{-T}v ∣ ∆y  ⟩] $$\n",
    "\n",
    "Hence, the VJP is given by $⟨\\text{solve}( 𝐃[f](f^{-1}(y))^T v ∣ outer  )$\n",
    "\n",
    "The big question: How do we get the transpose?!\n",
    "\n",
    "⟹ All sublayers must make the transpose available! \n",
    "\n",
    "$$ 𝐃[f](f^{-1}(y))^T v = 𝐃[f_1∘f_2 ∘…∘f_n]^T v = \\Big(𝐃[f_1]∘𝐃[f_2] ∘…∘𝐃[f_n]\\Big)^T v = 𝐃[f_n]^⊤ ∘ … ∘ 𝐃[f_1]^T v$$\n",
    "\n",
    "BUT THIS IS JUST THE VJP of $f$ !!!\n",
    "\n",
    "\n",
    "Thus, the goal becomes:\n",
    "\n",
    "$$ \\text{solve}( VJP(f, x, v), w) $$\n",
    "\n",
    "And we can make use of any iterative solver!!\n",
    "\n",
    "\n",
    "However, we need a library that works with general tensorial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iResNet(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)\n",
    "        return x\n",
    "\n",
    "    def inverse(self, y):\n",
    "        \"\"\"via fixed point iteration.\"\"\"\n",
    "        x = y\n",
    "        for k in range(10):\n",
    "            x = y - self(x)\n",
    "        return x\n",
    "\n",
    "    def vjp_inverse(self, outer_grad, ctx):\n",
    "        x = ctx[\"x\"]\n",
    "        return solve(lambda v: vjp(f, x, v), outer_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anderson(f, x0, m=5, lam=1e-4, max_iter=50, tol=1e-2, beta=1.0):\n",
    "    \"\"\"Anderson acceleration for fixed point iteration.\"\"\"\n",
    "    bsz, d, H, W = x0.shape\n",
    "    X = torch.zeros(bsz, m, d * H * W, dtype=x0.dtype, device=x0.device)\n",
    "    F = torch.zeros(bsz, m, d * H * W, dtype=x0.dtype, device=x0.device)\n",
    "    X[:, 0], F[:, 0] = x0.view(bsz, -1), f(x0).view(bsz, -1)\n",
    "    X[:, 1], F[:, 1] = F[:, 0], f(F[:, 0].view_as(x0)).view(bsz, -1)\n",
    "\n",
    "    H = torch.zeros(bsz, m + 1, m + 1, dtype=x0.dtype, device=x0.device)\n",
    "    H[:, 0, 1:] = H[:, 1:, 0] = 1\n",
    "    y = torch.zeros(bsz, m + 1, 1, dtype=x0.dtype, device=x0.device)\n",
    "    y[:, 0] = 1\n",
    "\n",
    "    res = []\n",
    "    for k in range(2, max_iter):\n",
    "        n = min(k, m)\n",
    "        G = F[:, :n] - X[:, :n]\n",
    "        H[:, 1 : n + 1, 1 : n + 1] = (\n",
    "            torch.bmm(G, G.transpose(1, 2))\n",
    "            + lam * torch.eye(n, dtype=x0.dtype, device=x0.device)[None]\n",
    "        )\n",
    "        alpha = torch.solve(y[:, : n + 1], H[:, : n + 1, : n + 1])[0][\n",
    "            :, 1 : n + 1, 0\n",
    "        ]  # (bsz x n)\n",
    "\n",
    "        X[:, k % m] = (\n",
    "            beta * (alpha[:, None] @ F[:, :n])[:, 0]\n",
    "            + (1 - beta) * (alpha[:, None] @ X[:, :n])[:, 0]\n",
    "        )\n",
    "        F[:, k % m] = f(X[:, k % m].view_as(x0)).view(bsz, -1)\n",
    "        res.append(\n",
    "            (F[:, k % m] - X[:, k % m]).norm().item()\n",
    "            / (1e-5 + F[:, k % m].norm().item())\n",
    "        )\n",
    "        if res[-1] < tol:\n",
    "            break\n",
    "    return X[:, k % m].view_as(x0), res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Final\n",
    "\n",
    "import torch.autograd as autograd\n",
    "\n",
    "\n",
    "class DEQFixedPoint(nn.Module):\n",
    "    maxiter: Final[int]\n",
    "\n",
    "    def __init__(self, maxiter: int = 5):\n",
    "        super().__init__()\n",
    "        self.f = nn.Linear(5, 5)\n",
    "        self.maxiter = maxiter\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # compute forward pass and re-engage autograd tape\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = x.clone()\n",
    "            for k in range(self.maxiter):\n",
    "                z = x - self.f(z)\n",
    "\n",
    "        # re-engage tape\n",
    "        z = x - self.f(z)\n",
    "\n",
    "        # set up Jacobian vector product (without additional forward calls)\n",
    "        z0 = z.clone().detach().requires_grad_()\n",
    "        f0 = x - self.f(z0)\n",
    "\n",
    "        z.register_hook(self.custom_backward)\n",
    "        return z\n",
    "\n",
    "    def vjp_f(self, z0, y):\n",
    "        return autograd.vjp(self.f, z0, y)\n",
    "\n",
    "    def custom_backward(grad: Tensor) -> Tensor:\n",
    "        return torch.linalg.solve(self.vjp, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = DEQFixedPoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model(torch.randn(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jit.script(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.randn(5).register_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn.Linear(4, 5)(torch.randn(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example: spectral normalization layer\n",
    "\n",
    "\n",
    "consider $y = x + \\frac{A}{‖A‖₂}x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Linear Solver Layer\n",
    "\n",
    "\n",
    "Consider: $f:(A, b) ↦ solve(A, b)$\n",
    "\n",
    "Then $\\frac{∂f}{∂A} = solve(A^⊤, -𝕀⊗x)$ and $\\frac{∂f}{∂b} = solve(A, 𝕀)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A = np.random.randn(5, 5)\n",
    "b = np.random.randn(5)\n",
    "x = np.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = jax.jacfwd(jnp.linalg.solve)\n",
    "g(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.linalg.solve(A.T, -np.einsum(\"ij, k -> ijk\", np.eye(5), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
