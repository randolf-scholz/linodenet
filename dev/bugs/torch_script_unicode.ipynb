{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c93bd-bbd3-4f65-b2db-52130940ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, jit, Tensor\n",
    "\n",
    "\n",
    "class WithoutUnicode(nn.Module):\n",
    "    def forward(self, delta_t: Tensor) -> Tensor:\n",
    "        return delta_t\n",
    "\n",
    "\n",
    "# non-jitted\n",
    "model = WithoutUnicode()\n",
    "torch.save(model, \"WithoutUnicode.pt\")\n",
    "torch.load(\"WithoutUnicode.pt\")  # <- Succeeds ✔\n",
    "\n",
    "\n",
    "model = jit.script(WithoutUnicode())\n",
    "jit.save(model, \"WithoutUnicode.pt\")\n",
    "jit.load(\"WithoutUnicode.pt\")  # <- Succeeds ✔\n",
    "\n",
    "\n",
    "class WithUnicode(nn.Module):\n",
    "    def forward(self, Δt: Tensor) -> Tensor:\n",
    "        return Δt\n",
    "\n",
    "\n",
    "model = WithUnicode()\n",
    "torch.save(model, \"WithUnicode.pt\")\n",
    "torch.load(\"WithUnicode.pt\")  # <- Succeeds ✔\n",
    "\n",
    "model = jit.script(WithUnicode())\n",
    "jit.save(model, \"WithUnicode.pt\")\n",
    "jit.load(\"WithUnicode.pt\")  # <- Fails ↯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f084d8b8-077e-4585-a625-cdb2c972b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, jit, Tensor\n",
    "\n",
    "\n",
    "class WithUnicode(nn.Module):\n",
    "    α: Tensor\n",
    "\n",
    "    def __init__(self, α: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"α\", torch.tensor(α))\n",
    "\n",
    "    def forward(self, x: Tensor, t: Tensor, t0: Tensor) -> Tensor:\n",
    "        Δt = t - t0\n",
    "        return self.α * x * Δt\n",
    "\n",
    "\n",
    "model = WithUnicode()\n",
    "torch.save(model, \"WithUnicode.pt\")\n",
    "torch.load(\"WithUnicode.pt\")  # <- Succeeds ✔\n",
    "\n",
    "model = jit.script(WithUnicode())\n",
    "jit.save(model, \"WithUnicode.pt\")\n",
    "jit.load(\"WithUnicode.pt\")  # <- Fails ↯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d36650-8e52-4cb2-9a58-7eb78acbb657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, jit, Tensor\n",
    "\n",
    "\n",
    "class WithUnicode(nn.Module):\n",
    "    α: Tensor\n",
    "\n",
    "    def __init__(self, α: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"α\", torch.tensor(α))\n",
    "\n",
    "    def forward(self, x: Tensor, t: Tensor, t0: Tensor) -> Tensor:\n",
    "        Δt = t - t0\n",
    "        return self.α * x * Δt\n",
    "\n",
    "\n",
    "model = jit.script(WithUnicode())\n",
    "jit.save(model, \"WithUnicode.pt\")\n",
    "jit.load(\"WithUnicode.pt\")  # <- Fails ↯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a513764-407c-464b-951a-58735461b018",
   "metadata": {},
   "outputs": [],
   "source": [
    "jit.load(\"WithUnicode.pt\")  # <- Fails ↯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398a50d7-9d6e-4434-8d4d-3e979a7dc906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Unlike the rest of the PyTorch this file must be python2 compliant.\n",
    "# This script outputs relevant system environment info\n",
    "# Run it with `python collect_env.py`.\n",
    "import datetime\n",
    "import locale\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "\n",
    "    TORCH_AVAILABLE = True\n",
    "except (ImportError, NameError, AttributeError, OSError):\n",
    "    TORCH_AVAILABLE = False\n",
    "\n",
    "# System Environment Information\n",
    "SystemEnv = namedtuple(\n",
    "    \"SystemEnv\",\n",
    "    [\n",
    "        \"torch_version\",\n",
    "        \"is_debug_build\",\n",
    "        \"cuda_compiled_version\",\n",
    "        \"gcc_version\",\n",
    "        \"clang_version\",\n",
    "        \"cmake_version\",\n",
    "        \"os\",\n",
    "        \"libc_version\",\n",
    "        \"python_version\",\n",
    "        \"python_platform\",\n",
    "        \"is_cuda_available\",\n",
    "        \"cuda_runtime_version\",\n",
    "        \"nvidia_driver_version\",\n",
    "        \"nvidia_gpu_models\",\n",
    "        \"cudnn_version\",\n",
    "        \"pip_version\",  # 'pip' or 'pip3'\n",
    "        \"pip_packages\",\n",
    "        \"conda_packages\",\n",
    "        \"hip_compiled_version\",\n",
    "        \"hip_runtime_version\",\n",
    "        \"miopen_runtime_version\",\n",
    "        \"caching_allocator_config\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "def run(command):\n",
    "    \"\"\"Returns (return-code, stdout, stderr)\"\"\"\n",
    "    p = subprocess.Popen(\n",
    "        command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True\n",
    "    )\n",
    "    raw_output, raw_err = p.communicate()\n",
    "    rc = p.returncode\n",
    "    if get_platform() == \"win32\":\n",
    "        enc = \"oem\"\n",
    "    else:\n",
    "        enc = locale.getpreferredencoding()\n",
    "    output = raw_output.decode(enc)\n",
    "    err = raw_err.decode(enc)\n",
    "    return rc, output.strip(), err.strip()\n",
    "\n",
    "\n",
    "def run_and_read_all(run_lambda, command):\n",
    "    \"\"\"Runs command using run_lambda; reads and returns entire output if rc is 0\"\"\"\n",
    "    rc, out, _ = run_lambda(command)\n",
    "    if rc != 0:\n",
    "        return None\n",
    "    return out\n",
    "\n",
    "\n",
    "def run_and_parse_first_match(run_lambda, command, regex):\n",
    "    \"\"\"Runs command using run_lambda, returns the first regex match if it exists\"\"\"\n",
    "    rc, out, _ = run_lambda(command)\n",
    "    if rc != 0:\n",
    "        return None\n",
    "    match = re.search(regex, out)\n",
    "    if match is None:\n",
    "        return None\n",
    "    return match.group(1)\n",
    "\n",
    "\n",
    "def run_and_return_first_line(run_lambda, command):\n",
    "    \"\"\"Runs command using run_lambda and returns first line if output is not empty\"\"\"\n",
    "    rc, out, _ = run_lambda(command)\n",
    "    if rc != 0:\n",
    "        return None\n",
    "    return out.split(\"\\n\")[0]\n",
    "\n",
    "\n",
    "def get_conda_packages(run_lambda):\n",
    "    if get_platform() == \"win32\":\n",
    "        system_root = os.environ.get(\"SYSTEMROOT\", \"C:\\\\Windows\")\n",
    "        findstr_cmd = os.path.join(system_root, \"System32\", \"findstr\")\n",
    "        grep_cmd = r'{} /R \"torch numpy cudatoolkit soumith mkl magma mypy\"'.format(\n",
    "            findstr_cmd\n",
    "        )\n",
    "    else:\n",
    "        grep_cmd = r'grep \"torch\\|numpy\\|cudatoolkit\\|soumith\\|mkl\\|magma\\|mypy\"'\n",
    "    conda = os.environ.get(\"CONDA_EXE\", \"conda\")\n",
    "    out = run_and_read_all(run_lambda, conda + \" list | \" + grep_cmd)\n",
    "    if out is None:\n",
    "        return out\n",
    "    # Comment starting at beginning of line\n",
    "    comment_regex = re.compile(r\"^#.*\\n\")\n",
    "    return re.sub(comment_regex, \"\", out)\n",
    "\n",
    "\n",
    "def get_gcc_version(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, \"gcc --version\", r\"gcc (.*)\")\n",
    "\n",
    "\n",
    "def get_clang_version(run_lambda):\n",
    "    return run_and_parse_first_match(\n",
    "        run_lambda, \"clang --version\", r\"clang version (.*)\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_cmake_version(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, \"cmake --version\", r\"cmake (.*)\")\n",
    "\n",
    "\n",
    "def get_nvidia_driver_version(run_lambda):\n",
    "    if get_platform() == \"darwin\":\n",
    "        cmd = \"kextstat | grep -i cuda\"\n",
    "        return run_and_parse_first_match(\n",
    "            run_lambda, cmd, r\"com[.]nvidia[.]CUDA [(](.*?)[)]\"\n",
    "        )\n",
    "    smi = get_nvidia_smi()\n",
    "    return run_and_parse_first_match(run_lambda, smi, r\"Driver Version: (.*?) \")\n",
    "\n",
    "\n",
    "def get_gpu_info(run_lambda):\n",
    "    if get_platform() == \"darwin\" or (\n",
    "        TORCH_AVAILABLE\n",
    "        and hasattr(torch.version, \"hip\")\n",
    "        and torch.version.hip is not None\n",
    "    ):\n",
    "        if TORCH_AVAILABLE and torch.cuda.is_available():\n",
    "            return torch.cuda.get_device_name(None)\n",
    "        return None\n",
    "    smi = get_nvidia_smi()\n",
    "    uuid_regex = re.compile(r\" \\(UUID: .+?\\)\")\n",
    "    rc, out, _ = run_lambda(smi + \" -L\")\n",
    "    if rc != 0:\n",
    "        return None\n",
    "    # Anonymize GPUs by removing their UUID\n",
    "    return re.sub(uuid_regex, \"\", out)\n",
    "\n",
    "\n",
    "def get_running_cuda_version(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, \"nvcc --version\", r\"release .+ V(.*)\")\n",
    "\n",
    "\n",
    "def get_cudnn_version(run_lambda):\n",
    "    \"\"\"This will return a list of libcudnn.so; it's hard to tell which one is being used\"\"\"\n",
    "    if get_platform() == \"win32\":\n",
    "        system_root = os.environ.get(\"SYSTEMROOT\", \"C:\\\\Windows\")\n",
    "        cuda_path = os.environ.get(\"CUDA_PATH\", \"%CUDA_PATH%\")\n",
    "        where_cmd = os.path.join(system_root, \"System32\", \"where\")\n",
    "        cudnn_cmd = '{} /R \"{}\\\\bin\" cudnn*.dll'.format(where_cmd, cuda_path)\n",
    "    elif get_platform() == \"darwin\":\n",
    "        # CUDA libraries and drivers can be found in /usr/local/cuda/. See\n",
    "        # https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html#install\n",
    "        # https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installmac\n",
    "        # Use CUDNN_LIBRARY when cudnn library is installed elsewhere.\n",
    "        cudnn_cmd = \"ls /usr/local/cuda/lib/libcudnn*\"\n",
    "    else:\n",
    "        cudnn_cmd = 'ldconfig -p | grep libcudnn | rev | cut -d\" \" -f1 | rev'\n",
    "    rc, out, _ = run_lambda(cudnn_cmd)\n",
    "    # find will return 1 if there are permission errors or if not found\n",
    "    if len(out) == 0 or (rc != 1 and rc != 0):\n",
    "        l = os.environ.get(\"CUDNN_LIBRARY\")\n",
    "        if l is not None and os.path.isfile(l):\n",
    "            return os.path.realpath(l)\n",
    "        return None\n",
    "    files_set = set()\n",
    "    for fn in out.split(\"\\n\"):\n",
    "        fn = os.path.realpath(fn)  # eliminate symbolic links\n",
    "        if os.path.isfile(fn):\n",
    "            files_set.add(fn)\n",
    "    if not files_set:\n",
    "        return None\n",
    "    # Alphabetize the result because the order is non-deterministic otherwise\n",
    "    files = list(sorted(files_set))\n",
    "    if len(files) == 1:\n",
    "        return files[0]\n",
    "    result = \"\\n\".join(files)\n",
    "    return \"Probably one of the following:\\n{}\".format(result)\n",
    "\n",
    "\n",
    "def get_nvidia_smi():\n",
    "    # Note: nvidia-smi is currently available only on Windows and Linux\n",
    "    smi = \"nvidia-smi\"\n",
    "    if get_platform() == \"win32\":\n",
    "        system_root = os.environ.get(\"SYSTEMROOT\", \"C:\\\\Windows\")\n",
    "        program_files_root = os.environ.get(\"PROGRAMFILES\", \"C:\\\\Program Files\")\n",
    "        legacy_path = os.path.join(\n",
    "            program_files_root, \"NVIDIA Corporation\", \"NVSMI\", smi\n",
    "        )\n",
    "        new_path = os.path.join(system_root, \"System32\", smi)\n",
    "        smis = [new_path, legacy_path]\n",
    "        for candidate_smi in smis:\n",
    "            if os.path.exists(candidate_smi):\n",
    "                smi = '\"{}\"'.format(candidate_smi)\n",
    "                break\n",
    "    return smi\n",
    "\n",
    "\n",
    "def get_platform():\n",
    "    if sys.platform.startswith(\"linux\"):\n",
    "        return \"linux\"\n",
    "    elif sys.platform.startswith(\"win32\"):\n",
    "        return \"win32\"\n",
    "    elif sys.platform.startswith(\"cygwin\"):\n",
    "        return \"cygwin\"\n",
    "    elif sys.platform.startswith(\"darwin\"):\n",
    "        return \"darwin\"\n",
    "    else:\n",
    "        return sys.platform\n",
    "\n",
    "\n",
    "def get_mac_version(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, \"sw_vers -productVersion\", r\"(.*)\")\n",
    "\n",
    "\n",
    "def get_windows_version(run_lambda):\n",
    "    system_root = os.environ.get(\"SYSTEMROOT\", \"C:\\\\Windows\")\n",
    "    wmic_cmd = os.path.join(system_root, \"System32\", \"Wbem\", \"wmic\")\n",
    "    findstr_cmd = os.path.join(system_root, \"System32\", \"findstr\")\n",
    "    return run_and_read_all(\n",
    "        run_lambda, \"{} os get Caption | {} /v Caption\".format(wmic_cmd, findstr_cmd)\n",
    "    )\n",
    "\n",
    "\n",
    "def get_lsb_version(run_lambda):\n",
    "    return run_and_parse_first_match(\n",
    "        run_lambda, \"lsb_release -a\", r\"Description:\\t(.*)\"\n",
    "    )\n",
    "\n",
    "\n",
    "def check_release_file(run_lambda):\n",
    "    return run_and_parse_first_match(\n",
    "        run_lambda, \"cat /etc/*-release\", r'PRETTY_NAME=\"(.*)\"'\n",
    "    )\n",
    "\n",
    "\n",
    "def get_os(run_lambda):\n",
    "    from platform import machine\n",
    "\n",
    "    platform = get_platform()\n",
    "\n",
    "    if platform == \"win32\" or platform == \"cygwin\":\n",
    "        return get_windows_version(run_lambda)\n",
    "\n",
    "    if platform == \"darwin\":\n",
    "        version = get_mac_version(run_lambda)\n",
    "        if version is None:\n",
    "            return None\n",
    "        return \"macOS {} ({})\".format(version, machine())\n",
    "\n",
    "    if platform == \"linux\":\n",
    "        # Ubuntu/Debian based\n",
    "        desc = get_lsb_version(run_lambda)\n",
    "        if desc is not None:\n",
    "            return \"{} ({})\".format(desc, machine())\n",
    "\n",
    "        # Try reading /etc/*-release\n",
    "        desc = check_release_file(run_lambda)\n",
    "        if desc is not None:\n",
    "            return \"{} ({})\".format(desc, machine())\n",
    "\n",
    "        return \"{} ({})\".format(platform, machine())\n",
    "\n",
    "    # Unknown platform\n",
    "    return platform\n",
    "\n",
    "\n",
    "def get_python_platform():\n",
    "    import platform\n",
    "\n",
    "    return platform.platform()\n",
    "\n",
    "\n",
    "def get_libc_version():\n",
    "    import platform\n",
    "\n",
    "    if get_platform() != \"linux\":\n",
    "        return \"N/A\"\n",
    "    return \"-\".join(platform.libc_ver())\n",
    "\n",
    "\n",
    "def get_pip_packages(run_lambda):\n",
    "    \"\"\"Returns `pip list` output. Note: will also find conda-installed pytorch\n",
    "    and numpy packages.\"\"\"\n",
    "    # People generally have `pip` as `pip` or `pip3`\n",
    "    # But here it is incoved as `python -mpip`\n",
    "    def run_with_pip(pip):\n",
    "        if get_platform() == \"win32\":\n",
    "            system_root = os.environ.get(\"SYSTEMROOT\", \"C:\\\\Windows\")\n",
    "            findstr_cmd = os.path.join(system_root, \"System32\", \"findstr\")\n",
    "            grep_cmd = r'{} /R \"numpy torch mypy\"'.format(findstr_cmd)\n",
    "        else:\n",
    "            grep_cmd = r'grep \"torch\\|numpy\\|mypy\"'\n",
    "        return run_and_read_all(run_lambda, pip + \" list --format=freeze | \" + grep_cmd)\n",
    "\n",
    "    pip_version = \"pip3\" if sys.version[0] == \"3\" else \"pip\"\n",
    "    out = run_with_pip(sys.executable + \" -mpip\")\n",
    "\n",
    "    return pip_version, out\n",
    "\n",
    "\n",
    "def get_cachingallocator_config():\n",
    "    ca_config = os.environ.get(\"PYTORCH_CUDA_ALLOC_CONF\", \"\")\n",
    "    return ca_config\n",
    "\n",
    "\n",
    "def get_env_info():\n",
    "    run_lambda = run\n",
    "    pip_version, pip_list_output = get_pip_packages(run_lambda)\n",
    "\n",
    "    if TORCH_AVAILABLE:\n",
    "        version_str = torch.__version__\n",
    "        debug_mode_str = str(torch.version.debug)\n",
    "        cuda_available_str = str(torch.cuda.is_available())\n",
    "        cuda_version_str = torch.version.cuda\n",
    "        if (\n",
    "            not hasattr(torch.version, \"hip\") or torch.version.hip is None\n",
    "        ):  # cuda version\n",
    "            hip_compiled_version = hip_runtime_version = miopen_runtime_version = \"N/A\"\n",
    "        else:  # HIP version\n",
    "            cfg = torch._C._show_config().split(\"\\n\")\n",
    "            hip_runtime_version = [\n",
    "                s.rsplit(None, 1)[-1] for s in cfg if \"HIP Runtime\" in s\n",
    "            ][0]\n",
    "            miopen_runtime_version = [\n",
    "                s.rsplit(None, 1)[-1] for s in cfg if \"MIOpen\" in s\n",
    "            ][0]\n",
    "            cuda_version_str = \"N/A\"\n",
    "            hip_compiled_version = torch.version.hip\n",
    "    else:\n",
    "        version_str = debug_mode_str = cuda_available_str = cuda_version_str = \"N/A\"\n",
    "        hip_compiled_version = hip_runtime_version = miopen_runtime_version = \"N/A\"\n",
    "\n",
    "    sys_version = sys.version.replace(\"\\n\", \" \")\n",
    "\n",
    "    return SystemEnv(\n",
    "        torch_version=version_str,\n",
    "        is_debug_build=debug_mode_str,\n",
    "        python_version=\"{} ({}-bit runtime)\".format(\n",
    "            sys_version, sys.maxsize.bit_length() + 1\n",
    "        ),\n",
    "        python_platform=get_python_platform(),\n",
    "        is_cuda_available=cuda_available_str,\n",
    "        cuda_compiled_version=cuda_version_str,\n",
    "        cuda_runtime_version=get_running_cuda_version(run_lambda),\n",
    "        nvidia_gpu_models=get_gpu_info(run_lambda),\n",
    "        nvidia_driver_version=get_nvidia_driver_version(run_lambda),\n",
    "        cudnn_version=get_cudnn_version(run_lambda),\n",
    "        hip_compiled_version=hip_compiled_version,\n",
    "        hip_runtime_version=hip_runtime_version,\n",
    "        miopen_runtime_version=miopen_runtime_version,\n",
    "        pip_version=pip_version,\n",
    "        pip_packages=pip_list_output,\n",
    "        conda_packages=get_conda_packages(run_lambda),\n",
    "        os=get_os(run_lambda),\n",
    "        libc_version=get_libc_version(),\n",
    "        gcc_version=get_gcc_version(run_lambda),\n",
    "        clang_version=get_clang_version(run_lambda),\n",
    "        cmake_version=get_cmake_version(run_lambda),\n",
    "        caching_allocator_config=get_cachingallocator_config(),\n",
    "    )\n",
    "\n",
    "\n",
    "env_info_fmt = \"\"\"\n",
    "PyTorch version: {torch_version}\n",
    "Is debug build: {is_debug_build}\n",
    "CUDA used to build PyTorch: {cuda_compiled_version}\n",
    "ROCM used to build PyTorch: {hip_compiled_version}\n",
    "\n",
    "OS: {os}\n",
    "GCC version: {gcc_version}\n",
    "Clang version: {clang_version}\n",
    "CMake version: {cmake_version}\n",
    "Libc version: {libc_version}\n",
    "\n",
    "Python version: {python_version}\n",
    "Python platform: {python_platform}\n",
    "Is CUDA available: {is_cuda_available}\n",
    "CUDA runtime version: {cuda_runtime_version}\n",
    "GPU models and configuration: {nvidia_gpu_models}\n",
    "Nvidia driver version: {nvidia_driver_version}\n",
    "cuDNN version: {cudnn_version}\n",
    "HIP runtime version: {hip_runtime_version}\n",
    "MIOpen runtime version: {miopen_runtime_version}\n",
    "\n",
    "Versions of relevant libraries:\n",
    "{pip_packages}\n",
    "{conda_packages}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def pretty_str(envinfo):\n",
    "    def replace_nones(dct, replacement=\"Could not collect\"):\n",
    "        for key in dct.keys():\n",
    "            if dct[key] is not None:\n",
    "                continue\n",
    "            dct[key] = replacement\n",
    "        return dct\n",
    "\n",
    "    def replace_bools(dct, true=\"Yes\", false=\"No\"):\n",
    "        for key in dct.keys():\n",
    "            if dct[key] is True:\n",
    "                dct[key] = true\n",
    "            elif dct[key] is False:\n",
    "                dct[key] = false\n",
    "        return dct\n",
    "\n",
    "    def prepend(text, tag=\"[prepend]\"):\n",
    "        lines = text.split(\"\\n\")\n",
    "        updated_lines = [tag + line for line in lines]\n",
    "        return \"\\n\".join(updated_lines)\n",
    "\n",
    "    def replace_if_empty(text, replacement=\"No relevant packages\"):\n",
    "        if text is not None and len(text) == 0:\n",
    "            return replacement\n",
    "        return text\n",
    "\n",
    "    def maybe_start_on_next_line(string):\n",
    "        # If `string` is multiline, prepend a \\n to it.\n",
    "        if string is not None and len(string.split(\"\\n\")) > 1:\n",
    "            return \"\\n{}\\n\".format(string)\n",
    "        return string\n",
    "\n",
    "    mutable_dict = envinfo._asdict()\n",
    "\n",
    "    # If nvidia_gpu_models is multiline, start on the next line\n",
    "    mutable_dict[\"nvidia_gpu_models\"] = maybe_start_on_next_line(\n",
    "        envinfo.nvidia_gpu_models\n",
    "    )\n",
    "\n",
    "    # If the machine doesn't have CUDA, report some fields as 'No CUDA'\n",
    "    dynamic_cuda_fields = [\n",
    "        \"cuda_runtime_version\",\n",
    "        \"nvidia_gpu_models\",\n",
    "        \"nvidia_driver_version\",\n",
    "    ]\n",
    "    all_cuda_fields = dynamic_cuda_fields + [\"cudnn_version\"]\n",
    "    all_dynamic_cuda_fields_missing = all(\n",
    "        mutable_dict[field] is None for field in dynamic_cuda_fields\n",
    "    )\n",
    "    if (\n",
    "        TORCH_AVAILABLE\n",
    "        and not torch.cuda.is_available()\n",
    "        and all_dynamic_cuda_fields_missing\n",
    "    ):\n",
    "        for field in all_cuda_fields:\n",
    "            mutable_dict[field] = \"No CUDA\"\n",
    "        if envinfo.cuda_compiled_version is None:\n",
    "            mutable_dict[\"cuda_compiled_version\"] = \"None\"\n",
    "\n",
    "    # Replace True with Yes, False with No\n",
    "    mutable_dict = replace_bools(mutable_dict)\n",
    "\n",
    "    # Replace all None objects with 'Could not collect'\n",
    "    mutable_dict = replace_nones(mutable_dict)\n",
    "\n",
    "    # If either of these are '', replace with 'No relevant packages'\n",
    "    mutable_dict[\"pip_packages\"] = replace_if_empty(mutable_dict[\"pip_packages\"])\n",
    "    mutable_dict[\"conda_packages\"] = replace_if_empty(mutable_dict[\"conda_packages\"])\n",
    "\n",
    "    # Tag conda and pip packages with a prefix\n",
    "    # If they were previously None, they'll show up as ie '[conda] Could not collect'\n",
    "    if mutable_dict[\"pip_packages\"]:\n",
    "        mutable_dict[\"pip_packages\"] = prepend(\n",
    "            mutable_dict[\"pip_packages\"], \"[{}] \".format(envinfo.pip_version)\n",
    "        )\n",
    "    if mutable_dict[\"conda_packages\"]:\n",
    "        mutable_dict[\"conda_packages\"] = prepend(\n",
    "            mutable_dict[\"conda_packages\"], \"[conda] \"\n",
    "        )\n",
    "    return env_info_fmt.format(**mutable_dict)\n",
    "\n",
    "\n",
    "def get_pretty_env_info():\n",
    "    return pretty_str(get_env_info())\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Collecting environment information...\")\n",
    "    output = get_pretty_env_info()\n",
    "    print(output)\n",
    "\n",
    "    if (\n",
    "        TORCH_AVAILABLE\n",
    "        and hasattr(torch, \"utils\")\n",
    "        and hasattr(torch.utils, \"_crash_handler\")\n",
    "    ):\n",
    "        minidump_dir = torch.utils._crash_handler.DEFAULT_MINIDUMP_DIR\n",
    "        if sys.platform == \"linux\" and os.path.exists(minidump_dir):\n",
    "            dumps = [\n",
    "                os.path.join(minidump_dir, dump) for dump in os.listdir(minidump_dir)\n",
    "            ]\n",
    "            latest = max(dumps, key=os.path.getctime)\n",
    "            ctime = os.path.getctime(latest)\n",
    "            creation_time = datetime.datetime.fromtimestamp(ctime).strftime(\n",
    "                \"%Y-%m-%d %H:%M:%S\"\n",
    "            )\n",
    "            msg = (\n",
    "                \"\\n*** Detected a minidump at {} created on {}, \".format(\n",
    "                    latest, creation_time\n",
    "                )\n",
    "                + \"if this is related to your bug please include it when you file a report ***\"\n",
    "            )\n",
    "            print(msg, file=sys.stderr)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59fa16-5cc8-4306-a0cd-7c43bef420b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
