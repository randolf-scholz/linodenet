{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "from math import prod, sqrt\n",
    "from typing import Optional, Protocol, Union, runtime_checkable\n",
    "\n",
    "import torch\n",
    "import torch.linalg\n",
    "from numpy.typing import NDArray\n",
    "from scipy import stats\n",
    "from torch import BoolTensor, Tensor, jit\n",
    "\n",
    "# import linodenet\n",
    "# from linodenet.constants import TRUE\n",
    "# from linodenet.projections import functional as projections\n",
    "# from linodenet.types import Device, Dtype, Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jit\n",
    "from torch import Tensor, jit\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "def wrapped_matrix_norm(x: Tensor, p: Union[int, str] = \"fro\") -> Tensor:\n",
    "    return torch.linalg.matrix_norm(r, ord=p)\n",
    "\n",
    "\n",
    "jit.script(wrapped_matrix_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit.script(matrix_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.matrix_norm(torch.randn(3, 3), ord=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = linodenet.initializations.canonical_skew_symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.compile(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.export.export(linodenet.initializations.canonical_skew_symmetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonical_skew_symmetric(\n",
    "    n: Shape, device: Device = None, dtype: Dtype = None\n",
    ") -> Tensor:\n",
    "    r\"\"\"Return the canonical skew symmetric matrix of size $n=2k$.\n",
    "\n",
    "    .. math:: ð•_n = ð•€_n âŠ— \\begin{bmatrix}0 & +1 \\\\ -1 & 0\\end{bmatrix}\n",
    "\n",
    "    Normalized such that if $xâˆ¼ð“(0,1)$, then $Aâ‹…xâˆ¼ð“(0,1)$.\n",
    "    \"\"\"\n",
    "    # convert to tuple\n",
    "    tup = (n,) if isinstance(n, int) else tuple(n)\n",
    "    dim, size = tup[-1], tup[:-1]\n",
    "    assert dim % 2 == 0, \"The dimension must be divisible by 2!\"\n",
    "    dim //= 2\n",
    "\n",
    "    J1 = torch.tensor([[0, 1], [-1, 0]], device=device, dtype=dtype)\n",
    "    eye = torch.eye(dim, device=device, dtype=dtype)\n",
    "    J = torch.kron(J1, eye)\n",
    "    # ones = torch.ones(size, device=device, dtype=dtype)\n",
    "    return J.repeat(size)\n",
    "    # return torch.einsum(\"..., de -> ...de\", ones, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(x: Tensor, *, c: float) -> Tensor:\n",
    "    return x * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit.script(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(1, 2, 3, 4) @ torch.randn(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "linodenet.initializations.canonical_skew_symmetric(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "linodenet.initializations.canonical_symplectic(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Linear(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Mapping\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "\n",
    "\n",
    "class Foo(nn.Module, Mapping):\n",
    "    tensors: dict[str, tensor]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensors)\n",
    "\n",
    "    def __iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pyflyby-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "from pkg_resources import load_entry_point\n",
    "from torch import Tensor, jit, nn\n",
    "\n",
    "from linodenet.lib import singular_triplet\n",
    "from linodenet.parametrize import SimpleParametrization, SpectralNormalization\n",
    "from linodenet.projections import is_symmetric, symmetric\n",
    "from linodenet.testing import check_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from torch.linalg import matrix_norm\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model, parametrization and inputs\n",
    "inputs = torch.randn(2, 3)\n",
    "model = nn.Linear(3, 3)\n",
    "weight = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "with torch.no_grad():\n",
    "    model.weight.copy_(weight)\n",
    "    assert matrix_norm(model.weight, ord=2) > 1\n",
    "\n",
    "print(f\"Original weight = {model.weight}\")\n",
    "print(f\"Original norm =  {matrix_norm(model.weight, ord=2)}\")\n",
    "\n",
    "spec = SpectralNormalization(model.weight)\n",
    "spec.weight, spec.original_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec.weight.norm().backward()\n",
    "# spec.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_norm(spec.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spec.reset_cache()  # <--- never forget\n",
    "assert spec.weight is spec.cached_tensors[\"weight\"]\n",
    "# assert spec.parametrized_tensor[\"weight\"]\n",
    "# cloned_model = deepcopy(model)\n",
    "\n",
    "# register the parametrization\n",
    "model.register_module(\"spec\", spec)\n",
    "# remove the weight attribute (it still exists on the parametrization)\n",
    "del model.weight\n",
    "\n",
    "# register the parametrization's weight-buffer as a buffer\n",
    "model.register_buffer(\"weight\", model.spec.cached_tensors[\"weight\"])\n",
    "\n",
    "# register the parametrization's weight as a parameter (optional)\n",
    "model.register_parameter(\n",
    "    \"parametrized_weight\", model.spec.parametrized_tensors[\"weight\"]\n",
    ")\n",
    "model.weight.norm().backward()\n",
    "model.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = SGD(model.spec.parameters(), lr=0.1)\n",
    "assert model.weight is model.spec.weight\n",
    "assert model.parametrized_weight is model.spec.parametrized_tensors[\"weight\"]\n",
    "assert matrix_norm(model.weight, ord=2) <= 1\n",
    "model.weight, model.parametrized_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec.reset_cache()\n",
    "model.weight, model.parametrized_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec.original_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec.weight.detach_()\n",
    "spec.weight.copy_(spec.original_weight)\n",
    "spec.weight.norm().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad(set_to_none=True)\n",
    "r = -model(inputs).norm()\n",
    "r.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight.grad, model.parametrized_weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(model.named_buffers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec.reset_cache()  # <--- never forget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.zero_grad(set_to_none=True)\n",
    "# model.spec.reset_cache()\n",
    "r = -model(inputs).norm()\n",
    "r.backward()\n",
    "# print(model.weight.grad, model.parametrized_weight.grad)\n",
    "\n",
    "cached_weigth_before = model.weight.clone()\n",
    "params_weight_before = model.parametrized_weight.clone()\n",
    "assert (cached_weigth_before == params_weight_before).all()\n",
    "\n",
    "# perform a step\n",
    "optim.step()\n",
    "cached_weigth_step = model.weight.clone()\n",
    "params_weight_step = model.parametrized_weight.clone()\n",
    "assert (cached_weigth_before == cached_weigth_step).all()\n",
    "assert not (params_weight_before == params_weight_step).all()\n",
    "assert not (params_weight_step == cached_weigth_step).all()\n",
    "\n",
    "# update the chaches\n",
    "model.spec.reset_cache()\n",
    "cached_weigth_update = model.weight.clone()\n",
    "params_weight_update = model.parametrized_weight.clone()\n",
    "assert model.weight is model.spec.weight\n",
    "assert model.parametrized_weight is model.spec.parametrized_tensors[\"weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_weigth_step == cached_weigth_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_weigth_update == params_weight_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not (cached_weigth_step == cached_weigth_update).all()\n",
    "assert (cached_weigth_update == params_weight_update).all()\n",
    "\n",
    "\n",
    "# after = torch.cat([model.weight.clone(), model.parametrized_weight.clone()], dim=-1)\n",
    "# assert not torch.allclose(before, after)\n",
    "# print(before - after)\n",
    "# print(torch.cat([model.weight, model.parametrized_weight], dim=-1))\n",
    "#\n",
    "# print(torch.cat([model.weight, model.parametrized_weight], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_weigth_step, cached_weigth_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_weight_step, params_weight_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.spec.parametrized_tensor[\"weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Step 1 --------------\")\n",
    "print(torch.cat([model.weight, model.parametrized_weight], dim=-1))\n",
    "print(matrix_norm(model.weight, ord=2))\n",
    "model.zero_grad(set_to_none=True)\n",
    "r = model(inputs).norm()\n",
    "r.backward()\n",
    "# assert model.parametrized_weight.grad is not None\n",
    "optim.step()\n",
    "model.spec.reset_cache()\n",
    "print(\"Recompute... --------------\")\n",
    "print(torch.cat([model.weight, model.parametrized_weight], dim=-1))\n",
    "print(matrix_norm(model.weight, ord=2))\n",
    "print(torch.cat([model.weight, model.parametrized_weight], dim=-1))\n",
    "print(matrix_norm(model.weight, ord=2))\n",
    "print(\"Step 2 --------------\")\n",
    "print(torch.cat([model.weight, model.parametrized_weight], dim=-1))\n",
    "print(matrix_norm(model.weight, ord=2))\n",
    "model.zero_grad(set_to_none=True)\n",
    "r = model(inputs).norm()\n",
    "r.backward()\n",
    "# assert model.parametrized_weight.grad is not None\n",
    "optim.step()\n",
    "model.spec.reset_cache()\n",
    "print(\"Recompute... --------------\")\n",
    "print(torch.cat([model.weight, model.parametrized_weight], dim=-1))\n",
    "print(torch.cat([model.spec.weight, model.spec.parametrized_tensor[\"weight\"]], dim=-1))\n",
    "print(matrix_norm(model.weight, ord=2))\n",
    "print(torch.cat([model.weight, model.parametrized_weight], dim=-1))\n",
    "print(matrix_norm(model.weight, ord=2))\n",
    "print(\"Step 3 --------------\")\n",
    "print(torch.cat([model.weight, model.parametrized_weight], dim=-1))\n",
    "print(matrix_norm(model.weight, ord=2))\n",
    "model.zero_grad(set_to_none=True)\n",
    "r = model(inputs).norm()\n",
    "r.backward()\n",
    "optim.step()\n",
    "model.spec.reset_cache()\n",
    "print(\"Recompute... --------------\")\n",
    "print(torch.cat([model.weight, model.parametrized_weight], dim=-1))\n",
    "print(matrix_norm(model.weight, ord=2))\n",
    "print(torch.cat([model.weight, model.parametrized_weight], dim=-1))\n",
    "print(matrix_norm(model.weight, ord=2))\n",
    "\n",
    "\n",
    "assert model.parametrized_weight is model.spec.parametrized_tensor[\"weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad(set_to_none=True)\n",
    "r = -model(inputs).norm()\n",
    "r.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parametrized_weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weight.grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model = nn.Linear(3, 3)\n",
    "    display(\n",
    "        id(model.weight),\n",
    "    )\n",
    "    spec = SpectralNormalization(model.weight)\n",
    "    model.spec = spec\n",
    "\n",
    "    del model.weight\n",
    "    model.register_parameter(\n",
    "        \"parametrized_weight\", model.spec.parametrized_tensor[\"weight\"]\n",
    "    )\n",
    "\n",
    "    model.register_buffer(\"weight\", model.spec.weight)\n",
    "    # model.weight.copy_(model.parametrized_weight)\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    display(\n",
    "        f\"{id(spec.weight)=}\",\n",
    "        spec.weight,\n",
    "        f\"{id(spec.parametrized_tensor['weight'])=}\",\n",
    "        spec.parametrized_tensor[\"weight\"],\n",
    "        spec.parametrized_tensor[\"weight\"].grad,\n",
    "    )\n",
    "    print(\"-\" * 80)\n",
    "    display(\n",
    "        id(model.weight),\n",
    "        model.weight,\n",
    "        id(model.parametrized_weight),\n",
    "        model.parametrized_weight,\n",
    "        model.parametrized_weight.grad,\n",
    "    )\n",
    "\n",
    "    spec.recompute_cache()\n",
    "\n",
    "\n",
    "inputs = torch.randn(2, 3)\n",
    "r = model(inputs)\n",
    "r.norm().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_model(model, input_args=inputs, test_jit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parametrized_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from linodenet.models.encoders.invertible_layers import LinearContraction\n",
    "from linodenet.parametrize import SpectralNormalization\n",
    "\n",
    "model = LinearContraction(4, 4)\n",
    "\n",
    "print(model.weight, model.cached_weight)\n",
    "model.recompute_cache()\n",
    "print(model.weight, model.cached_weight)\n",
    "\n",
    "print(\"==============================================================\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = nn.Linear(4, 4)\n",
    "param = nn.Parameter(model.weight.clone().detach() * 2)\n",
    "spec = SpectralNormalization(param)\n",
    "\n",
    "print(spec.parametrized_tensor[\"weight\"], spec.weight, sep=\"\\n\")\n",
    "assert spec.parametrized_tensor[\"weight\"] is param\n",
    "assert spec.weight is spec.cached_tensors[\"weight\"]\n",
    "print(\"==============================================================\")\n",
    "\n",
    "spec.cached_tensors[\"weight\"].copy_(spec.parametrized_tensor[\"weight\"])\n",
    "\n",
    "print(spec.parametrized_tensor[\"weight\"], spec.weight, sep=\"\\n\")\n",
    "assert spec.parametrized_tensor[\"weight\"] is param\n",
    "assert spec.weight is spec.cached_tensors[\"weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
