{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from contextlib import AbstractContextManager\n",
    "from typing import Protocol, runtime_checkable\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "\n",
    "from linodenet.lib import singular_triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import jit, nn\n",
    "from linodenet.testing import test_model\n",
    "\n",
    "A = nn.Linear(4, 4)\n",
    "P = nn.Linear(4, 4)\n",
    "X = torch.randn(3, 4)\n",
    "A.weight = P.weight\n",
    "scripted = jit.script(A)\n",
    "scripted(X)\n",
    "\n",
    "test_model(A, inputs=X, test_jit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m, n = 5, 3\n",
    "A = torch.randn(m, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A.layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Parametrization(nn.Module):\n",
    "    \"\"\"A parametrized tensor.\"\"\"\n",
    "\n",
    "    parametrized_tensors: dict[str, Tensor]\n",
    "    cached_tensors: dict[str, Tensor]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def register_parametrization(self, name: str, param: nn.Parameter) -> None:\n",
    "        \"\"\"Register a parametrization.\"\"\"\n",
    "        if not isinstance(param, nn.Parameter):\n",
    "            raise ValueError(f\"Given tensor is not a nn.Parameter!\")\n",
    "\n",
    "        # create the cached tensor.\n",
    "        self.register_cached_tensor(f\"cached_{name}\", torch.empty_like(param))\n",
    "\n",
    "        # register the parametrization.\n",
    "        self.parametrized_tensors[name] = param\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def register_cached_tensor(self, name: str, tensor: Tensor) -> None:\n",
    "        \"\"\"Register a cached tensor.\"\"\"\n",
    "        if name in self.cached_tensors:\n",
    "            raise ValueError(f\"Cache with {name=!r} already registered!\")\n",
    "        if name in self.named_buffers():\n",
    "            raise ValueError(f\"Buffer with {name=!r} already taken!\")\n",
    "\n",
    "        self.register_buffer(name, tensor)\n",
    "        self.cached_tensors[name] = getattr(self, name)\n",
    "\n",
    "    def recompute_cache(self) -> None:\n",
    "        # Compute the cached weight matrix\n",
    "        new_tensors = self.forward()\n",
    "\n",
    "        # copy the new tensors into the cache\n",
    "        for key, tensor in new_tensors.items():\n",
    "            self.cached_tensors[key].copy_(tensor)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def projection(self) -> None:\n",
    "        # update the cached weight matrix\n",
    "        self.recompute_cache()\n",
    "\n",
    "        # copy the cached values into the parametrized tensors\n",
    "        for key, tensor in self.parametrized_tensors.items():\n",
    "            tensor.copy_(self.cached_tensors[key])\n",
    "\n",
    "    def reset_cache(self) -> None:\n",
    "        # apply projection step.\n",
    "        self.projection()\n",
    "\n",
    "        # reengage the autograd engine\n",
    "        # detach() is necessary to avoid \"Trying to backward through the graph a second time\" error\n",
    "        for key, tensor in self.cached_tensors.items():\n",
    "            tensor.detach_()\n",
    "\n",
    "        # recompute the cache\n",
    "        # Note: we need the second run to set up the gradients\n",
    "        self.recompute_cache()\n",
    "\n",
    "    def reset_cache_expanded(self) -> None:\n",
    "        # apply projection step.\n",
    "        with torch.no_grad():\n",
    "            # update the cached weight matrix\n",
    "            new_tensors = self.forward()\n",
    "\n",
    "            if new_tensors.keys() != self.cached_tensors.keys():\n",
    "                raise ValueError(\n",
    "                    f\"{new_tensors.keys()=} != {self.cached_tensors.keys()=}\"\n",
    "                )\n",
    "\n",
    "            # copy the new tensors into the cache\n",
    "            for key, tensor in new_tensors.items():\n",
    "                self.cached_tensors[key].copy_(tensor)\n",
    "\n",
    "        # copy the cached values into the parametrized tensors\n",
    "        for key, tensor in self.parametrized_tensors.items():\n",
    "            tensor.copy_(self.cached_tensors[key])\n",
    "\n",
    "        # reengage the autograd engine\n",
    "        # detach() is necessary to avoid \"Trying to backward through the graph a second time\" error\n",
    "        for key, tensor in self.cached_tensors.items():\n",
    "            tensor.detach_()\n",
    "\n",
    "        # recompute the cache\n",
    "        # Note: we need the second run to set up the gradients\n",
    "        # Compute the cached weight matrix\n",
    "        new_tensors = self.forward()\n",
    "\n",
    "        # copy the new tensors into the cache\n",
    "        for key, tensor in new_tensors.items():\n",
    "            self.cached_tensors[key].copy_(tensor)\n",
    "\n",
    "\n",
    "class SpectralNormalization(Parametrization):\n",
    "    \"\"\"Spectral normalization.\"\"\"\n",
    "\n",
    "    # constants\n",
    "    GAMMA: Tensor\n",
    "    ONE: Tensor\n",
    "\n",
    "    # cached\n",
    "    u: Tensor\n",
    "    v: Tensor\n",
    "    sigma: Tensor\n",
    "\n",
    "    # parametrized\n",
    "    weight: Tensor\n",
    "\n",
    "    def __init__(self, weight: nn.Parameter, /, gamma: float = 1.0) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(weight.shape) == 2\n",
    "        m, n = weight.shape\n",
    "\n",
    "        options = {\n",
    "            \"dtype\": weight.dtype,\n",
    "            \"device\": weight.device,\n",
    "            \"layout\": weight.layout,\n",
    "        }\n",
    "\n",
    "        # parametrized and cached\n",
    "        self.register_parametrization(\"weight\", weight)\n",
    "        self.register_cached_tensor(\"u\", torch.empty(m, **options))\n",
    "        self.register_cached_tensor(\"v\", torch.empty(n, **options))\n",
    "        self.register_cached_tensor(\"sigma\", torch.empty(1, **options))\n",
    "\n",
    "        # constants\n",
    "        self.register_buffer(\"ONE\", torch.empty(1, **options))\n",
    "        self.register_buffer(\"GAMMA\", torch.empty(gamma, **options))\n",
    "\n",
    "    def forward(self) -> dict[str, Tensor]:\n",
    "        sigma, u, v = singular_triplet(self.weight, u0=self.u, v0=self.v)\n",
    "        gamma = torch.minimum(self.ONE, self.GAMMA / sigma)\n",
    "        weight = gamma * self.weight\n",
    "        return {\"weight\": weight, \"u\": u, \"v\": v, \"sigma\": sigma}\n",
    "\n",
    "\n",
    "def reset_all_caches(module: nn.Module) -> None:\n",
    "    \"\"\"Reset all caches in a module.\"\"\"\n",
    "    for submodule in module.modules():\n",
    "        if isinstance(submodule, ParametrizationABC):\n",
    "            submodule.reset_cache()\n",
    "\n",
    "\n",
    "class reset_caches(AbstractContextManager):\n",
    "    \"\"\"reset_caches context manager.\"\"\"\n",
    "\n",
    "    def __init__(self, module: nn.Module) -> None:\n",
    "        self.module = module\n",
    "\n",
    "    def __enter__(self):\n",
    "        reset_all_caches(self.module)\n",
    "        return self.module\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        reset_all_caches(self.module)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "next(t.device for t in A.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
