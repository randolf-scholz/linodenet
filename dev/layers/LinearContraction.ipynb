{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'  # always print last expr.\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, jit, nn\n",
    "from torch.linalg import matrix_norm\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import SGD\n",
    "\n",
    "from linodenet.lib import singular_triplet\n",
    "from linodenet.models.encoders.invertible_layers import (\n",
    "    LinearContraction,\n",
    "    iResNetBlock,\n",
    "    iSequential,\n",
    ")\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test simple LinearContraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N, m, n = 32, 256, 256\n",
    "\n",
    "x = torch.randn(m)\n",
    "X = torch.randn(N, m)\n",
    "model = jit.script(LinearContraction(m, n))\n",
    "\n",
    "mem_params = sum([\n",
    "    param.nelement() * param.element_size() for param in model.parameters()\n",
    "])\n",
    "mem_bufs = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "print(f\"{(mem_params + mem_bufs) // (1024**2)} MiB\")\n",
    "\n",
    "jit.save(model, \"LinearContraction.pt\")\n",
    "# model = jit.load(\"model.pt\")\n",
    "\n",
    "optim = SGD(model.parameters(), lr=0.5)\n",
    "# print(model.weight)\n",
    "# print(model.cached_weight)\n",
    "# model.reset_cache()\n",
    "# print(model.cached_weight)\n",
    "# print(model.cached_weight)\n",
    "# model.reset_cache()\n",
    "print(model.sigma)\n",
    "print(matrix_norm(model.weight, ord=2))\n",
    "print(matrix_norm(model.cached_weight, ord=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in range(3):\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    # y = -F.linear(x, model.cached_weight).norm()\n",
    "    y = -model(x).norm()\n",
    "    y.backward()\n",
    "    optim.step()\n",
    "    print(f\"{k=} {y.item()} ============ \")\n",
    "    model.reset_cache()\n",
    "    print(model.sigma)\n",
    "    print(matrix_norm(model.weight, ord=2))\n",
    "    print(matrix_norm(model.cached_weight, ord=2))\n",
    "\n",
    "# model.reset_cache()\n",
    "# print(model.sigma)\n",
    "# print(matrix_norm(model.weight, ord=2))\n",
    "# print(matrix_norm(model.cached_weight, ord=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    LinearContraction(m, n),\n",
    "    LinearContraction(n, m),\n",
    ")\n",
    "model = jit.script(model)\n",
    "\n",
    "mem_params = sum([\n",
    "    param.nelement() * param.element_size() for param in model.parameters()\n",
    "])\n",
    "mem_bufs = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "print(f\"{(mem_params + mem_bufs) // (1024**2)} MiB\")\n",
    "\n",
    "jit.save(model, \"sequential.pt\")\n",
    "\n",
    "optim = SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "\n",
    "def reset_caches(module):\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"reset_cache\"):\n",
    "            m.reset_cache()\n",
    "\n",
    "\n",
    "def show_params(module):\n",
    "    for m in model.modules():\n",
    "        if m.original_name == \"LinearContraction\":\n",
    "            print(m.sigma)\n",
    "            print(matrix_norm(m.weight, ord=2))\n",
    "            print(matrix_norm(m.cached_weight, ord=2))\n",
    "\n",
    "\n",
    "show_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in range(3):\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    y = -model(x).norm()\n",
    "    y.backward()\n",
    "    optim.step()\n",
    "    print(f\"{k=} ============ {y.item()}\")\n",
    "    reset_caches(model)\n",
    "    show_params(model)\n",
    "    # model.reset_cache()\n",
    "    # print(model.sigma)\n",
    "    # print(matrix_norm(model.weight, ord=2))\n",
    "    # print(matrix_norm(model.cached_weight, ord=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test iResNetBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def surgery(model):\n",
    "    print(\"Applying Surgery!!!\")\n",
    "    with torch.no_grad():\n",
    "        inner = list(model.block.modules())[1:]\n",
    "        outer = list(model.inverse.block.modules())[1:]\n",
    "\n",
    "        for layer, other in zip(inner, outer):\n",
    "            other.weight = layer.weight\n",
    "            other.bias = layer.bias\n",
    "            other.cached_weight = layer.cached_weight\n",
    "            other.sigma = layer.sigma\n",
    "            other.u = layer.u\n",
    "            other.v = layer.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inner_model = nn.Sequential(\n",
    "    LinearContraction(m, n),\n",
    "    LinearContraction(n, m),\n",
    ")\n",
    "\n",
    "model = iResNetBlock(inner_model)\n",
    "model = jit.script(model)\n",
    "\n",
    "jit.save(model, \"iREsNetBlock.pt\")\n",
    "model = jit.load(\"iREsNetBlock.pt\")\n",
    "\n",
    "mem_params = sum([\n",
    "    param.nelement() * param.element_size() for param in model.parameters()\n",
    "])\n",
    "mem_bufs = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "print(f\"{(mem_params + mem_bufs) // (1024**2)} MiB\")\n",
    "optim = SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "\n",
    "def reset_caches(module):\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"reset_cache\"):\n",
    "            m.reset_cache()\n",
    "\n",
    "\n",
    "def show_params(module):\n",
    "    for m in model.modules():\n",
    "        if (\n",
    "            getattr(m, \"original_name\", False)\n",
    "            or getattr(m.__class__, \"__name__\", False)\n",
    "        ) == \"LinearContraction\":\n",
    "            print(m.sigma)\n",
    "            print(matrix_norm(m.weight, ord=2))\n",
    "            print(matrix_norm(m.cached_weight, ord=2))\n",
    "\n",
    "\n",
    "show_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in range(3):\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    y = -model(x).norm()\n",
    "    y.backward()\n",
    "    optim.step()\n",
    "    print(f\"{k=} ============ {y.item()}\")\n",
    "    reset_caches(model)\n",
    "    print(\"~~~~ Encoder Params ~~~~~\")\n",
    "    show_params(model.block)\n",
    "    print(\"~~~~ Decoder Params ~~~~~\")\n",
    "    show_params(model.inverse.block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHECK IF encoder.weight \"IS\" decoder.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder_layers = list(model.block.modules())[1:]\n",
    "decoder_layers = list(model.block.modules())[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for layer, other in zip(encoder_layers, decoder_layers):\n",
    "    assert layer.weight is other.weight\n",
    "    assert layer.sigma is other.sigma\n",
    "    assert layer.cached_weight is other.cached_weight\n",
    "    assert layer.u is other.u\n",
    "    assert layer.v is other.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
