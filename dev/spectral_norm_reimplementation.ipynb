{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'  # always print last expr.\n",
    "\n",
    "\n",
    "from math import sqrt\n",
    "from typing import Any, Final, Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, jit, nn\n",
    "from torch.linalg import matrix_norm\n",
    "from torch.nn import functional\n",
    "from torch.optim import SGD\n",
    "\n",
    "\n",
    "class iResNetLayer(nn.Module):\n",
    "    maxiter: Final[int]\n",
    "    r\"\"\"CONST: Maximum number of steps in power-iteration\"\"\"\n",
    "    atol: Final[float]\n",
    "    r\"\"\"CONST: Absolute tolerance for fixed point iteration\"\"\"\n",
    "    rtol: Final[float]\n",
    "    r\"\"\"CONST: Relative tolerance for fixed point iteration\"\"\"\n",
    "    converged: Tensor\n",
    "    r\"\"\"BUFFER: Boolean tensor indicating convergence\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer: nn.Module,\n",
    "        *,\n",
    "        maxiter: int = 1000,\n",
    "        atol: float = 1e-8,\n",
    "        rtol: float = 1e-5,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.maxiter = maxiter\n",
    "        self.atol = atol\n",
    "        self.rtol = rtol\n",
    "        self.register_buffer(\"converged\", torch.tensor(False))\n",
    "\n",
    "    @jit.export\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x + self.layer(x)\n",
    "\n",
    "    @jit.export\n",
    "    def inverse(self, y: Tensor) -> Tensor:\n",
    "        r\"\"\"Compute the inverse through fixed point iteration.\n",
    "\n",
    "        Terminates once ``maxiter`` or tolerance threshold\n",
    "        $|x'-x|≤\\text{atol} + \\text{rtol}⋅|x|$ is reached.\n",
    "        \"\"\"\n",
    "        x = y.clone()\n",
    "        residual = torch.zeros_like(y)\n",
    "\n",
    "        for it in range(self.maxiter):\n",
    "            x_prev = x\n",
    "            x = y - self.layer(x)\n",
    "            residual = (x - x_prev).norm()\n",
    "            self.converged = residual < self.atol + self.rtol * x_prev.norm()\n",
    "            if self.converged:\n",
    "                # print(f\"Converged in {it} iterations.\")\n",
    "                break\n",
    "        if not self.converged:\n",
    "            print(\n",
    "                f\"No convergence in {self.maxiter} iterations. \"\n",
    "                f\"Max residual:{residual} > {self.atol}.\"\n",
    "            )\n",
    "        return x\n",
    "\n",
    "\n",
    "class OptimizediResNetLayer(nn.Module):\n",
    "    maxiter: Final[int]\n",
    "    r\"\"\"CONST: Maximum number of steps in power-iteration\"\"\"\n",
    "    atol: Final[float]\n",
    "    r\"\"\"CONST: Absolute tolerance for fixed point iteration\"\"\"\n",
    "    rtol: Final[float]\n",
    "    r\"\"\"CONST: Relative tolerance for fixed point iteration\"\"\"\n",
    "    converged: Tensor\n",
    "    r\"\"\"BUFFER: Boolean tensor indicating convergence\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        m: int,\n",
    "        *,\n",
    "        maxiter: int = 1000,\n",
    "        atol: float = 1e-8,\n",
    "        rtol: float = 1e-5,\n",
    "        bias: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(m, m, bias=bias)\n",
    "        self.maxiter = maxiter\n",
    "        self.atol = atol\n",
    "        self.rtol = rtol\n",
    "        self.register_buffer(\"converged\", torch.tensor(False))\n",
    "\n",
    "    @jit.export\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        sigma = matrix_norm(self.layer.weight, ord=2)\n",
    "        cached_weight = self.layer.weight / sigma\n",
    "        return x + functional.linear(x, cached_weight, self.layer.bias)\n",
    "\n",
    "    @jit.export\n",
    "    def inverse(self, y: Tensor) -> Tensor:\n",
    "        r\"\"\"Compute the inverse through fixed point iteration.\n",
    "\n",
    "        Terminates once ``maxiter`` or tolerance threshold\n",
    "        $|x'-x|≤\\text{atol} + \\text{rtol}⋅|x|$ is reached.\n",
    "        \"\"\"\n",
    "        x = y.clone()\n",
    "        residual = torch.zeros_like(y)\n",
    "        sigma = matrix_norm(self.layer.weight, ord=2)\n",
    "        cached_weight = self.layer.weight / sigma\n",
    "\n",
    "        for it in range(self.maxiter):\n",
    "            x_prev = x\n",
    "            x = y - functional.linear(x, cached_weight, self.layer.bias)\n",
    "            residual = (x - x_prev).norm()\n",
    "            self.converged = residual < self.atol + self.rtol * x_prev.norm()\n",
    "            if self.converged:\n",
    "                # print(f\"Converged in {it} iterations.\")\n",
    "                break\n",
    "        if not self.converged:\n",
    "            print(\n",
    "                f\"No convergence in {self.maxiter} iterations. \"\n",
    "                f\"Max residual:{residual} > {self.atol}.\"\n",
    "            )\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinearContraction(nn.Module):\n",
    "    # Constants\n",
    "    input_size: Final[int]\n",
    "    output_size: Final[int]\n",
    "    c: Final[float]\n",
    "    r\"\"\"CONST: The maximal Lipschitz constant.\"\"\"\n",
    "    one: Tensor\n",
    "    r\"\"\"CONST: A tensor with value 1.0\"\"\"\n",
    "\n",
    "    # Buffers\n",
    "    cached_sigma: Tensor\n",
    "    r\"\"\"BUFFER: Cached value of $‖W‖_2$\"\"\"\n",
    "    cached_weight: Tensor\n",
    "    r\"\"\"BUFFER: Cached value of $W$/‖W‖₂.\"\"\"\n",
    "    refresh_cache: Tensor\n",
    "    r\"\"\"BUFFER: A boolean tensor indicating whether to recompute $‖W‖_2$\"\"\"\n",
    "    u: Tensor\n",
    "    r\"\"\"BUFFER: Cached left singular vector of $W$.\"\"\"\n",
    "    v: Tensor\n",
    "    r\"\"\"BUFFER: Cached right singular vector of $W$.\"\"\"\n",
    "\n",
    "    # Parameters\n",
    "    weight: Tensor\n",
    "    r\"\"\"PARAM: The weight matrix.\"\"\"\n",
    "    bias: Optional[Tensor]\n",
    "    r\"\"\"PARAM: The bias term.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, input_size: int, output_size: int, *, c: float = 1.0, bias: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weight = nn.Parameter(Tensor(output_size, input_size))\n",
    "        self.c = c\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(Tensor(output_size))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "        self.register_buffer(\"one\", torch.tensor(1.0), persistent=True)\n",
    "        # self.register_buffer(\"c\", torch.tensor(float(c)), persistent=True)\n",
    "        self.register_buffer(\n",
    "            \"spectral_norm\", matrix_norm(self.weight, ord=2), persistent=False\n",
    "        )\n",
    "        self.register_buffer(\"cached_sigma\", torch.tensor(1.0))\n",
    "        self.register_buffer(\"cached_weight\", self.weight.clone())\n",
    "        self.register_buffer(\"refresh_cache\", torch.tensor(True))\n",
    "\n",
    "        self.register_forward_pre_hook(self.__renormalize_weight)\n",
    "        self.register_full_backward_pre_hook(self.raise_flag)\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        r\"\"\"Reset both weight matrix and bias vector.\"\"\"\n",
    "        nn.init.kaiming_uniform_(self.weight, a=sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            bound = 1 / sqrt(self.input_size)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    # @jit.export\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        r\"\"\".. Signature:: ``(..., n) -> (..., n)``.\"\"\"\n",
    "        return functional.linear(x, self.cached_weight, self.bias)\n",
    "\n",
    "    # @jit.export\n",
    "    def renormalize_weight(self, inputs: tuple[Tensor] = (torch.tensor([]),)) -> None:\n",
    "        \"\"\"Renormalizes weight so that ‖W‖₂ ≤ 1-δ.\"\"\"\n",
    "        if self.refresh_cache:\n",
    "            self.refresh_cache = torch.tensor(False)\n",
    "            self.cached_sigma = matrix_norm(self.weight, ord=2)\n",
    "            gamma = 1 / self.cached_sigma\n",
    "            self.cached_weight = gamma * self.weight\n",
    "            # self.weight[:] = gamma * self.weight\n",
    "\n",
    "    @staticmethod\n",
    "    def __renormalize_weight(self, inputs: tuple[Tensor]) -> None:\n",
    "        return self.renormalize_weight()\n",
    "\n",
    "    @staticmethod\n",
    "    def raise_flag(self, grad_output: list[Tensor]) -> None:\n",
    "        # pass  # WTF! just pass will throw an error?!\n",
    "        # print(\"here!\")\n",
    "        self.refresh_cache = torch.tensor(True)\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #      self.refresh_cache = torch.tensor(True)\n",
    "\n",
    "    #     @staticmethod\n",
    "    #     def __raise_refresh_cache_flag(self, grad_output: list[Tensor] = ()) -> None:\n",
    "    #         self.raise_refresh_cache_flag()\n",
    "\n",
    "    #     # @jit.export\n",
    "    #     def raise_refresh_cache_flag(self) -> None:\n",
    "    #         # print(grad_output)\n",
    "    #         # self.refresh_cache = torch.tensor(True)\n",
    "    #         # self.cached_weight = torch.tensor([])\n",
    "    #         # self.cached_sigma = torch.tensor([])\n",
    "    #         pass\n",
    "\n",
    "    # fac = 1.0 / (self.c + self.sigma_cached)\n",
    "    # return functional.linear(x, fac * self.weight, self.bias)\n",
    "\n",
    "\n",
    "class NaiveContraction(nn.Module):\n",
    "    # Parameters\n",
    "    weight: Tensor\n",
    "    r\"\"\"PARAM: The weight matrix.\"\"\"\n",
    "    bias: Optional[Tensor]\n",
    "    r\"\"\"PARAM: The bias term.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, input_size: int, output_size: int, *, c: float = 1.0, bias: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(input_size, output_size, bias=bias)\n",
    "        self.weight = self.layer.weight\n",
    "        self.bias = self.layer.bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        sigma = matrix_norm(self.weight, ord=2)\n",
    "        return functional.linear(x, self.weight / sigma, self.bias)\n",
    "        # return self.layer(x / sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Foo(nn.Module):\n",
    "    bflag: Tensor\n",
    "    fflag: Tensor\n",
    "\n",
    "    def __init__(self, m: int) -> None:\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(m, m)\n",
    "        self.register_buffer(\"bflag\", torch.tensor(False))\n",
    "        self.register_buffer(\"fflag\", torch.tensor(False))\n",
    "        self.register_forward_pre_hook(self.__fhook)\n",
    "        self.register_full_backward_pre_hook(self.__bhook)\n",
    "        self.register_state_dict_pre_hook(self.__shook)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.layer(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def __fhook(self, inputs: tuple[Tensor]) -> None:\n",
    "        return self.fhook(inputs)\n",
    "\n",
    "    @staticmethod\n",
    "    def __bhook(self, grad_output) -> None:\n",
    "        return self.bhook(grad_output)\n",
    "\n",
    "    @staticmethod\n",
    "    def __shook(self, missing_keys) -> None:\n",
    "        print(\"called load_state_dict_hook!\")\n",
    "\n",
    "    @jit.export\n",
    "    def fhook(self, inputs: tuple[Tensor]) -> None:\n",
    "        print(\"called fhook!\")\n",
    "        self.fflag = ~self.fflag\n",
    "\n",
    "    @jit.export\n",
    "    def bhook(self, grad_output) -> None:\n",
    "        print(\"called bhook!\")\n",
    "        self.bflag = ~self.bflag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T, N, m, n = 32, 128, 256, 128\n",
    "A0 = torch.randn(m, m)\n",
    "X = torch.randn(T, N, m)\n",
    "Y = torch.randn(T, N, m)\n",
    "xi = torch.randn(N, m)\n",
    "x = X[0]\n",
    "y = Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "module = Foo(m)\n",
    "optim = SGD(module.parameters(), lr=0.01)\n",
    "print(module.fflag, module.bflag)\n",
    "module.zero_grad(set_to_none=True)\n",
    "module(x).norm().backward()\n",
    "optim.step()\n",
    "print(module.fflag, module.bflag)\n",
    "module.zero_grad(set_to_none=True)\n",
    "module(x).norm().backward()\n",
    "optim.step()\n",
    "print(module.fflag, module.bflag)\n",
    "module.zero_grad(set_to_none=True)\n",
    "module(x).norm().backward()\n",
    "optim.step()\n",
    "print(module.fflag, module.bflag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed Test NaiveContraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer = NaiveContraction(m, m)\n",
    "model = jit.script(iResNetLayer(layer))\n",
    "model.layer.weight.data = A0.clone()\n",
    "assert model.layer.bias is None\n",
    "assert torch.allclose(model.layer.weight, A0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model.zero_grad(set_to_none=True)\n",
    "r = torch.tensor(0.0)\n",
    "for y in Y:\n",
    "    x = model.inverse(y)\n",
    "    r += torch.tensordot(x, xi)\n",
    "r.backward()\n",
    "G_naive = model.layer.weight.grad.clone().detach().flatten().numpy()\n",
    "print(r)\n",
    "# reference_grad = model.layer.weight.grad.clone().detach()\n",
    "# reference_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed Custom Cntraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer = LinearContraction(m, m)\n",
    "model = iResNetLayer(layer)\n",
    "model.layer.weight.data = A0.clone()\n",
    "assert model.layer.bias is None\n",
    "assert torch.allclose(model.layer.weight, A0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model.zero_grad(set_to_none=True)\n",
    "r = torch.tensor(0.0)\n",
    "for y in Y:\n",
    "    x = model.inverse(y)\n",
    "    r += torch.tensordot(x, xi)\n",
    "r.backward()\n",
    "G_custom = model.layer.weight.grad.clone().detach().flatten().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model.zero_grad(set_to_none=True)\n",
    "r = torch.tensor(0.0)\n",
    "for y in Y:\n",
    "    x = model.inverse(y)\n",
    "    r += torch.tensordot(x, xi)\n",
    "r.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = jit.script(OptimizediResNetLayer(m))\n",
    "model.layer.weight.data = A0.clone()\n",
    "assert model.layer.bias is None\n",
    "assert torch.allclose(model.layer.weight, A0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model.zero_grad(set_to_none=True)\n",
    "r = torch.tensor(0.0)\n",
    "for y in Y:\n",
    "    x = model.inverse(y)\n",
    "    r += torch.tensordot(x, xi)\n",
    "r.backward()\n",
    "G_opt = model.layer.weight.grad.clone().detach().flatten().numpy()\n",
    "print(r)\n",
    "# reference_grad = model.layer.weight.grad.clone().detach()\n",
    "# reference_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "grads = np.array([G_naive, G_custom, G_opt])\n",
    "distance_matrix(grads, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regular = LinearContraction(m, n)\n",
    "model = regular\n",
    "print(matrix_norm(model.weight, ord=2), model.weight)\n",
    "print(model.refresh_cache)\n",
    "y = model(x)\n",
    "print(matrix_norm(model.weight, ord=2), model.weight)\n",
    "print(model.refresh_cache)\n",
    "model.raise_refresh_cache_flag()\n",
    "print(model.refresh_cache)\n",
    "y.norm().backward()\n",
    "print(matrix_norm(model.weight, ord=2), model.weight)\n",
    "print(model.refresh_cache)\n",
    "y = model(x)\n",
    "print(matrix_norm(model.weight, ord=2), model.weight)\n",
    "print(model.refresh_cache)\n",
    "model.raise_refresh_cache_flag()\n",
    "print(model.refresh_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regular = LinearContraction(m, n)\n",
    "scripted = jit.script(regular)\n",
    "model = scripted\n",
    "print(matrix_norm(model.weight, ord=2), model.weight)\n",
    "print(model.refresh_cache)\n",
    "y = model(x)\n",
    "print(matrix_norm(model.weight, ord=2), model.weight)\n",
    "print(model.refresh_cache)\n",
    "model.raise_refresh_cache_flag()\n",
    "print(model.refresh_cache)\n",
    "y.norm().backward()\n",
    "print(matrix_norm(model.weight, ord=2), model.weight)\n",
    "print(model.refresh_cache)\n",
    "y = model(x)\n",
    "print(matrix_norm(model.weight, ord=2), model.weight)\n",
    "print(model.refresh_cache)\n",
    "model.raise_refresh_cache_flag()\n",
    "print(model.refresh_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regular = LinearContraction(m, n)\n",
    "scripted = jit.script(regular)\n",
    "scripted.save(\"serialized_module.pt\")\n",
    "loaded = jit.load(\"serialized_module.pt\")\n",
    "model = loaded\n",
    "print(matrix_norm(model.weight, ord=2), model.weight)\n",
    "print(model.refresh_cache)\n",
    "y = model(x)\n",
    "print(matrix_norm(model.weight, ord=2), model.weight)\n",
    "print(model.refresh_cache)\n",
    "model.raise_refresh_cache_flag()\n",
    "print(model.refresh_cache)\n",
    "y.norm().backward()\n",
    "print(matrix_norm(model.weight, ord=2), model.weight)\n",
    "print(model.refresh_cache)\n",
    "y = model(x)\n",
    "print(matrix_norm(model.weight, ord=2), model.weight)\n",
    "print(model.refresh_cache)\n",
    "model.raise_refresh_cache_flag()\n",
    "print(model.refresh_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regular = LinearContraction(m, n)\n",
    "scripted = jit.script(regular)\n",
    "scripted.save(\"serialized_module.pt\")\n",
    "loaded = jit.load(\"serialized_module.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix_norm(loaded.weight, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded.renormalize_matrix()\n",
    "matrix_norm(loaded.weight, ord=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A second heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
