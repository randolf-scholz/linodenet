{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20191951-beec-414a-b3c6-89dca8277ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'  # always print last expr.\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.export import export, Dim"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T14:05:41.621515885Z",
     "start_time": "2024-02-08T14:05:41.612132715Z"
    }
   },
   "id": "6fe78b73ceee6905",
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e466620b611c275b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from collections.abc import Mapping\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, jit, nn\n",
    "from typing_extensions import Any, Final, Optional, Self"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcb5790eaf23f1b2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ReZeroCell(nn.Module):\n",
    "    r\"\"\"ReZero module.\n",
    "\n",
    "    Simply multiplies the inputs by a scalar initialized to zero.\n",
    "    \"\"\"\n",
    "\n",
    "    HP = {\n",
    "        \"__name__\": __qualname__,\n",
    "        \"__module__\": __name__,\n",
    "    }\n",
    "    r\"\"\"The hyperparameter dictionary\"\"\"\n",
    "\n",
    "    # CONSTANTS\n",
    "    learnable: Final[bool]\n",
    "    r\"\"\"CONST: Whether the scalar is learnable.\"\"\"\n",
    "\n",
    "    # PARAMETERS\n",
    "    scalar: Tensor\n",
    "    r\"\"\"The scalar to multiply the inputs by.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        module: Optional[nn.Module] = None,\n",
    "        *,\n",
    "        scalar: Optional[Tensor] = None,\n",
    "        learnable: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.learnable = bool(learnable)\n",
    "        self.module = module\n",
    "        initial_value = torch.as_tensor(0.0 if scalar is None else scalar)\n",
    "        self.scalar = nn.Parameter(initial_value) if self.learnable else initial_value\n",
    "\n",
    "\n",
    "    @jit.export\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\".. Signature:: ``(...,) -> (...,)``.\"\"\"\n",
    "        if self.module is None:\n",
    "            return self.scalar * x\n",
    "        return self.scalar * self.module(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T14:42:10.851409088Z",
     "start_time": "2024-02-08T14:42:10.806073972Z"
    }
   },
   "id": "11881e93d97f0f62",
   "execution_count": 98
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "mod = ReZeroCell(nn.Linear(3,3))\n",
    "m = jit.script(mod)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T14:43:23.341439200Z",
     "start_time": "2024-02-08T14:43:23.291964001Z"
    }
   },
   "id": "75bb6392d558c225",
   "execution_count": 107
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "graph(%self : __torch__.___torch_mangle_9.ReZeroCell,\n      %x.1 : Tensor):\n  %scalar : Tensor = prim::GetAttr[name=\"scalar\"](%self)\n  %module : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"module\"](%self)\n  %9 : Tensor = prim::CallMethod[name=\"forward\"](%module, %x.1) # /tmp/ipykernel_60887/4245271241.py:40:29\n  %10 : Tensor = aten::mul(%scalar, %9) # /tmp/ipykernel_60887/4245271241.py:40:15\n  return (%10)"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.graph"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T14:43:26.276233047Z",
     "start_time": "2024-02-08T14:43:26.263583438Z"
    }
   },
   "id": "e3e489e466d3072a",
   "execution_count": 108
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "595df3d68ab0e913"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "aamodel = torch.compile(mod, dynamic=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T14:09:05.404591955Z",
     "start_time": "2024-02-08T14:09:05.394679622Z"
    }
   },
   "id": "1170a6b577e63fa4",
   "execution_count": 94
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ad42ad779cfc701b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.], grad_fn=<CompiledFunctionBackward>)"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T13:37:21.921020933Z",
     "start_time": "2024-02-08T13:37:21.914457079Z"
    }
   },
   "id": "23d8695a9465fa78",
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "x = torch.randn(7,2)\n",
    "args = (x,)\n",
    "shapes = {\"x\" : {0: Dim(\"batch\"), 1: Dim(\"feature\")}}\n",
    "exported_mod = export(mod, args, dynamic_shapes=shapes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T14:09:54.824942810Z",
     "start_time": "2024-02-08T14:09:54.753748160Z"
    }
   },
   "id": "d0499d6afa24a6ff",
   "execution_count": 96
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[97], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mexported_mod\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/KIWI/linodenet/.venv/lib/python3.11/site-packages/torch/export/exported_program.py:275\u001B[0m, in \u001B[0;36mExportedProgram.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    274\u001B[0m     ordered_tensor_constants \u001B[38;5;241m=\u001B[39m ()\n\u001B[0;32m--> 275\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_input_constraints\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    276\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mordered_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mordered_buffers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mordered_tensor_constants\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\n\u001B[1;32m    277\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    279\u001B[0m \u001B[38;5;66;03m# NOTE: calling convention is first params, then buffers, then args as user supplied them.\u001B[39;00m\n\u001B[1;32m    280\u001B[0m \u001B[38;5;66;03m# See: torch/_functorch/aot_autograd.py#L1034\u001B[39;00m\n\u001B[1;32m    281\u001B[0m res \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfx\u001B[38;5;241m.\u001B[39mInterpreter(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgraph_module)\u001B[38;5;241m.\u001B[39mrun(\n\u001B[1;32m    282\u001B[0m     \u001B[38;5;241m*\u001B[39mordered_params,\n\u001B[1;32m    283\u001B[0m     \u001B[38;5;241m*\u001B[39mordered_buffers,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    286\u001B[0m     enable_io_processing\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    287\u001B[0m )\n",
      "File \u001B[0;32m~/Projects/KIWI/linodenet/.venv/lib/python3.11/site-packages/torch/export/exported_program.py:570\u001B[0m, in \u001B[0;36mExportedProgram._check_input_constraints\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_input_constraints\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs):\n\u001B[1;32m    568\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_export\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _check_input_constraints_for_graph\n\u001B[0;32m--> 570\u001B[0m     \u001B[43m_check_input_constraints_for_graph\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    571\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrange_constraints\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mequality_constraints\u001B[49m\n\u001B[1;32m    572\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/KIWI/linodenet/.venv/lib/python3.11/site-packages/torch/_export/utils.py:56\u001B[0m, in \u001B[0;36m_check_input_constraints_for_graph.<locals>.inner\u001B[0;34m(*args)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m _assertion_graph_res \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     55\u001B[0m _assertion_graph \u001B[38;5;241m=\u001B[39m _assertion_graph_res\u001B[38;5;241m.\u001B[39mgraph_module\n\u001B[0;32m---> 56\u001B[0m \u001B[43m_assertion_graph\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/KIWI/linodenet/.venv/lib/python3.11/site-packages/torch/fx/graph_module.py:738\u001B[0m, in \u001B[0;36mGraphModule.recompile.<locals>.call_wrapped\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    737\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall_wrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 738\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wrapped_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/KIWI/linodenet/.venv/lib/python3.11/site-packages/torch/fx/graph_module.py:317\u001B[0m, in \u001B[0;36m_WrappedCall.__call__\u001B[0;34m(self, obj, *args, **kwargs)\u001B[0m\n\u001B[1;32m    315\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(\u001B[38;5;28;01mNone\u001B[39;00m)  \u001B[38;5;66;03m# noqa: TRY200\u001B[39;00m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 317\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "File \u001B[0;32m~/Projects/KIWI/linodenet/.venv/lib/python3.11/site-packages/torch/fx/graph_module.py:304\u001B[0m, in \u001B[0;36m_WrappedCall.__call__\u001B[0;34m(self, obj, *args, **kwargs)\u001B[0m\n\u001B[1;32m    302\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcls_call(obj, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    303\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 304\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcls\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    306\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m e\u001B[38;5;241m.\u001B[39m__traceback__\n",
      "File \u001B[0;32m~/Projects/KIWI/linodenet/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/KIWI/linodenet/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m<eval_with_key>.143:6\u001B[0m, in \u001B[0;36mforward\u001B[0;34m(self, arg0_1, l_x_)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, arg0_1, l_x_):\n\u001B[1;32m      5\u001B[0m     sym_size \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39maten\u001B[38;5;241m.\u001B[39msym_size\u001B[38;5;241m.\u001B[39mint(l_x_, \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m----> 6\u001B[0m     sym_size_1 \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maten\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msym_size\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mint\u001B[49m\u001B[43m(\u001B[49m\u001B[43ml_x_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m;  l_x_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ()\n",
      "File \u001B[0;32m~/Projects/KIWI/linodenet/.venv/lib/python3.11/site-packages/torch/_ops.py:513\u001B[0m, in \u001B[0;36mOpOverload.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_op\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mIndexError\u001B[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "exported_mod(torch.randn(4))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T14:09:56.293821054Z",
     "start_time": "2024-02-08T14:09:56.223126038Z"
    }
   },
   "id": "e94ad2338dd1eb6f",
   "execution_count": 97
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExportedProgram:\n",
      "    class GraphModule(torch.nn.Module):\n",
      "        def forward(self, arg0_1: \"f32[32, 64]\", arg1_1: \"f32[32]\", arg2_1: \"f32[64, 128]\", arg3_1: \"f32[64]\", arg4_1: \"f32[32]\", l_x1_: \"f32[s0, 64]\", l_x2_: \"f32[s0, 128]\"):\n",
      "            # File: /tmp/ipykernel_60887/258828328.py:17, code: out1 = self.branch1(x1)\n",
      "            t: \"f32[64, 32]\" = torch.ops.aten.t.default(arg0_1);  arg0_1 = None\n",
      "            addmm: \"f32[s0, 32]\" = torch.ops.aten.addmm.default(arg1_1, l_x1_, t);  arg1_1 = l_x1_ = t = None\n",
      "            relu: \"f32[s0, 32]\" = torch.ops.aten.relu.default(addmm);  addmm = None\n",
      "            \n",
      "            # File: /tmp/ipykernel_60887/258828328.py:18, code: out2 = self.branch2(x2)\n",
      "            t_1: \"f32[128, 64]\" = torch.ops.aten.t.default(arg2_1);  arg2_1 = None\n",
      "            addmm_1: \"f32[s0, 64]\" = torch.ops.aten.addmm.default(arg3_1, l_x2_, t_1);  arg3_1 = l_x2_ = t_1 = None\n",
      "            relu_1: \"f32[s0, 64]\" = torch.ops.aten.relu.default(addmm_1);  addmm_1 = None\n",
      "            \n",
      "            # File: /tmp/ipykernel_60887/258828328.py:19, code: return (out1 + self.buffer, out2)\n",
      "            add: \"f32[s0, 32]\" = torch.ops.aten.add.Tensor(relu, arg4_1);  relu = arg4_1 = None\n",
      "            return (add, relu_1)\n",
      "            \n",
      "Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='arg0_1'), target='branch1.0.weight'), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='arg1_1'), target='branch1.0.bias'), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='arg2_1'), target='branch2.0.weight'), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='arg3_1'), target='branch2.0.bias'), InputSpec(kind=<InputKind.BUFFER: 3>, arg=TensorArgument(name='arg4_1'), target='L__self___buffer'), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='l_x1_'), target=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='l_x2_'), target=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='add'), target=None), OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='relu_1'), target=None)])\n",
      "Range constraints: {s0: ValueRanges(lower=2, upper=oo, is_bool=False)}\n",
      "Equality constraints: [(InputDim(input_name='l_x1_', dim=0), InputDim(input_name='l_x2_', dim=0))]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.export import Dim, export\n",
    "\n",
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.branch1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64, 32), torch.nn.ReLU()\n",
    "        )\n",
    "        self.branch2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 64), torch.nn.ReLU()\n",
    "        )\n",
    "        self.buffer = torch.ones(32)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.branch1(x1)\n",
    "        out2 = self.branch2(x2)\n",
    "        return (out1 + self.buffer, out2)\n",
    "\n",
    "example_args = (torch.randn(32, 64), torch.randn(32, 128))\n",
    "\n",
    "# Create a dynamic batch size\n",
    "batch = Dim(\"batch\")\n",
    "# Specify that the first dimension of each input is that batch size\n",
    "dynamic_shapes = {\"x1\": {0: batch}, \"x2\": {0: batch}}\n",
    "\n",
    "exported_program: torch.export.ExportedProgram = export(\n",
    "    M(), args=example_args, dynamic_shapes=dynamic_shapes\n",
    ")\n",
    "print(exported_program)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T14:08:47.687337090Z",
     "start_time": "2024-02-08T14:08:47.471581288Z"
    }
   },
   "id": "46c2c80dbd98a09a",
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "aclass ReZero(nn.ModuleList):\n",
    "    r\"\"\"A ReZero model.\"\"\"\n",
    "\n",
    "    def __init__(self, blocks: nn.Module, weights: Optional[Tensor] = None) -> None:\n",
    "        super().__init__(blocks)\n",
    "\n",
    "        self.weights = nn.Parameter(\n",
    "            torch.zeros(len(blocks)) if weights is None else weights\n",
    "        )\n",
    "        # self.blocks = nn.ModuleList(blocks)\n",
    "        # super().__init__(blocks)\n",
    "\n",
    "    @jit.export\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        for k, block in enumerate(self):\n",
    "            x = x + self.weights[k] * block(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T13:11:48.094232889Z",
     "start_time": "2024-02-08T13:11:48.087474356Z"
    }
   },
   "id": "35e0965351425f21",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "RecursiveScriptModule(\n  original_name=ReZero\n  (0): RecursiveScriptModule(original_name=Linear)\n  (1): RecursiveScriptModule(original_name=Linear)\n  (2): RecursiveScriptModule(original_name=Linear)\n)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = ReZero([nn.Linear(3, 3), nn.Linear(3, 3), nn.Linear(3,3)])\n",
    "m = jit.script(mod)\n",
    "m"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T13:11:54.604149041Z",
     "start_time": "2024-02-08T13:11:54.563164538Z"
    }
   },
   "id": "632bbde910adf832",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.4803, -1.1254, -0.8179],\n        [-0.8927, -0.9191,  0.3820],\n        [-0.3159,  2.1621, -1.3353],\n        [-0.3503,  0.8862,  1.0051],\n        [-0.4189,  0.7528, -0.8066],\n        [ 2.1228,  1.4426,  1.6509],\n        [-0.5515, -1.2795,  0.0146]], grad_fn=<DifferentiableGraphBackward>)"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(torch.randn(7, 3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T13:12:03.720572321Z",
     "start_time": "2024-02-08T13:12:03.710088805Z"
    }
   },
   "id": "22be2440d4a57415",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "RecursiveScriptModule(\n  original_name=ReZero\n  (0): RecursiveScriptModule(original_name=Linear)\n  (1): RecursiveScriptModule(original_name=Linear)\n  (2): RecursiveScriptModule(original_name=Linear)\n)"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T13:12:04.873525818Z",
     "start_time": "2024-02-08T13:12:04.868168821Z"
    }
   },
   "id": "2cd68a7c693c808c",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([0.], requires_grad=True)"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[:1].weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T13:12:24.039805031Z",
     "start_time": "2024-02-08T13:12:23.995523600Z"
    }
   },
   "id": "a00a08f6ea12dbcf",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d9ee7b80b63d9a05"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "aa\n",
    "class Constant(nn.Module):\n",
    "    r\"\"\"Constant function.\"\"\"\n",
    "\n",
    "    def __init__(self, value: float | Tensor) -> None:\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"value\", torch.as_tensor(value))\n",
    "\n",
    "    def forward(self, _: Tensor) -> Tensor:\n",
    "        return self.value\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T08:34:02.171792238Z",
     "start_time": "2024-02-08T08:33:59.851681072Z"
    }
   },
   "id": "29c288dbaf9dfe38",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def foo(x: Tensor, y: Tensor) -> None:\n",
    "    \n",
    "    for x_, y_ in zip(x, y):\n",
    "        print(x_, y_)\n",
    "        print(x_ + y_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T13:06:22.996350489Z",
     "start_time": "2024-02-08T13:06:22.995624984Z"
    }
   },
   "id": "b0cfb65553b45f94",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.9633\n",
      " 0.2476\n",
      " 0.9208\n",
      "[ CPUFloatType{3} ]  0.1607\n",
      " 0.1260\n",
      " 0.8766\n",
      "[ CPUFloatType{3} ]\n",
      " 1.1239\n",
      " 0.3736\n",
      " 1.7974\n",
      "[ CPUFloatType{3} ]\n",
      " 0.0732\n",
      " 0.6961\n",
      " 0.3948\n",
      "[ CPUFloatType{3} ]  0.1357\n",
      " 0.2627\n",
      " 0.4702\n",
      "[ CPUFloatType{3} ]\n",
      " 0.2089\n",
      " 0.9588\n",
      " 0.8650\n",
      "[ CPUFloatType{3} ]\n",
      " 0.4245\n",
      " 0.0720\n",
      " 0.6513\n",
      "[ CPUFloatType{3} ]  0.5397\n",
      " 0.8934\n",
      " 0.6739\n",
      "[ CPUFloatType{3} ]\n",
      " 0.9642\n",
      " 0.9655\n",
      " 1.3251\n",
      "[ CPUFloatType{3} ]\n"
     ]
    }
   ],
   "source": [
    "jit.script(foo)(torch.rand(3, 3), torch.rand(3, 3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T13:06:35.434099373Z",
     "start_time": "2024-02-08T13:06:35.363414005Z"
    }
   },
   "id": "8f9598d08f9ed63",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "forward() is missing value for argument '_'. Declaration: forward(__torch__.Constant self, Tensor _) -> Tensor",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mjit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscript\u001B[49m\u001B[43m(\u001B[49m\u001B[43mConstant\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/KIWI/linodenet/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/KIWI/linodenet/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: forward() is missing value for argument '_'. Declaration: forward(__torch__.Constant self, Tensor _) -> Tensor"
     ]
    }
   ],
   "source": [
    "jit.script(Constant(1.0))()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T08:34:21.314791549Z",
     "start_time": "2024-02-08T08:34:21.057327707Z"
    }
   },
   "id": "e7084da499559e4d",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b7f6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import metadata\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import torchinfo\n",
    "\n",
    "from torch import jit\n",
    "from linodenet.models import LinODE, LinODECell, LinODEnet\n",
    "from linodenet.models.filters import SequentialFilter\n",
    "from linodenet.projections.functional import skew_symmetric, symmetric\n",
    "\n",
    "NUM_DIM = 128\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "DTYPE = torch.float32\n",
    "\n",
    "\n",
    "def join_dicts(d: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"Recursively join dict by composing keys with '/'.\"\"\"\n",
    "    result = {}\n",
    "    for key, val in d.items():\n",
    "        if isinstance(val, dict):\n",
    "            result |= join_dicts(\n",
    "                {f\"{key}/{subkey}\": item for subkey, item in val.items()}\n",
    "            )\n",
    "        else:\n",
    "            result[key] = val\n",
    "    return result\n",
    "\n",
    "\n",
    "def add_prefix(d: dict[str, Any], /, prefix: str) -> dict[str, Any]:\n",
    "    return {f\"{prefix}/{key}\": item for key, item in d.items()}\n",
    "\n",
    "\n",
    "# OPTIMIZER_CONIFG = {\n",
    "#     \"__name__\": \"SGD\",\n",
    "#     \"lr\": 0.001,\n",
    "#     \"momentum\": 0,\n",
    "#     \"dampening\": 0,\n",
    "#     \"weight_decay\": 0,\n",
    "#     \"nesterov\": False,\n",
    "# }\n",
    "\n",
    "# OPTIMIZER_CONFIG = {\n",
    "#     \"__name__\": \"Adam\",\n",
    "#     \"lr\": 0.01,\n",
    "#     \"betas\": (0.9, 0.999),\n",
    "#     \"eps\": 1e-08,\n",
    "#     \"weight_decay\": 0,\n",
    "#     \"amsgrad\": False,\n",
    "# }\n",
    "\n",
    "\n",
    "OPTIMIZER_CONFIG = {\n",
    "    \"__name__\": \"AdamW\",\n",
    "    \"lr\": 0.001,\n",
    "    \"betas\": (0.9, 0.999),\n",
    "    \"eps\": 1e-08,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"amsgrad\": False,\n",
    "}\n",
    "\n",
    "\n",
    "SYSTEM = {\n",
    "    \"__name__\": \"LinODECell\",\n",
    "    \"input_size\": int,\n",
    "    \"kernel_initialization\": \"skew-symmetric\",\n",
    "}\n",
    "\n",
    "EMBEDDING = {\n",
    "    \"__name__\": \"ConcatEmbedding\",\n",
    "    \"input_size\": int,\n",
    "    \"hidden_size\": int,\n",
    "}\n",
    "FILTER = {\n",
    "    \"__name__\": \"SequentialFilter\",\n",
    "    \"input_size\": int,\n",
    "    \"hidden_size\": int,\n",
    "    \"autoregressive\": True,\n",
    "}\n",
    "\n",
    "# FILTER = {\n",
    "#     \"__name__\": \"RecurrentCellFilter\",\n",
    "#     \"concat\": True,\n",
    "#     \"input_size\": int,\n",
    "#     \"hidden_size\": int,\n",
    "#     \"autoregressive\": True,\n",
    "#     \"Cell\": {\n",
    "#         \"__name__\": \"GRUCell\",\n",
    "#         \"input_size\": int,\n",
    "#         \"hidden_size\": int,\n",
    "#         \"bias\": True,\n",
    "#         \"device\": None,\n",
    "#         \"dtype\": None,\n",
    "#     },\n",
    "# }\n",
    "from linodenet.models.encoders import ResNet, iResNet\n",
    "\n",
    "# ENCODER = {\"__name__\": \"ResNet\", \"__module__\": \"linodenet.models.encoders\",\"input_size\": int, \"nblocks\": 5, \"rezero\": True}\n",
    "# DECODER = {\"__name__\": \"ResNet\", \"__module__\": \"linodenet.models.encoders\",\"input_size\": int, \"nblocks\": 5, \"rezero\": True}\n",
    "\n",
    "\n",
    "LR_SCHEDULER_CONFIG = {\n",
    "    \"__name__\": \"ReduceLROnPlateau\",\n",
    "    \"mode\": \"min\",\n",
    "    # (str) – One of min, max. In min mode, lr will be reduced when the quantity monitored has stopped decreasing; in max mode it will be reduced when the quantity monitored has stopped increasing. Default: ‘min’.\n",
    "    \"factor\": 0.1,\n",
    "    # (float) – Factor by which the learning rate will be reduced. new_lr = lr * factor. Default: 0.1.\n",
    "    \"patience\": 10,\n",
    "    # (int) – Number of epochs with no improvement after which learning rate will be reduced. For example, if patience = 2, then we will ignore the first 2 epochs with no improvement, and will only decrease the LR after the 3rd epoch if the loss still hasn’t improved then. Default: 10.\n",
    "    \"threshold\": 0.0001,\n",
    "    # (float) – Threshold for measuring the new optimum, to only focus on significant changes. Default: 1e-4.\n",
    "    \"threshold_mode\": \"rel\",\n",
    "    # (str) – One of rel, abs. In rel mode, dynamic_threshold = best * ( 1 + threshold ) in ‘max’ mode or best * ( 1 - threshold ) in min mode. In abs mode, dynamic_threshold = best + threshold in max mode or best - threshold in min mode. Default: ‘rel’.\n",
    "    \"cooldown\": 0,\n",
    "    # (int) – Number of epochs to wait before resuming normal operation after lr has been reduced. Default: 0.\n",
    "    \"min_lr\": 1e-08,\n",
    "    # (float or list) – A scalar or a list of scalars. A lower bound on the learning rate of all param groups or each group respectively. Default: 0.\n",
    "    \"eps\": 1e-08,\n",
    "    # (float) – Minimal decay applied to lr. If the difference between new and old lr is smaller than eps, the update is ignored. Default: 1e-8.\n",
    "    \"verbose\": True,\n",
    "    # (bool) – If True, prints a message to stdout for each update. Default: False.\n",
    "}\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    \"__name__\": \"LinODEnet\",\n",
    "    \"input_size\": NUM_DIM,\n",
    "    \"hidden_size\": 128,\n",
    "    \"embedding_type\": \"concat\",\n",
    "    \"Filter\": SequentialFilter.HP,\n",
    "    \"System\": SYSTEM,\n",
    "    \"Encoder\": ResNet.HP,\n",
    "    \"Decoder\": ResNet.HP,\n",
    "    \"Embedding\": EMBEDDING,\n",
    "}\n",
    "\n",
    "\n",
    "HPARAMS = join_dicts({\n",
    "    \"Optimizer\": OPTIMIZER_CONFIG,\n",
    "    \"LR_Scheduler\": LR_SCHEDULER_CONFIG,\n",
    "    \"Model\": MODEL_CONFIG,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d695b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = LinODEnet\n",
    "model = MODEL(**MODEL_CONFIG)\n",
    "model.to(device=DEVICE, dtype=DTYPE)\n",
    "torchinfo.summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211ce43a-8480-419d-b34c-c548c4c8ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "jit.save(model, \"model.pt\")\n",
    "model = jit.load(\"model.pt\")\n",
    "torchinfo.summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43ba99-5e11-4375-80b0-12cd4acda11e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
