{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20191951-beec-414a-b3c6-89dca8277ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'  # always print last expr.\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4924c28-4ed7-41d4-a961-bd4ece3c62bd",
   "metadata": {},
   "source": [
    "## LinODEnet v2\n",
    "\n",
    "\n",
    "We add special treatment of covariates $u$\n",
    "\n",
    "Inhomogeneous Linear Equation\n",
    "\n",
    "$$ \\dot{x}(t) = Ax(t) + f(t) $$\n",
    "\n",
    "Solution:\n",
    "\n",
    "$$\\begin{aligned} \n",
    "     x(t) &=  e^{A(t-t_0)}x_{t_0} + \\int_{t_0}^t e^{A(t-s)} f(s) ds \n",
    "\\\\   x(t+âˆ†t) &=  e^{Aâˆ†t}x_t + \\int_{t}^{t+âˆ†t} e^{A(t+âˆ†t-s)} f(s) ds \n",
    "\\\\   x(t+âˆ†t) &=  e^{Aâˆ†t}x_t + \\int_{0}^{âˆ†t} e^{A(âˆ†t-âˆ†Ï„)} f(t+{âˆ†Ï„}) d{âˆ†Ï„}  \\qquad s=t+âˆ†Ï„\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "Special cases:\n",
    "\n",
    "1. $f(t) = b$ constant in $t$, then $â‡x(t+âˆ†t)=e^{Aâˆ†t}x_t + \\frac{e^{Aâˆ†t}-ð•€}{A}b$\n",
    "2. $f(t) = aâ‹…t + b$ linear in $t$, then ?\n",
    "\n",
    "Generalized Exponential Integral:\n",
    "\n",
    "\n",
    "$$ E_n(x) = âˆ« \\frac{e^{-xt}}{t^n} dt = x^{n-1} Î“(1-n,x)$$\n",
    "\n",
    "Misra Function: $Ï†_m(x) = E_{-m}(x)$\n",
    "\n",
    "Block matrix trick:\n",
    "\n",
    "$$ \\exp \\bigg(\\begin{bmatrix}A& B \\\\ 0 & 0 \\end{bmatrix}â‹…t\\bigg) = \\begin{bmatrix}e^{At} & âˆ«_0^t e^{AÏ„}BdÏ„ \\\\ 0 & ð•€ \\end{bmatrix}$$ \n",
    "\n",
    "\n",
    "\n",
    "Formula: (Higham)\n",
    "\n",
    "\n",
    "$$ x(t+âˆ†t) = e^{Aâˆ†t}x_t + âˆ‘_{k=1}^âˆž Ï†_k(Aâˆ†t) u_k âˆ†t^k \\qquad  u_k = ðƒ^{k-1}f(t)$$\n",
    "\n",
    "Recursion:  $$ \\varphi_{\\ell}(z)=z \\varphi_{\\ell+1}(z)+\\frac{1}{\\ell !}, \\quad \\varphi_{0}(z)=e^{z}$$\n",
    "\n",
    "So: $$Ï†_1(At) = \\frac{e^{At}-ð•€}{At} \\qquad Ï†_2(At) = \\frac{e^{At} -At -ð•€}{(At)^2} \\qquad Ï†_3(At) = \\frac{e^{At}-Â½(At)^2 -At -ð•€}{(At)^3} $$\n",
    "\n",
    "Truncating at $K=2$, i.e. $f(t)=Ï‰â‹…t+b$\n",
    "\n",
    "\n",
    "$$ x(t+âˆ†t) = e^{Aâˆ†t}x_t + \\frac{e^{Aâˆ†t}-ð•€}{A {âˆ†t}}{âˆ†t} b +  \\frac{e^{Aâˆ†t}-A-ð•€}{(A {âˆ†t})^2}{âˆ†t}^2Ï‰$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28805f4f-f3ac-42b5-85a3-79633b0a9c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a367bf56-f73f-4345-895c-65741fcd7ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e7afbc-0b6c-405e-98be-e537434fd573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import linodenet\n",
    "from linodenet.models.system import LinODECell\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b7f6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Final\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, jit, nn\n",
    "\n",
    "from linodenet.initializations.functional import FunctionalInitialization\n",
    "from linodenet.models.embeddings import ConcatEmbedding, ConcatProjection\n",
    "from linodenet.models.encoders import iResNet\n",
    "from linodenet.models.filters import Filter, RecurrentCellFilter\n",
    "from linodenet.models.system import LinODECell\n",
    "from linodenet.projections import Projection\n",
    "from linodenet.util import autojit, deep_dict_update, initialize_from_config\n",
    "\n",
    "\n",
    "__logger__ = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# @autojit\n",
    "class LinODEnet(nn.Module):\n",
    "    r\"\"\"Linear ODE Network is a FESD model.\n",
    "\n",
    "    +---------------------------------------------------+--------------------------------------+\n",
    "    | Component                                         | Formula                              |\n",
    "    +===================================================+======================================+\n",
    "    | Filter  `F` (default: :class:`~torch.nn.GRUCell`) | `\\hat x_i' = F(\\hat x_i, x_i)`       |\n",
    "    +---------------------------------------------------+--------------------------------------+\n",
    "    | Encoder `Ï•` (default: :class:`~iResNet`)          | `\\hat z_i' = Ï•(\\hat x_i')`           |\n",
    "    +---------------------------------------------------+--------------------------------------+\n",
    "    | System  `S` (default: :class:`~LinODECell`)       | `\\hat z_{i+1} = S(\\hat z_i', Î” t_i)` |\n",
    "    +---------------------------------------------------+--------------------------------------+\n",
    "    | Decoder `Ï€` (default: :class:`~iResNet`)          | `\\hat x_{i+1}  =  Ï€(\\hat z_{i+1})`   |\n",
    "    +---------------------------------------------------+--------------------------------------+\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    input_size:  int\n",
    "        The dimensionality of the input space.\n",
    "    hidden_size: int\n",
    "        The dimensionality of the latent space.\n",
    "    output_size: int\n",
    "        The dimensionality of the output space.\n",
    "    ZERO: Tensor\n",
    "        BUFFER: A constant tensor of value float(0.0)\n",
    "    xhat_pre: Tensor\n",
    "        BUFFER: Stores pre-jump values.\n",
    "    xhat_post: Tensor\n",
    "        BUFFER: Stores post-jump values.\n",
    "    zhat_pre: Tensor\n",
    "        BUFFER: Stores pre-jump latent values.\n",
    "    zhat_post: Tensor\n",
    "        BUFFER: Stores post-jump latent values.\n",
    "    kernel: Tensor\n",
    "        PARAM: The system matrix of the linear ODE component.\n",
    "    encoder: nn.Module\n",
    "        MODULE: Responsible for embedding `xÌ‚â†’zÌ‚`.\n",
    "    embedding: nn.Module\n",
    "        MODULE: Responsible for embedding `xÌ‚â†’zÌ‚`.\n",
    "    system: nn.Module\n",
    "        MODULE: Responsible for propagating `zÌ‚_tâ†’zÌ‚_{t+âˆ†t}`.\n",
    "    decoder: nn.Module\n",
    "        MODULE: Responsible for projecting `zÌ‚â†’xÌ‚`.\n",
    "    projection: nn.Module\n",
    "        MODULE: Responsible for projecting `zÌ‚â†’xÌ‚`.\n",
    "    filter: nn.Module\n",
    "        MODULE: Responsible for updating `(xÌ‚, x_obs) â†’xÌ‚'`.\n",
    "    \"\"\"\n",
    "\n",
    "    name: Final[str] = __name__\n",
    "    \"\"\"str: The name of the model.\"\"\"\n",
    "\n",
    "    HP = {\n",
    "        \"__name__\": __qualname__,  # type: ignore[name-defined]\n",
    "        \"__doc__\": __doc__,\n",
    "        \"__module__\": __module__,  # type: ignore[name-defined]\n",
    "        \"input_size\": int,\n",
    "        \"hidden_size\": int,\n",
    "        \"output_size\": int,\n",
    "        \"System\": LinODECell.HP,\n",
    "        \"Embedding\": ConcatEmbedding.HP,\n",
    "        \"Projection\": ConcatProjection.HP,\n",
    "        \"Filter\": RecurrentCellFilter.HP | {\"autoregressive\": True},\n",
    "        \"Encoder\": iResNet.HP,\n",
    "        \"Decoder\": iResNet.HP,\n",
    "    }\n",
    "    r\"\"\"Dictionary of Hyperparameters.\"\"\"\n",
    "\n",
    "    # Constants\n",
    "    input_size: Final[int]\n",
    "    r\"\"\"CONST: The dimensionality of the inputs.\"\"\"\n",
    "    hidden_size: Final[int]\n",
    "    r\"\"\"CONST: The dimensionality of the linear ODE.\"\"\"\n",
    "    output_size: Final[int]\n",
    "    r\"\"\"CONST: The dimensionality of the outputs.\"\"\"\n",
    "\n",
    "    # Buffers\n",
    "    zero: Tensor\n",
    "    r\"\"\"BUFFER: A tensor of value float(0.0)\"\"\"\n",
    "    xhat_pre: Tensor\n",
    "    r\"\"\"BUFFER: Stores pre-jump values.\"\"\"\n",
    "    xhat_post: Tensor\n",
    "    r\"\"\"BUFFER: Stores post-jump values.\"\"\"\n",
    "    zhat_pre: Tensor\n",
    "    r\"\"\"BUFFER: Stores pre-jump latent values.\"\"\"\n",
    "    zhat_post: Tensor\n",
    "    r\"\"\"BUFFER: Stores post-jump latent values.\"\"\"\n",
    "    timedeltas: Tensor\n",
    "    \"\"\"BUFFER: Stores the timedelta values.\"\"\"\n",
    "\n",
    "    # Parameters:\n",
    "    kernel: Tensor\n",
    "    r\"\"\"PARAM: The system matrix of the linear ODE component.\"\"\"\n",
    "    z0: Tensor\n",
    "    r\"\"\"PARAM: The initial latent state.\"\"\"\n",
    "\n",
    "    # Sub-Modules\n",
    "    # encoder: Any\n",
    "    # r\"\"\"MODULE: Responsible for embedding `xÌ‚â†’zÌ‚`.\"\"\"\n",
    "    # embedding: nn.Module\n",
    "    # r\"\"\"MODULE: Responsible for embedding `xÌ‚â†’zÌ‚`.\"\"\"\n",
    "    # system: nn.Module\n",
    "    # r\"\"\"MODULE: Responsible for propagating `zÌ‚_tâ†’zÌ‚_{t+âˆ†t}`.\"\"\"\n",
    "    # decoder: nn.Module\n",
    "    # r\"\"\"MODULE: Responsible for projecting `zÌ‚â†’xÌ‚`.\"\"\"\n",
    "    # projection: nn.Module\n",
    "    # r\"\"\"MODULE: Responsible for projecting `zÌ‚â†’xÌ‚`.\"\"\"\n",
    "    # filter: nn.Module\n",
    "    # r\"\"\"MODULE: Responsible for updating `(xÌ‚, x_obs) â†’xÌ‚'`.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, **HP: Any):\n",
    "        super().__init__()\n",
    "        self.CFG = HP = deep_dict_update(self.HP, HP)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = input_size\n",
    "\n",
    "        HP[\"Encoder\"][\"input_size\"] = hidden_size\n",
    "        HP[\"Decoder\"][\"input_size\"] = hidden_size\n",
    "        HP[\"System\"][\"input_size\"] = hidden_size\n",
    "        HP[\"Filter\"][\"hidden_size\"] = input_size\n",
    "        HP[\"Filter\"][\"input_size\"] = input_size\n",
    "        HP[\"Embedding\"][\"input_size\"] = input_size\n",
    "        HP[\"Embedding\"][\"hidden_size\"] = hidden_size\n",
    "        HP[\"Projection\"][\"input_size\"] = input_size\n",
    "        HP[\"Projection\"][\"hidden_size\"] = hidden_size\n",
    "\n",
    "        # if HP[\"embedding_type\"] == \"linear\":\n",
    "        #     _embedding: nn.Module = nn.Linear(input_size, hidden_size)\n",
    "        #     _projection: nn.Module = nn.Linear(hidden_size, input_size)\n",
    "        # elif HP[\"embedding_type\"] == \"concat\":\n",
    "        #     _embedding = ConcatEmbedding(input_size, hidden_size)\n",
    "        #     _projection = ConcatProjection(input_size, hidden_size)\n",
    "        # else:\n",
    "        #     raise NotImplementedError(\n",
    "        #         f\"{HP['embedding_type']=}\" + \"not in {'linear', 'concat'}\"\n",
    "        #     )\n",
    "\n",
    "        # TODO: replace with add_module once supported!\n",
    "        # self.add_module(\"embedding\", _embedding)\n",
    "        # self.add_module(\"encoder\", HP[\"Encoder\"](**HP[\"Encoder_cfg\"]))\n",
    "        # self.add_module(\"system\", HP[\"System\"](**HP[\"System_cfg\"]))\n",
    "        # self.add_module(\"decoder\", HP[\"Decoder\"](**HP[\"Decoder_cfg\"]))\n",
    "        # self.add_module(\"projection\", _projection)\n",
    "        # self.add_module(\"filter\", HP[\"Filter\"](**HP[\"Filter_cfg\"]))\n",
    "        __logger__.debug(\"%s Initializing Embedding %s\", self.name, HP[\"Embedding\"])\n",
    "        self.embedding: nn.Module = initialize_from_config(HP[\"Embedding\"])\n",
    "        __logger__.debug(\"%s Initializing Embedding %s\", self.name, HP[\"Embedding\"])\n",
    "        self.projection: nn.Module = initialize_from_config(HP[\"Projection\"])\n",
    "        __logger__.debug(\"%s Initializing Encoder %s\", self.name, HP[\"Encoder\"])\n",
    "        self.encoder: nn.Module = initialize_from_config(HP[\"Encoder\"])\n",
    "        __logger__.debug(\"%s Initializing System %s\", self.name, HP[\"Encoder\"])\n",
    "        self.system: nn.Module = initialize_from_config(HP[\"System\"])\n",
    "        __logger__.debug(\"%s Initializing Decoder %s\", self.name, HP[\"Encoder\"])\n",
    "        self.decoder: nn.Module = initialize_from_config(HP[\"Decoder\"])\n",
    "        __logger__.debug(\"%s Initializing Filter %s\", self.name, HP[\"Encoder\"])\n",
    "        self.filter: Filter = initialize_from_config(HP[\"Filter\"])\n",
    "\n",
    "        assert isinstance(self.system.kernel, Tensor)\n",
    "        self.kernel = self.system.kernel\n",
    "        self.z0 = nn.Parameter(torch.randn(self.hidden_size))\n",
    "\n",
    "        # Buffers\n",
    "        self.register_buffer(\"zero\", torch.tensor(0.0), persistent=False)\n",
    "        self.register_buffer(\"timedeltas\", torch.tensor(()), persistent=False)\n",
    "        self.register_buffer(\"xhat_pre\", torch.tensor(()), persistent=False)\n",
    "        self.register_buffer(\"xhat_post\", torch.tensor(()), persistent=False)\n",
    "        self.register_buffer(\"zhat_pre\", torch.tensor(()), persistent=False)\n",
    "        self.register_buffer(\"zhat_post\", torch.tensor(()), persistent=False)\n",
    "\n",
    "    @jit.export\n",
    "    def forward(self, T: Tensor, X: Tensor) -> Tensor:\n",
    "        r\"\"\"Signature: `[...,N]Ã—[...,N,d] âŸ¶ [...,N,d]`.\n",
    "\n",
    "        **Model Sketch**::\n",
    "\n",
    "            âŸ¶ [ODE] âŸ¶ (zÌ‚áµ¢)                (zÌ‚áµ¢') âŸ¶ [ODE] âŸ¶\n",
    "                       â†“                   â†‘\n",
    "                      [Î¨]                 [Î¦]\n",
    "                       â†“                   â†‘\n",
    "                      (xÌ‚áµ¢) â†’ [ filter ] â†’ (xÌ‚áµ¢')\n",
    "                                 â†‘\n",
    "                              (táµ¢, xáµ¢)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        T: Tensor, shape=(...,LEN) or PackedSequence\n",
    "            The timestamps of the observations.\n",
    "        X: Tensor, shape=(...,LEN,DIM) or PackedSequence\n",
    "            The observed, noisy values at times `tâˆˆT`. Use ``NaN`` to indicate missing values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        XÌ‚_pre: Tensor, shape=(...,LEN,DIM)\n",
    "            The estimated true state of the system at the times `tâ»âˆˆT` (pre-update).\n",
    "        XÌ‚_post: Tensor, shape=(...,LEN,DIM)\n",
    "            The estimated true state of the system at the times `tâºâˆˆT` (post-update).\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        - https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/\n",
    "        \"\"\"\n",
    "        BATCH_SIZE = X.shape[:-2]\n",
    "        # prepend a single zero for the first iteration.\n",
    "        pad_dim = list(BATCH_SIZE) + [1]\n",
    "        pad = torch.zeros(pad_dim, device=T.device, dtype=T.dtype)\n",
    "        DT = torch.diff(T, prepend=pad, dim=-1)  # (..., LEN) â†’ (..., LEN)\n",
    "        DT = DT.moveaxis(-1, 0)  # (..., LEN) â†’ (LEN, ...)\n",
    "        X = torch.moveaxis(X, -2, 0)  # (...,LEN,DIM) â†’ (LEN,...,DIM)\n",
    "\n",
    "        Zhat_pre: list[Tensor] = []\n",
    "        Xhat_pre: list[Tensor] = []\n",
    "        Xhat_post: list[Tensor] = []\n",
    "        Zhat_post: list[Tensor] = []\n",
    "\n",
    "        zÌ‚_post = self.z0\n",
    "\n",
    "        for dt, x_obs in zip(DT, X):\n",
    "            # Propagate the latent state forward in time.\n",
    "            zÌ‚_pre = self.system(dt, zÌ‚_post)  # (...,), (...,LAT) -> (...,LAT)\n",
    "\n",
    "            # Decode the latent state at the observation time.\n",
    "            xÌ‚_pre = self.projection(self.decoder(zÌ‚_pre))  # (...,LAT) -> (...,DIM)\n",
    "\n",
    "            # Update the state estimate by filtering the observation.\n",
    "            xÌ‚_post = self.filter(x_obs, xÌ‚_pre)  # (...,DIM), (..., DIM) â†’ (...,DIM)\n",
    "\n",
    "            # Encode the latent state at the observation time.\n",
    "            zÌ‚_post = self.encoder(self.embedding(xÌ‚_post))  # (...,DIM) â†’ (...,LAT)\n",
    "\n",
    "            # Save all tensors for later.\n",
    "            Zhat_pre.append(zÌ‚_pre)\n",
    "            Xhat_pre.append(xÌ‚_pre)\n",
    "            Xhat_post.append(xÌ‚_post)\n",
    "            Zhat_post.append(zÌ‚_post)\n",
    "\n",
    "        self.xhat_pre = torch.stack(Xhat_pre, dim=-2)\n",
    "        self.xhat_post = torch.stack(Xhat_post, dim=-2)\n",
    "        self.zhat_pre = torch.stack(Zhat_pre, dim=-2)\n",
    "        self.zhat_post = torch.stack(Zhat_post, dim=-2)\n",
    "        self.timedeltas = DT.moveaxis(0, -1)\n",
    "\n",
    "        return self.xhat_post\n",
    "\n",
    "        # TODO: Control variables\n",
    "        # xhat = self.control(xhat, u)\n",
    "        # u: possible controls:\n",
    "        #  1. set to value\n",
    "        #  2. add to value\n",
    "        # do these via indicator variable\n",
    "        # u = (time, value, mode-indicator, col-indicator)\n",
    "        # => apply control to specific column.\n",
    "\n",
    "        # TODO: Smarter initialization\n",
    "        # IDEA: The problem is the initial state of RNNCell is not defined and typically put equal\n",
    "        # to zero. Staying with the idea that the Cell acts as a filter, that is updates the state\n",
    "        # estimation given an observation, we could \"trust\" the original observation in the sense\n",
    "        # that we solve the fixed point equation h0 = g(x0, h0) and put the solution as the initial\n",
    "        # state.\n",
    "        # issue: if x0 is really sparse this is useless.\n",
    "        # better idea: we probably should go back and forth.\n",
    "        # other idea: use a set-based model and put h = g(T,X), including the whole TS.\n",
    "        # This set model can use triplet notation.\n",
    "        # bias weighting towards close time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d695b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(LinODEnet(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c511b83-3300-4050-8f58-87249e7eb316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Sequential):\n",
    "    ...\n",
    "    \n",
    "    @overload\n",
    "    def __init__(self, *blocks: nn.Module) -> None:\n",
    "        # Give blocks explicitly \n",
    "        \n",
    "    @overload\n",
    "    def __init__(self, arg: 'OrderedDict[str, Module]') -> None:\n",
    "        # Give blocks explicitly as dict\n",
    "        \n",
    "    @overload\n",
    "    def __init__(self, *, **HP) -> None:\n",
    "        # Construct from Hyperparameters.\n",
    "        \n",
    "        \n",
    "    def __init__(self, *blocks, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee17a6f-11b7-4b8e-b28e-12879a404e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83500fd0-a401-4f9b-81dd-887ec63f2fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
