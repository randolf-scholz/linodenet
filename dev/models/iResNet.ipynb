{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa35f1b2-5d20-402f-8568-6aea788fe37e",
   "metadata": {},
   "source": [
    "# iResNet-Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0261cdb2-e7c9-41ed-93a6-4a66118a49c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b3f983-377c-44e2-8ebb-21af8aecd91e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tsdm\n",
    "import warnings\n",
    "import torch\n",
    "import math\n",
    "import torchdiffeq\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import GRUCell\n",
    "import numpy as np\n",
    "from opt_einsum import contract\n",
    "from tqdm.auto import trange\n",
    "from typing import Union, Callable\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60efefc5-3083-4776-ac0a-934c38e6cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsdm.util import ACTIVATIONS, deep_dict_update, deep_kval_update, scaled_norm\n",
    "\n",
    "ACTIVATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531e7186-ceee-48b3-a989-88e502429b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearContraction(torch.jit.ScriptModule):\n",
    "    __constants__ = [\"input_size\", \"output_size\"]\n",
    "    input_size: int\n",
    "    output_size: int\n",
    "    weight: Tensor\n",
    "    bias: Union[Tensor, None]\n",
    "\n",
    "    def __init__(self, input_size: int, output_size: int, bias: bool = True) -> None:\n",
    "        super(LinearContraction, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_size, input_size))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_size))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return \"input_size={}, output_size={}, bias={}\".format(\n",
    "            self.input_size, self.output_size, self.bias is not None\n",
    "        )\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def forward(self, input: Tensor, c: float = 0.97) -> Tensor:\n",
    "        σ_max = torch.linalg.norm(self.weight, ord=2)\n",
    "        fac = torch.minimum(c / σ_max, torch.ones(1))\n",
    "        return nn.functional.linear(input, fac * self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d83d51a-c49c-4b91-82d7-d1a8806aeccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LinearContraction(\n",
    "    n_samples: int = 10_000, dim_in: int = None, dim_out: int = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Tests empirically whether the LinearContraction module is a contraction.\n",
    "    \"\"\"\n",
    "    n_samples = n_samples or np.random.randint(low=1000, high=10_000)\n",
    "    dim_in = dim_in or np.random.randint(low=2, high=100)\n",
    "    dim_out = dim_out or np.random.randint(low=2, high=100)\n",
    "    x = torch.randn(n_samples, dim_in)\n",
    "    y = torch.randn(n_samples, dim_in)\n",
    "    distances = torch.cdist(x, y)\n",
    "\n",
    "    model = LinearContraction(dim_in, dim_out)\n",
    "    xhat = model(x)\n",
    "    yhat = model(y)\n",
    "    latent_distances = torch.cdist(xhat, yhat)\n",
    "\n",
    "    assert torch.all(latent_distances <= distances)\n",
    "\n",
    "    scaling_factor = (latent_distances / distances).flatten()\n",
    "    fig, ax = plt.subplots(figsize=(8, 4), tight_layout=True)\n",
    "    tsdm.util.visualize_distribution(scaling_factor, ax=ax)\n",
    "    ax.set_title(\n",
    "        f\"LinearContraction -- Scaling Factor Distribution (samples:{n_samples}, dim-in:{dim_in}, dim-out:{dim_out})\"\n",
    "    )\n",
    "    ax.set_xlabel(r\"$s(x, y) = \\frac{\\|\\phi(x)-\\phi(y)\\|}{\\|x-y\\|}$\")\n",
    "    ax.set_ylabel(r\"density $p(s\\mid x, y)$ where $x_i,y_i\\sim \\mathcal N(0,1)$\")\n",
    "\n",
    "\n",
    "test_LinearContraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab3b76-800c-4201-abf2-63f929ee3467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class iResNetBlock(torch.jit.ScriptModule):\n",
    "    __constants__ = [\"input_size\", \"output_size\", \"maxiter\"]\n",
    "    input_size: int\n",
    "    hidden_size: int\n",
    "    output_size: int\n",
    "    maxiter: int\n",
    "    bias: bool\n",
    "\n",
    "    HP = {\n",
    "        \"activation\": \"ReLU\",\n",
    "        \"activation_config\": {\"inplace\": False},\n",
    "        \"bias\": True,\n",
    "        \"hidden_size\": None,\n",
    "        \"input_size\": None,\n",
    "        \"maxiter\": 100,\n",
    "    }\n",
    "\n",
    "    def __init__(self, input_size: int, **HP):\n",
    "        super(iResNetBlock, self).__init__()\n",
    "\n",
    "        self.HP[\"input_size\"] = input_size\n",
    "        tsdm.utils.deep_dict_update(self.HP, HP)\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = input_size\n",
    "        self.hidden_size = self.HP[\"hidden_size\"] or math.ceil(math.sqrt(input_size))\n",
    "\n",
    "        self.maxiter = self.HP[\"maxiter\"]\n",
    "        self.bias = self.HP[\"bias\"]\n",
    "\n",
    "        activation = ACTIVATIONS[self.HP[\"activation\"]]\n",
    "\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            LinearContraction(self.input_size, self.hidden_size, self.bias),\n",
    "            LinearContraction(self.hidden_size, self.input_size, self.bias),\n",
    "            activation(**self.HP[\"activation_config\"]),\n",
    "        )\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def forward(self, x):\n",
    "        \"\"\"n-dim to n-dim\"\"\"\n",
    "\n",
    "        xhat = x + self.bottleneck(x)\n",
    "\n",
    "        return xhat\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def inverse(self, y, maxiter: int = 1000, rtol: float = 1e-05, atol: float = 1e-08):\n",
    "        #         with torch.no_grad():\n",
    "        xhat = y.clone()\n",
    "        xhat_dash = y.clone()\n",
    "        residual = torch.zeros_like(y)\n",
    "\n",
    "        for k in range(self.maxiter):\n",
    "            xhat_dash = y - self.bottleneck(xhat)\n",
    "            residual = torch.abs(xhat_dash - xhat) - rtol * torch.absolute(xhat)\n",
    "\n",
    "            if torch.all(residual <= atol):\n",
    "                return xhat_dash\n",
    "            else:\n",
    "                xhat = xhat_dash\n",
    "\n",
    "        warnings.warn(\n",
    "            f\"No convergence in {maxiter} iterations. Max residual:{torch.max(residual)} > {atol}.\"\n",
    "        )\n",
    "        return xhat_dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053b2ef8-8876-4b79-954d-b07734266883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_iResNetBlock(\n",
    "    n_samples: int = 1_000, input_size: int = None, hidden_size: int = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Tests empirically whether the iResNetBlock is indeed invertible.\n",
    "    \"\"\"\n",
    "    n_samples = 10_000 or np.random.randint(low=1000, high=10_000)\n",
    "    input_size = np.random.randint(low=2, high=100)\n",
    "    hidden_size = np.random.randint(low=2, high=100)\n",
    "    HP = {}\n",
    "\n",
    "    model = iResNetBlock(input_size, **HP)\n",
    "\n",
    "    x = torch.randn(n_samples, input_size)\n",
    "    y = torch.randn(n_samples, input_size)\n",
    "\n",
    "    fx = model(x)\n",
    "    xhat = model.inverse(fx)\n",
    "\n",
    "    ify = model.inverse(y)\n",
    "    yhat = model(ify)\n",
    "\n",
    "    dist_lmap = tsdm.utils.scaled_norm(x - fx, axis=-1)\n",
    "    dist_rmap = tsdm.utils.scaled_norm(y - ify, axis=-1)\n",
    "    err_linverse = tsdm.utils.scaled_norm(x - xhat, axis=-1)\n",
    "    err_rinverse = tsdm.utils.scaled_norm(y - yhat, axis=-1)\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        ncols=2, nrows=2, figsize=(10, 5), tight_layout=True, sharex=\"row\", sharey=\"row\"\n",
    "    )\n",
    "    tsdm.utils.visualize_distribution(err_linverse, ax=ax[0, 0])\n",
    "    tsdm.utils.visualize_distribution(err_rinverse, ax=ax[0, 1])\n",
    "    tsdm.utils.visualize_distribution(dist_lmap, ax=ax[1, 0])\n",
    "    tsdm.utils.visualize_distribution(dist_rmap, ax=ax[1, 1])\n",
    "\n",
    "    assert torch.quantile(err_linverse, 0.99) <= 10**-6\n",
    "    assert torch.quantile(err_rinverse, 0.99) <= 10**-6\n",
    "    #     assert torch.mean()\n",
    "\n",
    "    #     ax.set_title(F\"Scaling Factor Distribution (samples:{n_samples}, dim-in:{dim_in}, dim-out:{dim_out}))\n",
    "    ax[0, 0].set_xlabel(r\"$r_\\text{left}(x) = \\|x - \\phi^{-1}(\\phi(x))\\|$\")\n",
    "    ax[0, 0].set_ylabel(r\"$p(r_\\text{left} \\mid x)$ where $x_i \\sim \\mathcal N(0,1)$\")\n",
    "    ax[0, 1].set_xlabel(r\"$r_\\text{right}(y) = \\|y - \\phi(\\phi^{-1}(y))\\|$\")\n",
    "    ax[0, 1].set_ylabel(r\"$p(r_\\text{right}\\mid y)$ where $y_j \\sim \\mathcal N(0,1)$\")\n",
    "\n",
    "    ax[1, 0].set_xlabel(r\"$d_\\text{left}(x) = \\|x - \\phi(x)\\|$\")\n",
    "    ax[1, 0].set_ylabel(r\"$p(d_\\text{left} \\mid x)$ where $x_i \\sim \\mathcal N(0,1)$\")\n",
    "    ax[1, 1].set_xlabel(r\"$d_\\text{right}(y) = \\|y - \\phi^{-1}(y)\\|$\")\n",
    "    ax[1, 1].set_ylabel(r\"$p(d_\\text{right} \\mid y)$ where $y_j \\sim \\mathcal N(0,1)$\")\n",
    "    fig.suptitle(\n",
    "        f\"iResNetBlock -- Inversion property (samples:{n_samples}, dim-in:{input_size}, dim-hidden:{hidden_size})\",\n",
    "        fontsize=16,\n",
    "    )\n",
    "\n",
    "\n",
    "test_iResNetBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9368ea7-338f-48f0-889c-81e894d02984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printgradnorm(self, grad_input, grad_output):\n",
    "    print(\"Inside \" + self.__class__.__name__ + \" backward\")\n",
    "    print(\"Inside class:\" + self.__class__.__name__)\n",
    "    print(\"\")\n",
    "    print(\"grad_input: \", type(grad_input))\n",
    "    print(\"grad_input[0]: \", type(grad_input[0]))\n",
    "    print(\"grad_output: \", type(grad_output))\n",
    "    print(\"grad_output[0]: \", type(grad_output[0]))\n",
    "    print(\"\")\n",
    "    print(\"grad_input size:\", grad_input[0].size())\n",
    "    print(\"grad_output size:\", grad_output[0].size())\n",
    "    print(\"grad_input norm:\", grad_input[0].norm())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7a6678-825f-4eb4-b5a9-b4e4a40af6a2",
   "metadata": {},
   "source": [
    "Consider:\n",
    "- loss $\\ell(x, \\hat x)$\n",
    "- $\\hat x =  F^{-1}(z, \\theta)$ where $F(z) = z + g(z, \\theta)$.\n",
    "    - The inverse solves the fixed point equation $\\hat x(z,\\theta) = z - g(\\hat x(z, \\theta), \\theta)$\n",
    "- Then \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell(x, \\hat x)}{\\partial \\theta} \n",
    "= \\frac{\\partial \\ell(x, \\hat x)}{\\partial \\hat x}\\frac{\\partial \\hat x}{\\partial \\theta} \n",
    "$$\n",
    "\n",
    "Where , since $\\hat x(z,\\theta) = z - g(\\hat x(z, \\theta), \\theta)$ we have\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat x}{\\partial \\theta} \n",
    "= - \\frac{\\partial g}{\\partial \\hat x}\\frac{\\partial \\hat x}{\\partial \\theta} - \\frac{\\partial g}{\\partial \\theta} \n",
    "\\implies\n",
    "\\Big(I - \\frac{\\partial g}{\\partial \\hat x}\\Big)\\frac{\\partial \\hat x}{\\partial \\theta}  = - \\frac{\\partial g}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "Plugging this into the loss we have\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\ell(x, \\hat x)}{\\partial \\theta} \n",
    "&= \\frac{\\partial \\ell(x, \\hat x)}{\\partial \\hat x}\\bigg(-\\Big(I - \\frac{\\partial g}{\\partial \\hat x}\\Big)^{-1}\\frac{\\partial g}{\\partial \\theta}\\bigg)\n",
    "\\\\\n",
    "&= a^T \\frac{\\partial g}{\\partial \\theta} \\qquad\\text{where }  \\Big(I - \\frac{\\partial g}{\\partial \\hat x}\\Big)a  = - \\frac{\\partial \\ell}{\\partial \\hat x}\n",
    "\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb49b950-a08f-4766-b4a1-3d5c199d27c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class inverse_iteration(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, bottleneck):\n",
    "        x = input.clone()\n",
    "        for k in range(1000):\n",
    "            # fixed point iteration\n",
    "            x = input - bottleneck(x)\n",
    "\n",
    "        ctx.save_for_backward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b37a42-ec92-4798-83cd-d7b5703c4f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10_000 or np.random.randint(low=1000, high=10_000)\n",
    "input_size = np.random.randint(low=2, high=100)\n",
    "hidden_size = np.random.randint(low=2, high=100)\n",
    "HP = {}\n",
    "x = torch.randn(n_samples, input_size)\n",
    "y = torch.randn(n_samples, input_size)\n",
    "model = iResNetBlock(input_size, **HP)\n",
    "modelB = LinearContraction(input_size, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae431a20-a354-4aa3-b00d-51b2f852034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inverse(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e1f27c-eded-4932-b7b0-e8988b9cd429",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_iteration.apply(x, model.bottleneck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2527efa-e5d1-4017-a8d8-f11ccfa0845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.norm(model.inverse(modelB(model(x)))).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41caf723-8505-494b-b143-2e856bc83b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class iResNet(torch.jit.ScriptModule):\n",
    "    HP = {\n",
    "        \"maxiter\": 10,\n",
    "        \"input_size\": None,\n",
    "        \"dropout\": None,\n",
    "        \"bias\": True,\n",
    "        \"nBlocks\": 5,\n",
    "        \"iResNetBlock\": {\n",
    "            \"input_size\": None,\n",
    "            \"activation\": \"ReLU\",\n",
    "            \"activation_config\": {\"inplace\": False},\n",
    "            \"bias\": True,\n",
    "            \"hidden_size\": None,\n",
    "            \"maxiter\": 100,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    input_size: int\n",
    "    output_size: int\n",
    "    nblocks: int\n",
    "\n",
    "    def __init__(self, input_size, **HP):\n",
    "        super(iResNet, self).__init__()\n",
    "\n",
    "        self.HP[\"input_size\"] = input_size\n",
    "        tsdm.utils.deep_dict_update(self.HP, HP)\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = input_size\n",
    "        self.HP[\"iResNetBlock\"][\"input_size\"] = self.input_size\n",
    "\n",
    "        self.nblocks = self.HP[\"nBlocks\"]\n",
    "        self.maxiter = self.HP[\"maxiter\"]\n",
    "        self.bias = self.HP[\"bias\"]\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[iResNetBlock(**self.HP[\"iResNetBlock\"]) for k in range(self.nblocks)]\n",
    "        )\n",
    "\n",
    "        self.reversed_blocks = nn.Sequential(*reversed(self.blocks))\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def forward(self, x):\n",
    "        \"\"\"n-dim to n-dim\"\"\"\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def inverse(self, y, maxiter: int = 100, rtol: float = 1e-05, atol: float = 1e-08):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for block in self.reversed_blocks:\n",
    "                # `reversed` does not work in torchscript v1.8.1\n",
    "                y = block.inverse(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def alt_inverse(\n",
    "        self, y, maxiter: int = 1000, rtol: float = 1e-05, atol: float = 1e-08\n",
    "    ):\n",
    "\n",
    "        xhat = y.clone()\n",
    "        xhat_dash = y.clone()\n",
    "        residual = torch.zeros_like(y)\n",
    "\n",
    "        for k in range(self.maxiter):\n",
    "            xhat_dash = y - self(xhat)\n",
    "            residual = torch.abs(xhat_dash - xhat) - rtol * torch.absolute(xhat)\n",
    "\n",
    "            if torch.all(residual <= atol):\n",
    "                return xhat_dash\n",
    "            else:\n",
    "                xhat = xhat_dash\n",
    "\n",
    "        warnings.warn(\n",
    "            f\"No convergence in {maxiter} iterations. Max residual:{torch.max(residual)} > {atol}.\"\n",
    "        )\n",
    "        return xhat_dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d7d77f-c35a-4754-8951-30eca09f0282",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nn.Sequential(nn.Linear(10, 11), nn.Linear(11, 12), nn.Linear(12, 13)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718cb40-a9fc-4dca-9d04-7a2d76bc4246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "n_samples = 10_000 or np.random.randint(low=1000, high=10_000)\n",
    "input_size = np.random.randint(low=2, high=100)\n",
    "nBlocks = np.random.randint(low=2, high=100)\n",
    "HP = {\"nBlocks\": nBlocks}\n",
    "print(f\"{n_samples=}, {input_size=},  {nBlocks=}\")\n",
    "model = iResNet(input_size, **HP)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d74572-7807-4095-84dc-2a95c7778194",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(n_samples, input_size)\n",
    "y = torch.randn(n_samples, input_size)\n",
    "\n",
    "fx = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b9ef7d-78d6-4c6b-80c9-7085b9c1246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat = model.inverse(fx)\n",
    "\n",
    "ify = model.inverse(y)\n",
    "yhat = model(ify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d542ad4-e2c5-4f94-9173-60c81b0dbe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_lmap = tsdm.utils.scaled_norm(x - fx, axis=-1)\n",
    "dist_rmap = tsdm.utils.scaled_norm(y - ify, axis=-1)\n",
    "err_linverse = tsdm.utils.scaled_norm(x - xhat, axis=-1)\n",
    "err_rinverse = tsdm.utils.scaled_norm(y - yhat, axis=-1)\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    ncols=2, nrows=2, figsize=(10, 5), tight_layout=True, sharex=\"row\", sharey=\"row\"\n",
    ")\n",
    "tsdm.utils.visualize_distribution(err_linverse, ax=ax[0, 0])\n",
    "tsdm.utils.visualize_distribution(err_rinverse, ax=ax[0, 1])\n",
    "tsdm.utils.visualize_distribution(dist_lmap, ax=ax[1, 0])\n",
    "tsdm.utils.visualize_distribution(dist_rmap, ax=ax[1, 1])\n",
    "\n",
    "# assert torch.quantile(err_linverse, 0.99) <= 10**-6\n",
    "# assert torch.quantile(err_rinverse, 0.99) <= 10**-6\n",
    "\n",
    "#     ax.set_title(F\"Scaling Factor Distribution (samples:{n_samples}, dim-in:{dim_in}, dim-out:{dim_out}))\n",
    "ax[0, 0].set_xlabel(r\"$r_\\text{left}(x) = \\|x - \\phi^{-1}(\\phi(x))\\|$\")\n",
    "ax[0, 0].set_ylabel(r\"$p(r_\\text{left} \\mid x)$ where $x_i \\sim \\mathcal N(0,1)$\")\n",
    "ax[0, 1].set_xlabel(r\"$r_\\text{right}(y) = \\|y - \\phi(\\phi^{-1}(y))\\|$\")\n",
    "ax[0, 1].set_ylabel(r\"$p(r_\\text{right}\\mid y)$ where $y_j \\sim \\mathcal N(0,1)$\")\n",
    "\n",
    "ax[1, 0].set_xlabel(r\"$d_\\text{left}(x) = \\|x - \\phi(x)\\|$\")\n",
    "ax[1, 0].set_ylabel(r\"$p(d_\\text{left} \\mid x)$ where $x_i \\sim \\mathcal N(0,1)$\")\n",
    "ax[1, 1].set_xlabel(r\"$d_\\text{right}(y) = \\|y - \\phi^{-1}(y)\\|$\")\n",
    "ax[1, 1].set_ylabel(r\"$p(d_\\text{right} \\mid y)$ where $y_j \\sim \\mathcal N(0,1)$\")\n",
    "fig.suptitle(\n",
    "    f\"{model.__class__.__name__} -- Inversion property (samples:{n_samples}, dim-in:{input_size})\",\n",
    "    fontsize=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57646dc0-219d-4e07-9229-fd8a07ce66e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = model(x)\n",
    "xhat = model.alt_inverse(fx)\n",
    "\n",
    "ify = model.alt_inverse(y)\n",
    "yhat = model(ify)\n",
    "\n",
    "dist_lmap = tsdm.utils.scaled_norm(x - fx, axis=-1)\n",
    "dist_rmap = tsdm.utils.scaled_norm(y - ify, axis=-1)\n",
    "err_linverse = tsdm.utils.scaled_norm(x - xhat, axis=-1)\n",
    "err_rinverse = tsdm.utils.scaled_norm(y - yhat, axis=-1)\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    ncols=2, nrows=2, figsize=(10, 5), tight_layout=True, sharex=\"row\", sharey=\"row\"\n",
    ")\n",
    "tsdm.utils.visualize_distribution(err_linverse, ax=ax[0, 0])\n",
    "tsdm.utils.visualize_distribution(err_rinverse, ax=ax[0, 1])\n",
    "tsdm.utils.visualize_distribution(dist_lmap, ax=ax[1, 0])\n",
    "tsdm.utils.visualize_distribution(dist_rmap, ax=ax[1, 1])\n",
    "\n",
    "# assert torch.quantile(err_linverse, 0.99) <= 10**-6\n",
    "# assert torch.quantile(err_rinverse, 0.99) <= 10**-6\n",
    "\n",
    "#     ax.set_title(F\"Scaling Factor Distribution (samples:{n_samples}, dim-in:{dim_in}, dim-out:{dim_out}))\n",
    "ax[0, 0].set_xlabel(r\"$r_\\text{left}(x) = \\|x - \\phi^{-1}(\\phi(x))\\|$\")\n",
    "ax[0, 0].set_ylabel(r\"$p(r_\\text{left} \\mid x)$ where $x_i \\sim \\mathcal N(0,1)$\")\n",
    "ax[0, 1].set_xlabel(r\"$r_\\text{right}(y) = \\|y - \\phi(\\phi^{-1}(y))\\|$\")\n",
    "ax[0, 1].set_ylabel(r\"$p(r_\\text{right}\\mid y)$ where $y_j \\sim \\mathcal N(0,1)$\")\n",
    "\n",
    "ax[1, 0].set_xlabel(r\"$d_\\text{left}(x) = \\|x - \\phi(x)\\|$\")\n",
    "ax[1, 0].set_ylabel(r\"$p(d_\\text{left} \\mid x)$ where $x_i \\sim \\mathcal N(0,1)$\")\n",
    "ax[1, 1].set_xlabel(r\"$d_\\text{right}(y) = \\|y - \\phi^{-1}(y)\\|$\")\n",
    "ax[1, 1].set_ylabel(r\"$p(d_\\text{right} \\mid y)$ where $y_j \\sim \\mathcal N(0,1)$\")\n",
    "fig.suptitle(\n",
    "    f\"{model.__class__.__name__} -- Inversion property (samples:{n_samples}, dim-in:{input_size}, dim-hidden:{hidden_size})\",\n",
    "    fontsize=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6f455d-7465-4f0f-b643-5a6015ad39f2",
   "metadata": {},
   "source": [
    "Anyone know how to register a custom backward function to a pytorch module? In implementing the i-ResNet Architecture, I roughly have\n",
    "\n",
    "```python\n",
    "class iResNetBlock(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.Linear(hidden_size, input_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.bottleneck(x)\n",
    "    \n",
    "    def inverse(self, y):\n",
    "        x = y.clone()\n",
    "        while not converged:\n",
    "            # fixed point iteration\n",
    "            x = y - self.bottleneck(x)\n",
    "   \n",
    "        return x\n",
    "        \n",
    "    def inverse_backwards():\n",
    "        pass\n",
    "    \n",
    "class iResNet(nn.Module):\n",
    "    def __init__(self, num_blocks):\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            iResNetBlock for k in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "    \n",
    "    def inverse(self, y):\n",
    "        for block in reversed(self.blocks):\n",
    "            y = block.inverse(y)\n",
    "        return y\n",
    "```\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f84234-74a6-4d7d-8be4-1e4d674437ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
