{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1052bdea-da37-473c-a559-6675530fec7d",
   "metadata": {},
   "source": [
    "# Custom C++ extension\n",
    "\n",
    "\n",
    "References: \n",
    "- https://pytorch.org/tutorials/advanced/cpp_extension.html\n",
    "- https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c224a529-2011-4f2c-b703-fcad293fd300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.cpp_extension import load\n",
    "\n",
    "lltm_cpp = load(name=\"lltm_cpp\", sources=[\"lltm.cpp\"], verbose=True)\n",
    "help(lltm_cpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a6865b-8abc-4e63-8885-0ffbf394e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn, jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fceea85-0977-497b-bb40-6b8bf3e56a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lltm_cpp.forward(\n",
    "    torch.randn(3, 3),\n",
    "    torch.randn(3, 6),\n",
    "    torch.randn(3, 3),\n",
    "    torch.randn(3, 3),\n",
    "    torch.randn(3, 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ee71f0-6459-4489-8cf0-a7dcbaac0667",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def forward(\n",
    "        self, a0: Tensor, a1: Tensor, a2: Tensor, a3: Tensor, a4: Tensor\n",
    "    ) -> Tensor:\n",
    "        return lltm_cpp.forward(a0, a1, a2, a3, a4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f72df0-2254-4eb4-a40e-62bc7292f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = torch.randn(3, 3)\n",
    "a1 = torch.randn(3, 6)\n",
    "a2 = torch.randn(3, 3)\n",
    "a3 = torch.randn(3, 3)\n",
    "a4 = torch.randn(3, 3)\n",
    "\n",
    "model = MyModule()\n",
    "model(a0, a1, a2, a3, a4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d8ac8b-7d2a-486a-8a03-430d194473de",
   "metadata": {},
   "outputs": [],
   "source": [
    "jit.script(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75baf0a4-c997-4f1a-9c98-70278d5ad26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138c2a80-596c-41a5-b763-46a09fbcd7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ff31f-8f3c-4104-a16e-cac905ba0809",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(lltm_cpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd49c4-e915-4ae1-af1d-fe6d015649be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import os\n",
    "\n",
    "# C++ ops compiled into lltm_ops.so\n",
    "torch.ops.load_library(\"lltm_ops.so\")\n",
    "\n",
    "\n",
    "class LLTMFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weights, bias, old_h, old_cell):\n",
    "        outputs = torch.ops.lltm_ops.lltm_forward(input, weights, bias, old_h, old_cell)\n",
    "        new_h, new_cell = outputs[:2]\n",
    "        variables = outputs[1:] + [weights]\n",
    "        ctx.save_for_backward(*variables)\n",
    "\n",
    "        return new_h, new_cell\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_h, grad_cell):\n",
    "        outputs = torch.ops.lltm_ops.lltm_backward(\n",
    "            grad_h.contiguous(), grad_cell.contiguous(), *ctx.saved_variables\n",
    "        )\n",
    "        d_old_h, d_input, d_weights, d_bias, d_old_cell = outputs\n",
    "        return d_input, d_weights, d_bias, d_old_h, d_old_cell\n",
    "\n",
    "\n",
    "class LLTM(torch.nn.Module):\n",
    "    def __init__(self, input_features, state_size):\n",
    "        super(LLTM, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.state_size = state_size\n",
    "        self.weights = torch.nn.Parameter(\n",
    "            torch.empty(3 * state_size, input_features + state_size)\n",
    "        )\n",
    "        self.bias = torch.nn.Parameter(torch.empty(3 * state_size))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.state_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, +stdv)\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        return LLTMFunction.apply(input, self.weights, self.bias, *state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d4e6a-7a03-4ff6-b580-ed5906c25b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
