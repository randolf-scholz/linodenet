{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, jit, nn\n",
    "from torch.optim import AdamW\n",
    "from torchinfo import summary\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "\n",
    "from linodenet.models import LatentStateSpaceModel as LSSM\n",
    "from linodenet.models.embeddings import ConcatEmbedding, ConcatProjection\n",
    "from linodenet.models.encoders.invertible_layers import (\n",
    "    LinearContraction,\n",
    "    NaiveLinearContraction,\n",
    "    iResNetBlock,\n",
    "    iSequential,\n",
    ")\n",
    "from linodenet.models.filters import LinearFilter, NonLinearFilter, SequentialFilter\n",
    "from linodenet.models.system import LinODECell\n",
    "from linodenet.utils import ReZeroCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "\n",
    "# Define the Lotka-Volterra equations\n",
    "def lotka_volterra(t, y, a, b, c, d):\n",
    "    x, y = y\n",
    "    dx_dt = a * x - b * x * y\n",
    "    dy_dt = -c * y + d * x * y\n",
    "    return [dx_dt, dy_dt]\n",
    "\n",
    "\n",
    "# Set the parameters\n",
    "a = 3.0  # prey growth rate\n",
    "b = 2.0  # predation rate\n",
    "c = 3.0  # predator death rate\n",
    "d = 1.0  # conversion factor of prey to predator\n",
    "\n",
    "# Set the initial conditions\n",
    "x0 = 2.0  # initial prey population\n",
    "y0 = 1.0  # initial predator population\n",
    "\n",
    "# Set the time span\n",
    "T_MIN = 0.0\n",
    "T_MAX = 30.0\n",
    "num_points = 1000\n",
    "\n",
    "# Solve the equations using solve_ivp\n",
    "sol = solve_ivp(\n",
    "    lotka_volterra,\n",
    "    [T_MIN, T_MAX],\n",
    "    [x0, y0],\n",
    "    args=(a, b, c, d),\n",
    "    dense_output=True,\n",
    ")\n",
    "\n",
    "# Generate time points for evaluation\n",
    "t_eval = np.linspace(T_MIN, T_MAX, num_points)\n",
    "\n",
    "# Evaluate the solution at the time points\n",
    "sol_eval = sol.sol(t_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "\n",
    "noise = np.random.gamma(shape=20, scale=1 / 20, size=(N, 1)).clip(0.5, 1.5)\n",
    "\n",
    "T = np.sort(np.random.uniform(T_MIN, T_MAX, N))\n",
    "X = noise * sol.sol(T).T\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 4), constrained_layout=True)\n",
    "\n",
    "# Plot the populations over time\n",
    "ax.plot(t_eval, sol_eval[0], label=\"Prey\")\n",
    "ax.plot(t_eval, sol_eval[1], label=\"Predator\")\n",
    "ax.plot(T, X, \".\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Population\")\n",
    "ax.set_title(\"Lotka-Volterra Equations\")\n",
    "ax.legend()\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# standardize\n",
    "T = (T - T.min()) / (T.max() - T.min())\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "m_train = T < 0.6\n",
    "m_test = T > 0.6\n",
    "T_train = T[m_train]\n",
    "T_test = T[m_test]\n",
    "X_train = X[m_train]\n",
    "X_test = X[m_test]\n",
    "\n",
    "plt.plot(T_train, X_train, \".\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent_size = 64\n",
    "input_size = 2\n",
    "\n",
    "x = torch.randn(input_size)\n",
    "z = torch.randn(latent_size)\n",
    "dta = torch.rand(1)\n",
    "dtb = torch.rand(1)\n",
    "T = torch.tensor(T, dtype=torch.float32)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "T_train = torch.tensor(T_train, dtype=torch.float32)\n",
    "T_test = torch.tensor(T_test, dtype=torch.float32)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Encoder = iSequential(\n",
    "    ConcatEmbedding(input_size, latent_size),\n",
    "    iResNetBlock(\n",
    "        nn.Sequential(\n",
    "            LinearContraction(latent_size, latent_size, L=0.99),\n",
    "            LinearContraction(latent_size, latent_size, L=0.99),\n",
    "            ReZeroCell(),\n",
    "        )\n",
    "    ),\n",
    "    iResNetBlock(\n",
    "        nn.Sequential(\n",
    "            LinearContraction(latent_size, latent_size, L=0.99),\n",
    "            LinearContraction(latent_size, latent_size, L=0.99),\n",
    "            ReZeroCell(),\n",
    "        )\n",
    "    ),\n",
    "    iResNetBlock(\n",
    "        nn.Sequential(\n",
    "            LinearContraction(latent_size, latent_size, L=0.99),\n",
    "            LinearContraction(latent_size, latent_size, L=0.99),\n",
    "            ReZeroCell(),\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "Decoder = iSequential(\n",
    "    iResNetBlock(\n",
    "        nn.Sequential(\n",
    "            LinearContraction(latent_size, latent_size, L=0.99),\n",
    "            LinearContraction(latent_size, latent_size, L=0.99),\n",
    "            ReZeroCell(),\n",
    "        )\n",
    "    ),\n",
    "    iResNetBlock(\n",
    "        nn.Sequential(\n",
    "            LinearContraction(latent_size, latent_size, L=0.99),\n",
    "            LinearContraction(latent_size, latent_size, L=0.99),\n",
    "            ReZeroCell(),\n",
    "        )\n",
    "    ),\n",
    "    iResNetBlock(\n",
    "        nn.Sequential(\n",
    "            LinearContraction(latent_size, latent_size, L=0.99),\n",
    "            LinearContraction(latent_size, latent_size, L=0.99),\n",
    "            ReZeroCell(),\n",
    "        )\n",
    "    ),\n",
    "    ConcatProjection(latent_size, input_size),\n",
    ")\n",
    "\n",
    "assert torch.allclose(x, Encoder.decode(Encoder.encode(x)), atol=1e-3, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Filter = SequentialFilter(\n",
    "    LinearFilter(input_size, autoregressive=True),\n",
    "    NonLinearFilter(input_size, autoregressive=True),\n",
    "    NonLinearFilter(input_size, autoregressive=True),\n",
    ")\n",
    "assert torch.allclose(x, Filter(x, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "System = LinODECell(latent_size)\n",
    "assert torch.allclose(System(dta + dtb, z), System(dta, System(dtb, z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LSSM(\n",
    "    encoder=Encoder,\n",
    "    system=System,\n",
    "    decoder=Decoder,\n",
    "    filter=Filter,\n",
    ").to(device=\"cpu\")\n",
    "# assert torch.allclose(X[:100], model(T[:100], X[:100]))\n",
    "print(f\"Number of named submodules: {len(list(model.named_modules()))}\")\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "contractions = [\n",
    "    m for m in model.encoder.modules() if m.__class__.__name__ == \"LinearContraction\"\n",
    "] + [m for m in model.decoder.modules() if m.__class__.__name__ == \"LinearContraction\"]\n",
    "\n",
    "\n",
    "def reset_all_caches():\n",
    "    for layer in contractions:\n",
    "        layer.reset_cache()\n",
    "\n",
    "\n",
    "print(contractions)\n",
    "reset_all_caches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "optim = AdamW(model.parameters(), lr=0.0001)\n",
    "model.zero_grad(set_to_none=True)\n",
    "reset_all_caches()\n",
    "\n",
    "for k in range(3):\n",
    "    t = torch.sort(torch.rand(100, device=device))[0]\n",
    "    x = torch.randn(100, input_size, device=device)\n",
    "    r = model(t, x)\n",
    "    r.norm().backward()\n",
    "    optim.step()\n",
    "    reset_all_caches()\n",
    "    model.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JIT warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = torch.jit.script(model)\n",
    "print(f\"Number of named submodules: {len(list(model.named_modules()))}\")\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def iter_modules(module):\n",
    "    \"\"\"Helper function needed because named_modules returns wrong results.\"\"\"\n",
    "    yield module\n",
    "    for name, submodule in module.named_children():\n",
    "        if name != \"inverse\":\n",
    "            yield from iter_modules(submodule)\n",
    "\n",
    "\n",
    "contractions = [\n",
    "    m for m in iter_modules(model.encoder) if m.original_name == \"LinearContraction\"\n",
    "] + [m for m in iter_modules(model.decoder) if m.original_name == \"LinearContraction\"]\n",
    "\n",
    "\n",
    "def reset_all_caches():\n",
    "    for layer in contractions:\n",
    "        layer.reset_cache()\n",
    "\n",
    "\n",
    "print(contractions)\n",
    "reset_all_caches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "optim = AdamW(model.parameters(), lr=0.0001)\n",
    "model.zero_grad(set_to_none=True)\n",
    "reset_all_caches()\n",
    "\n",
    "for k in range(2):\n",
    "    t = torch.sort(torch.rand(100, device=device))[0]\n",
    "    x = torch.randn(100, input_size, device=device)\n",
    "    r = model(t, x)\n",
    "    r.norm().backward()\n",
    "    optim.step()\n",
    "    reset_all_caches()\n",
    "    model.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tsdm.random.samplers import SlidingWindowSampler\n",
    "\n",
    "horizon = 1 / 16\n",
    "stride = 1 / 128\n",
    "\n",
    "train_sampler = SlidingWindowSampler(\n",
    "    T_train, horizons=(horizon,), stride=stride, shuffle=True\n",
    ")\n",
    "test_sampler = SlidingWindowSampler(T_test, horizons=(horizon,), stride=stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tsdm.metrics import TimeSeriesMSE\n",
    "\n",
    "loss = TimeSeriesMSE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "n_forecast = 30\n",
    "\n",
    "train_samples = []\n",
    "for horizon in train_sampler:\n",
    "    t = T_train[horizon]\n",
    "    x = X_train[horizon]\n",
    "    y = x.clone()\n",
    "    x[-n_forecast:] = float(\"nan\")\n",
    "    train_samples.append((t, x, y))\n",
    "\n",
    "test_samples = []\n",
    "for horizon in test_sampler:\n",
    "    t = T_test[horizon]\n",
    "    x = X_test[horizon]\n",
    "    y = x.clone()\n",
    "    x[-n_forecast:] = float(\"nan\")\n",
    "    test_samples.append((t, x, y))\n",
    "\n",
    "\n",
    "def collate_fn(\n",
    "    samples: list[tuple[Tensor, Tensor, Tensor]]\n",
    ") -> tuple[Tensor, Tensor, Tensor]:\n",
    "    nan = torch.tensor(float(\"nan\"), device=device)\n",
    "    t_list, x_list, y_list = list(zip(*samples))\n",
    "\n",
    "    return (\n",
    "        pad_sequence(t_list, batch_first=True),\n",
    "        pad_sequence(x_list, batch_first=True, padding_value=nan),\n",
    "        pad_sequence(y_list, batch_first=True, padding_value=nan),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_samples, collate_fn=collate_fn, batch_size=128, shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(test_samples, collate_fn=collate_fn, batch_size=128)\n",
    "infer_loader = DataLoader(test_samples, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_score(model, dloader):\n",
    "    total = torch.tensor(0.0, device=device)\n",
    "\n",
    "    for t, x, y in dloader:\n",
    "        t = t.to(device)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        yhat = model(t, x)\n",
    "        r = loss(y, yhat)\n",
    "        assert r.isfinite()\n",
    "        total += r\n",
    "\n",
    "    # total /= len(dloader)\n",
    "    return total.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_score(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def grad_norm(model):\n",
    "    total = torch.tensor(0.0, device=device)\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total += p.grad.norm() / p.numel()\n",
    "    total = total / len(list(model.parameters()))\n",
    "    return total.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = AdamW(model.parameters())\n",
    "reset_all_caches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in (outer := trange(10000)):\n",
    "    for t, x, y in train_loader:\n",
    "        with torch.no_grad():\n",
    "            t = t.to(device)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "        yhat = model(t, x)\n",
    "        loss_post = loss(y, yhat)\n",
    "        loss_pre = loss(y, model.xhat_pre)\n",
    "        total = loss_pre + loss_post\n",
    "        assert total.isfinite()\n",
    "        total.backward()\n",
    "        grad = grad_norm(model)\n",
    "        # pbar.set_postfix(loss=f\"{r.item():.4f}\", grad=f\"{grad:.4f}\")\n",
    "        optim.step()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        reset_all_caches()\n",
    "    score = test_score(model, test_loader)\n",
    "    outer.set_postfix(\n",
    "        loss_post=f\"{loss_post.item():.4f}\",\n",
    "        loss_pre=f\"{loss_pre.item():.4f}\",\n",
    "        score=f\"{score:.4f}\",\n",
    "        grad=f\"{grad:.4f}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for ax in axes:\n",
    "    t, x, y = next(iter(infer_loader))\n",
    "    t, x, y = t[0], x[0], y[0]\n",
    "    i_forecast = torch.argmax(x[:, 0])\n",
    "    print(len(t), torch.isnan(x).sum() // 2)\n",
    "    t_long = torch.linspace(t.min(), t.max(), 1000)\n",
    "\n",
    "    yhat = model(t.to(device), x.to(device)).cpu().detach()\n",
    "    ax.axvspan(t.min(), t[i_forecast], alpha=0.2)\n",
    "    ax.plot(t, y, \".\", t, yhat);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.system.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
