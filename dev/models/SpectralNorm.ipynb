{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: What is the best way of implementing a LinearContraction leayer in python?\n",
    "\n",
    "I.e. a linear layer with $‚ÄñA‚Äñ_2 = œÉ_{\\max}(A)‚â§ 1$.\n",
    "\n",
    "**TODOs:**\n",
    "\n",
    "- test torch.nn.utils.paramtrizations.spectal_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'  # always print last expr.\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "np.set_printoptions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, jit\n",
    "from torch.linalg import vector_norm\n",
    "\n",
    "torch.set_default_tensor_type(\n",
    "    torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.parametrizations import spectral_norm\n",
    "from torch import nn, jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snm = spectral_norm(nn.Linear(20, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch shipped spectral norm not jitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jit.script(snm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Custom Implemention\n",
    "\n",
    "We will more or less duplicate the torch implementation with some minor improvements\n",
    "\n",
    "- Run init until convergence\n",
    "- Provide option to run forward until convergence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoGradTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "# Inherit from Function\n",
    "class LinearFunction(Function):\n",
    "\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = LinearFunction.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import gradcheck\n",
    "\n",
    "# gradcheck takes a tuple of tensors as input, check if your gradient\n",
    "# evaluated with these tensors are close enough to numerical\n",
    "# approximations and returns True if they all verify this condition.\n",
    "input = (\n",
    "    torch.randn(20, 20, dtype=torch.double, requires_grad=True),\n",
    "    torch.randn(30, 20, dtype=torch.double, requires_grad=True),\n",
    ")\n",
    "test = gradcheck(linear, input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpectralNorm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class SpectralNorm(torch.autograd.Function):\n",
    "    r\"\"\"`‚ÄñA‚Äñ_2=Œª_{ùóÜùñ∫ùóë}(A^ùñ≥A)`.\n",
    "\n",
    "    The spectral norm `‚à•A‚à•_2 ‚âî ùóåùóéùóâ_x ‚à•Ax‚à•_2 / ‚à•x‚à•_2` can be shown to be equal to\n",
    "    `œÉ_\\max(A) = ‚àö{Œª_{ùóÜùñ∫ùóë} (A·µÄA)}`, the largest singular value of `A`.\n",
    "\n",
    "    It can be computed efficiently via Power iteration.\n",
    "\n",
    "    One can show that the derivative is equal to:\n",
    "\n",
    "    .. math::\n",
    "        \\frac{‚àÇ¬Ω‚à•A‚à•_2}/{‚àÇA} = uv·µÄ\n",
    "\n",
    "    where `u,v` are the left/right-singular vector corresponding to `œÉ_\\max`\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    - | `Spectral Normalization for Generative Adversarial Networks\n",
    "        <https://openreview.net/forum?id=B1QRgziT->`_\n",
    "      | Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida\n",
    "      | `International Conference on Learning Representations 2018\n",
    "        <https://iclr.cc/Conferences/2018>`_\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def jvp(ctx: Any, *grad_inputs: Any) -> Any:\n",
    "        u, v = ctx.saved_tensors\n",
    "        return torch.outer(u, v) @ grad_inputs[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx: Any, *tensors: Tensor, **kwargs: Any) -> Tensor:\n",
    "        r\"\"\"Forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ctx\n",
    "        tensors\n",
    "        kwargs\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "        \"\"\"\n",
    "        A = tensors[0]\n",
    "        atol: float = kwargs[\"atol\"] if \"atol\" in kwargs else 1e-6\n",
    "        rtol: float = kwargs[\"rtol\"] if \"rtol\" in kwargs else 1e-6\n",
    "        maxiter: int = kwargs[\"maxiter\"] if \"maxiter\" in kwargs else 1000\n",
    "        m, n, *other = A.shape\n",
    "        assert not other, \"Expected 2D input.\"\n",
    "        # initialize u and v, median should be useful guess.\n",
    "        u = u_next = A.median(dim=1).values\n",
    "        v = v_next = A.median(dim=0).values\n",
    "\n",
    "        for _ in range(maxiter):\n",
    "            u = u_next / torch.norm(u_next)\n",
    "            v = v_next / torch.norm(v_next)\n",
    "            # choose optimal œÉ given u and v: œÉ = argmin ‚ÄñA - œÉuv·µÄ‚Äñ¬≤\n",
    "            œÉ: Tensor = torch.einsum(\"ij, i, j ->\", A, u, v)  # u.T @ A @ v\n",
    "            # Residual: if Av = œÉu and A·µÄu = œÉv\n",
    "\n",
    "            u_next = A @ v\n",
    "            v_next = A.T @ u\n",
    "\n",
    "            # u_next = torch.einsum('ij, ...j->...i', A, v)\n",
    "            # v_next = torch.einsum('ij, ...i->...j', A, u)\n",
    "\n",
    "            œÉu = œÉ * u\n",
    "            œÉv = œÉ * v\n",
    "\n",
    "            ru = u_next - œÉ * u\n",
    "            rv = v_next - œÉ * v\n",
    "            if (\n",
    "                vector_norm(ru) <= rtol * vector_norm(œÉu) + atol\n",
    "                and vector_norm(rv) <= rtol * vector_norm(œÉv) + atol\n",
    "            ):\n",
    "                break\n",
    "\n",
    "        ctx.save_for_backward(u, v)\n",
    "        return œÉ\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx: Any, grad_outputs: Tensor) -> Tensor:\n",
    "        r\"\"\"Backward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ctx\n",
    "        grad_outputs\n",
    "        \"\"\"\n",
    "        u, v = ctx.saved_tensors\n",
    "        return torch.outer(grad_outputs * u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_norm = SpectralNorm.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "inputs = torch.randn(20, 30, dtype=torch.double, requires_grad=True)\n",
    "test = gradcheck(spectral_norm, inputs, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "15.1 ¬± 4.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit.script\n",
    "def spectral_norm(\n",
    "    A: Tensor, atol: float = 1e-6, rtol: float = 1e-6, maxiter: int = 1000\n",
    ") -> Tensor:\n",
    "    r\"\"\"Compute the spectral norm `‚ÄñA‚Äñ_2` by power iteration.\n",
    "\n",
    "    Stopping critertion:\n",
    "    - maxiter reached\n",
    "    - `‚Äñ (A^TA -ŒªI)x ‚Äñ_2 ‚â§ ùóãùóçùóàùóÖ‚ãÖ‚Äñ Œªx ‚Äñ_2 + ùñ∫ùóçùóàùóÖ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A: tensor\n",
    "    atol: float = 1e-4\n",
    "    rtol: float =  1e-3,\n",
    "    maxiter: int = 10\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "\n",
    "    #     with torch.no_grad():\n",
    "    x = torch.randn(n, device=A.device, dtype=A.dtype)\n",
    "    x = x / vector_norm(x)\n",
    "\n",
    "    z = A.T @ (A @ x)\n",
    "    c, d = vector_norm(z, dim=0), vector_norm(x, dim=0)\n",
    "    Œª = c / d\n",
    "    r = z - Œª * x\n",
    "\n",
    "    for _ in range(maxiter):\n",
    "        x = z / c\n",
    "        z = A.T @ (A @ x)\n",
    "        c, d = vector_norm(z, dim=0), vector_norm(x, dim=0)\n",
    "        Œª = c / d\n",
    "        r = z - Œª * x\n",
    "        if vector_norm(r) <= rtol * vector_norm(Œª * x) + atol:\n",
    "            break\n",
    "\n",
    "    œÉ_max = torch.sqrt(Œª)\n",
    "\n",
    "    v = x / vector_norm(x)\n",
    "    u = A @ v / œÉ_max\n",
    "    u /= vector_norm(u)\n",
    "    #     print(u, v, torch.outer(u,v), A@v-Œª*u)\n",
    "\n",
    "    return œÉ_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralNorm(torch.autograd.Function):\n",
    "    r\"\"\"`‚ÄñA‚Äñ_2=Œª_{ùóÜùñ∫ùóë}(A^ùñ≥A)`.\n",
    "\n",
    "    The spectral norm `‚à•A‚à•_2 ‚âî ùóåùóéùóâ_x ‚à•Ax‚à•_2 / ‚à•x‚à•_2` can be shown to be equal to\n",
    "    `œÉ_\\max(A) = ‚àö{Œª_{ùóÜùñ∫ùóë} (A·µÄA)}`, the largest singular value of `A`.\n",
    "\n",
    "    It can be computed efficiently via Power iteration.\n",
    "\n",
    "    One can show that the derivative is equal to:\n",
    "\n",
    "    .. math::\n",
    "        \\frac{‚àÇ¬Ω‚à•A‚à•_2}/{‚àÇA} = uv·µÄ\n",
    "\n",
    "    where `u,v` are the left/right-singular vector corresponding to `œÉ_\\max`\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx, A: Tensor, atol: float = 1e-6, rtol: float = 1e-6, maxiter: int = 1000\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        m, n = A.shape\n",
    "\n",
    "        #     with torch.no_grad():\n",
    "        x = torch.randn(n, device=A.device, dtype=A.dtype)\n",
    "        x = x / vector_norm(x)\n",
    "\n",
    "        z = A.T @ (A @ x)\n",
    "        c, d = vector_norm(z, dim=0), vector_norm(x, dim=0)\n",
    "        Œª = c / d\n",
    "        r = z - Œª * x\n",
    "\n",
    "        for _ in range(maxiter):\n",
    "            x = z / c\n",
    "            z = A.T @ (A @ x)\n",
    "            c, d = vector_norm(z, dim=0), vector_norm(x, dim=0)\n",
    "            Œª = c / d\n",
    "            r = z - Œª * x\n",
    "            if vector_norm(r) <= rtol * vector_norm(Œª * x) + atol:\n",
    "                break\n",
    "\n",
    "        œÉ_max = torch.sqrt(Œª)\n",
    "\n",
    "        #         ctx.u = x/vector_norm(x)\n",
    "        #         ctx.v = z/vector_norm(z)\n",
    "        v = x / vector_norm(x)\n",
    "        u = A @ v / œÉ_max\n",
    "        u /= vector_norm(u)\n",
    "        #         print(u, v, torch.outer(u,v), A@v-Œª*u)\n",
    "\n",
    "        ctx.save_for_backward(u, v)\n",
    "        return œÉ_max\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: Tensor) -> Tensor:\n",
    "        #         u, v = ctx.u, ctx.v\n",
    "        u, v = ctx.saved_tensors\n",
    "        #         print(grad_output, u, v)\n",
    "        return grad_output * torch.outer(u, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test against ground truth\n",
    "\n",
    "**Theorem:** $\\frac{‚àÇ‚ÄñA‚Äñ_2}{‚àÇA} = u_1v_1^ùñ≥$, if $A = ‚àë_i œÉ_i u_iv_i^ùñ≥$ is the SVD of $A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ortho_group, dirichlet\n",
    "import numpy as np\n",
    "\n",
    "M, N = 64, 128\n",
    "K = min(M, N)\n",
    "U = ortho_group.rvs(M)\n",
    "V = ortho_group.rvs(N)\n",
    "œÉ = dirichlet.rvs(np.ones(min(M, N))).squeeze()\n",
    "œÉ = np.flip(np.sort(œÉ))\n",
    "œÉt = œÉ[0]\n",
    "X = np.einsum(\"i, mi, ni -> mn\", œÉ, U[:, :K], V[:, :K])\n",
    "X = torch.tensor(X).double()\n",
    "H = torch.randn(M, N).double()\n",
    "u = torch.tensor(U[:, 0])\n",
    "v = torch.tensor(V[:, 0])\n",
    "gt = torch.outer(u, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linalg.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {\n",
    "    \"norm\": lambda X: torch.linalg.norm(X, ord=2),\n",
    "    \"matrix_norm\": lambda X: torch.linalg.matrix_norm(X, ord=2),\n",
    "    \"svdvals\": lambda X: torch.linalg.svdvals(X)[0],\n",
    "    \"spectral_norm\": spectral_norm,\n",
    "    \"SpectralNorm\": SpectralNorm.apply,\n",
    "}\n",
    "\n",
    "for name, method in methods.items():\n",
    "    A = torch.nn.Parameter(X.clone(), requires_grad=True)\n",
    "    œÉ_max = method(A)\n",
    "    œÉ_max.backward()\n",
    "    g = A.grad\n",
    "    fward_error = torch.abs(œÉt - œÉ_max).item()\n",
    "    bward_error = torch.sqrt(torch.mean((gt - g) ** 2)).item()\n",
    "    print(f\"{fward_error:.4e}  {bward_error:.4e}\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Speet Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 10 -n 10\n",
    "X = torch.nn.Parameter(torch.randn(M, N), requires_grad=True)\n",
    "œÉ_max = torch.linalg.norm(X, ord=2)\n",
    "œÉ_max.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with matrix_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 10 -n 10\n",
    "X = torch.nn.Parameter(torch.randn(M, N), requires_grad=True)\n",
    "œÉ_max = torch.linalg.matrix_norm(X, ord=2)\n",
    "œÉ_max.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with svdvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 10 -n 10\n",
    "X = torch.nn.Parameter(torch.randn(M, N), requires_grad=True)\n",
    "œÉ_max = torch.linalg.svdvals(X)[0]\n",
    "œÉ_max.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### with spectral_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 10 -n 10\n",
    "X = torch.nn.Parameter(torch.randn(M, N), requires_grad=True)\n",
    "œÉ_max = spectral_norm(X)\n",
    "œÉ_max.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### with SpectralNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 10 -n 10\n",
    "X = torch.nn.Parameter(torch.randn(M, N), requires_grad=True)\n",
    "œÉ_max = SpectralNorm.apply(X)\n",
    "œÉ_max.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
