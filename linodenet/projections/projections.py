r"""Projection Mappings."""
from __future__ import annotations

import logging
from typing import Final

import torch
from torch import Tensor, jit

LOGGER = logging.getLogger(__name__)

__all__: Final[list[str]] = [
    "identity",
    "symmetric",
    "skew_symmetric",
    "orthogonal",
    "diagonal",
    "normal",
]


@jit.script
def identity(x: Tensor) -> Tensor:
    r"""Return x as-is.

    .. math::
        \min_Y Â½âˆ¥X-Yâˆ¥_F^2

    **Signature:** ``(..., n,n) âŸ¶ (..., n, n)``

    Parameters
    ----------
    x: Tensor

    Returns
    -------
    Tensor
    """
    return x


@jit.script
def symmetric(x: Tensor) -> Tensor:
    r"""Return the closest symmetric matrix to X.

    .. math::
        \min_Y Â½âˆ¥X-Yâˆ¥_F^2 s.t. Yáµ€ = Y

    One can show analytically that Y = Â½(X + Xáµ€) is the unique minimizer.

    **Signature:** ``(..., n,n) âŸ¶ (..., n, n)``

    Parameters
    ----------
    x: Tensor

    Returns
    -------
    Tensor
    """
    return (x + x.swapaxes(-1, -2)) / 2


@jit.script
def skew_symmetric(x: Tensor) -> Tensor:
    r"""Return the closest skew-symmetric matrix to X.

    .. math::
        \min_Y Â½âˆ¥X-Yâˆ¥_F^2 s.t. Yáµ€ = -Y

    One can show analytically that Y = Â½(X - Xáµ€) is the unique minimizer.

    **Signature:** ``(..., n,n) âŸ¶ (..., n, n)``

    Parameters
    ----------
    x: Tensor

    Returns
    -------
    Tensor
    """
    return (x - x.swapaxes(-1, -2)) / 2


@jit.script
def normal(x: Tensor) -> Tensor:
    r"""Return the closest normal matrix to X.

    .. math::
        \min_Y Â½âˆ¥X-Yâˆ¥_F^2 s.t. Yáµ€Y = YYáµ€

    The Lagrangian
    --------------

    .. math::
        â„’(Y, Î›) = Â½âˆ¥X-Yâˆ¥_F^2 + âŸ¨Î›, [Y, Yáµ€]âŸ©

    First order necessary KKT condition
    -----------------------------------

    .. math::
            0 &= âˆ‡â„’(Y, Î›) = (Y-X) + Y(Î› + Î›áµ€) - (Î› + Î›áµ€)Y
        \\âŸº Y &= X + [Y, Î›]

    Second order sufficient KKT condition
    -------------------------------------

    .. math::
             âŸ¨âˆ‡h|SâŸ©=0     &âŸ¹ âŸ¨S|âˆ‡Â²â„’|SâŸ© â‰¥ 0
         \\âŸº âŸ¨[Y, Î›]|SâŸ©=0 &âŸ¹ âŸ¨S|ğ•€âŠ—ğ•€ + Î›âŠ—ğ•€ âˆ’ ğ•€âŠ—Î›|SâŸ© â‰¥ 0
         \\âŸº âŸ¨[Y, Î›]|SâŸ©=0 &âŸ¹ âŸ¨S|SâŸ© + âŸ¨[S, Î›]|SâŸ© â‰¥ 0

    **Signature:** ``(..., n,n) âŸ¶ (..., n, n)``

    Parameters
    ----------
    x: Tensor

    Returns
    -------
    Tensor
    """
    raise NotImplementedError("TODO: implement Fixpoint / Gradient based algorithm.")


@jit.script
def orthogonal(x: Tensor) -> Tensor:
    r"""Return the closest orthogonal matrix to X.

    .. math::
        \min_Y Â½âˆ¥X-Yâˆ¥_F^2 s.t. Yáµ€Y = ğ•€ = YYáµ€

    One can show analytically that `Y = UVáµ€` is the unique minimizer,
    where `X=UÎ£Váµ€` is the SVD of `X`.

    **Signature:** ``(..., n,n) âŸ¶ (..., n, n)``

    References
    ----------
    - <https://math.stackexchange.com/q/2215359>_

    Parameters
    ----------
    x: Tensor

    Returns
    -------
    Tensor
    """
    U, _, V = torch.svd(x, some=False, compute_uv=True)
    return torch.einsum("...ij, ...kj -> ...ik", U, V)


@jit.script
def diagonal(x: Tensor) -> Tensor:
    r"""Return the closest diagonal matrix to X.

    .. math::
        \min_Y Â½âˆ¥X-Yâˆ¥_F^2 s.t. Y = ğ•€âŠ™Y

    One can show analytically that `Y = diag(X)` is the unique minimizer.

    **Signature:** ``(..., n,n) âŸ¶ (..., n, n)``

    Parameters
    ----------
    x: Tensor

    Returns
    -------
    Tensor
    """
    eye = torch.eye(x.shape[-1], dtype=torch.bool, device=x.device)
    zero = torch.tensor(0.0, dtype=x.dtype, device=x.device)
    return torch.where(eye, x, zero)
