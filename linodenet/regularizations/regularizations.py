r"""Regularizations for LinODE kernel matrix.

Functional version.
"""

from __future__ import annotations

import logging
from typing import Final, Optional

import torch.linalg
from torch import Tensor, jit

from linodenet import projections

logger = logging.getLogger(__name__)
__all__: Final[list[str]] = [
    "logdetexp",
    "symmetric",
    "skew_symmetric",
    "diagonal",
    "orthogonal",
    "normal",
]


def logdetexp(x: Tensor, p: float = 1.0) -> Tensor:
    r"""Bias $\det(e^A)$ towards 1.

    By Jacobis formula

    .. math::

        \det(e^A) = e^{ùóçùóã(A)} ‚ü∫ \log(\det(e^A)) = ùóçùóã(A) ‚ü∫ \log(\det(A)) = ùóçùóã(\log(A))

    In particular, we can regularize the LinODE model by adding a regularization term of the form

    .. math::

        |ùóçùóã(A)|

    Signature
    ---------
    (..., n,n) ‚ü∂ (...,)

    Parameters
    ----------
    x: Tensor
    p: float, default=1.0

    Returns
    -------
    Tensor
    """
    traces = torch.sum(torch.diagonal(x, dim1=-1, dim2=-2), dim=-1)
    return torch.abs(traces) ** p


@jit.script
def skew_symmetric(x: Tensor, p: Optional[float] = None) -> Tensor:
    r"""Bias the matrix towards being skew-symmetric.

    Signature
    ---------
    (..., n,n) ‚ü∂ (...,)

    Parameters
    ----------
    x: Tensor
    p: Optional[float], If `None` uses Frobenius norm

    Returns
    -------
    Tensor
    """
    r = x - projections.skew_symmetric(x)
    if p is None:
        return torch.linalg.matrix_norm(r)
    return torch.linalg.matrix_norm(r, p)


@jit.script
def symmetric(x: Tensor, p: Optional[float] = None) -> Tensor:
    r"""Bias the matrix towards being symmetric.

    Signature
    ---------
    (..., n,n) ‚ü∂ (...,)

    Parameters
    ----------
    x: Tensor
    p: Optional[float], If `None` uses Frobenius norm

    Returns
    -------
    Tensor
    """
    r = x - projections.symmetric(x)
    if p is None:
        return torch.linalg.matrix_norm(r)
    return torch.linalg.matrix_norm(r, p)


@jit.script
def orthogonal(x: Tensor, p: Optional[float] = None) -> Tensor:
    r"""Bias the matrix towards being orthogonal.

    Note that, given $n√ón$ matrix $X$ with SVD $X=UŒ£V·µÄ$ holds

    .. math::
          &(1) &  ‚Äñ X - Œ†X‚Äñ_F &= ‚Äñ Œ£ - ùïÄ ‚Äñ_F
        \\&(1) &  ‚ÄñX·µÄX - ùïÄ‚Äñ_F &= ‚ÄñŒ£·µÄŒ£ - ùïÄ‚Äñ_F
        \\&(1) &  ‚ÄñXX·µÄ - X‚Äñ_F &= ‚ÄñŒ£Œ£·µÄ - ùïÄ‚Äñ_F

    Signature
    ---------
    (..., n,n) ‚ü∂ (...,)

    Parameters
    ----------
    x: Tensor
    p: Optional[float], If `None` uses Frobenius norm

    Returns
    -------
    Tensor
    """
    r = x - projections.orthogonal(x)
    if p is None:
        return torch.linalg.matrix_norm(r)
    return torch.linalg.matrix_norm(r, p)


@jit.script
def normal(x: Tensor, p: Optional[float] = None) -> Tensor:
    r"""Bias the matrix towards being normal.

    Signature
    ---------
    (..., n,n) ‚ü∂ (...,)

    Parameters
    ----------
    x: Tensor
    p: Optional[float], If `None` uses Frobenius norm

    Returns
    -------
    Tensor
    """
    r = x - projections.normal(x)
    if p is None:
        return torch.linalg.matrix_norm(r)
    return torch.linalg.matrix_norm(r, p)


@jit.script
def diagonal(x: Tensor, p: Optional[float] = None) -> Tensor:
    r"""Bias the matrix towards being diagonal.

    Signature
    ---------
    (..., n,n) ‚ü∂ (...,)

    Parameters
    ----------
    x: Tensor
    p: Optional[float], If `None` uses Frobenius norm

    Returns
    -------
    Tensor
    """
    r = x - projections.diagonal(x)
    if p is None:
        return torch.linalg.matrix_norm(r)
    return torch.linalg.matrix_norm(r, p)
