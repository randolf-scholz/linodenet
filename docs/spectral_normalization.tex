\RequirePackage{iftex}
\RequirePDFTeX
\NeedsTeXFormat{LaTeX2e}
\documentclass[10pt]{article}
\usepackage{ismll-packages}
\usepackage{ismll-mathoperators}
\usepackage{ismll-style}
\usepackage{unicode-symbols}

\title{Derivative of the first order SVD}
\author{Randolf Scholz}
\begin{document}

\maketitle


Consider computing the first order SVD expansion. By the Eckartâ€“Youngâ€“Mirsky theorem, this is equivalent to solving


%
\begin{align*}%
\minimize_{Ïƒï¼Œuï¼Œv} Â½â€–A - Ïƒuv^âŠ¤â€–Â²_F \qq{s.t.}  â€–uâ€–=1 \qq{and} â€–vâ€–=1 \qq{and} Ïƒâ‰¥0%
\end{align*}%
%
Equivalently this may be formalized as
%
\begin{align*}%
Ïƒ = \max_{uï¼Œv} u^âŠ¤ A v \qq{s.t.}  â€–uâ€–=1 \qq{and} â€–vâ€–=1%%
\end{align*}%
%
Which is a non-convex quadratically constrained quadratic program (QCQP)
%
\begin{align*}%
Ïƒ = \max_{u,v}
\frac{1}{2}\bmat{u \\ v}^âŠ¤
â‹…\bmat{ğŸ_{mÃ—m} & A \\  A^âŠ¤ & ğŸ_{nÃ—n}}
â‹…\bmat{u \\ v}
\;\text{s.t.}\;
\begin{aligned}
\bmat{u \\ v}^âŠ¤
â‹…\,\bmat{ğ•€_m & ğŸ_{mÃ—n} \\  ğŸ_{nÃ—m} & ğŸ_{nÃ—n}}
â‹…\bmat{u \\ v}
&= 1
\\
\bmat{u \\ v}^âŠ¤
â‹…\bmat{ğŸ_{mÃ—m} & ğŸ_{mÃ—n} \\  ğŸ_{nÃ—m} & ğ•€_n}
â‹…\bmat{u \\ v}
&= 1
\end{aligned}
\end{align*}%
%



\paragraph{The Jacobian and Lagrangian}
%
The derivative of the objective function is

\begin{align*}%
ğ‰_f(Aï¼Œ\bsmat{Ïƒ \\ u \\ v}) = \bmat{ A-Ïƒuv^âŠ¤ &  \mat{ Ïƒ - u^âŠ¤Av \\ ÏƒÂ²u - ÏƒAv \\ ÏƒÂ²v - ÏƒA^âŠ¤u}}
âŸ¹ ğ‡_f(\bsmat{Ïƒ \\ u \\ v}) = \bmat{ 1 & 2Ïƒu - Av & 2Ïƒv - A^âŠ¤u \\ -Av & ÏƒÂ²ğ•€â‚˜  & -ÏƒA \\ -A^âŠ¤u & -ÏƒA^âŠ¤ & ÏƒÂ²ğ•€â‚™ }
\end{align*}%
%
Consider the function
%
\begin{align*}%
f(Aï¼Œ\bsmat{Ïƒ \\ u \\ v}) = \pmat{ Ïƒ - u^âŠ¤Av \\ ÏƒÂ²u - ÏƒAv \\ ÏƒÂ²v - ÏƒA^âŠ¤u} â‰¡ ğŸ%
âŸ¹ ğ‰_f(Aï¼Œ\bsmat{Ïƒ \\ u \\ v}) = \barr{c|ccc}{-Î¾vu^âŠ¤ & 1 & 2Ïƒu - Av & 2Ïƒv - A^âŠ¤u \\ -ÏƒvÏ•^âŠ¤ & -Av & ÏƒÂ²ğ•€â‚˜  & -ÏƒA \\ -ÏƒuÏˆ^âŠ¤ & -A^âŠ¤u & -ÏƒA^âŠ¤ & ÏƒÂ²ğ•€â‚™ }
\end{align*}%
%
Thus, gradient descent schema is

%
\begin{align*}%
Ïƒ' &= Ïƒ - Î·_Ïƒ(Ïƒ - u^âŠ¤Av) \\
u' &= u - Î·_u(ÏƒÂ²u - ÏƒAv) \\
v' &= v - Î·_v(ÏƒÂ²v - ÏƒA^âŠ¤u)
\end{align*}%
%
And the newton step with diagonal approximation of the hessian:
%
\begin{align*}%
\begin{aligned}
Ïƒ' &= Ïƒ - 1(Ïƒ - u^âŠ¤Av)              &&= u^âŠ¤Av \\
u' &= u - \tfrac{1}{ÏƒÂ²}(ÏƒÂ²u - ÏƒAv)  &&= \tfrac{1}{Ïƒ}Av \\
v' &= v - \tfrac{1}{ÏƒÂ²}(ÏƒÂ²v - ÏƒA^âŠ¤u)&&= \tfrac{1}{Ïƒ}A^âŠ¤u
\end{aligned}
\end{align*}%
%

\section{Analysis of the backward}

At the equilibrium point, we have:
%
\begin{align*}%
Ïƒ &= u^âŠ¤ A v &  Av &= Ïƒu & A^âŠ¤u &= Ïƒv & u^âŠ¤ u &= 1  & v^âŠ¤v &=1%
\end{align*}%
%
Note that this states that $Ïƒ$ is an eigenvalue:
%
\begin{align*}%
\bmat{0 & A \\ A^âŠ¤ & 0}\bmat{u\\v} = Ïƒ\bmat{u\\v}%
\end{align*}%
%
In particular, Rayleigh iteration could be useful.
%
from this we can derive
%
\begin{alignat*}{3}%
âˆ†Ïƒ &= {âˆ†u}^âŠ¤ A v + u^âŠ¤{âˆ†A}v + u^âŠ¤A{âˆ†v}
&&= {âˆ†u}^âŠ¤ u + u^âŠ¤ {âˆ†A}v + v^âŠ¤ {âˆ†v}
&&= u^âŠ¤{âˆ†A}v%
\end{alignat*}%
%
Where in the last step we used $âˆ†uâŸ‚u$ and $âˆ†vâŸ‚v$, which follows from the side condition. Further we have:
%
\begin{align*}%
\begin{aligned}
   {âˆ†Ïƒ}u + Ïƒ{âˆ†u} &= {âˆ†A}v  + A{âˆ†v}
\\  {âˆ†Ïƒ}v + Ïƒ{âˆ†v} &= {âˆ†A}^âŠ¤u + A^âŠ¤{âˆ†u}%
\end{aligned}
\iff
\underbrace{\bmat[c]{Ïƒğ•€â‚˜ & -A \\ -A^âŠ¤ & Ïƒğ•€â‚™}}_{â‰•K}â‹…\bmat{âˆ†u\\âˆ†v} = \bmat{{âˆ†A}v - {âˆ†Ïƒ}u \\ {âˆ†A}^âŠ¤u - {âˆ†Ïƒ}v}
\end{align*}%
%
which allows us to express $âˆ†u$ and $âˆ†v$ in terms of $âˆ†A$.
The constraints yield
%
\begin{align*}%
  u^âŠ¤ âˆ†u + {âˆ†u}^âŠ¤u &= 0  âŸº uâŸ‚âˆ†u  %
\\v^âŠ¤ âˆ†v + {âˆ†v}^âŠ¤v &= 0  âŸº vâŸ‚âˆ†v  %
\end{align*}%
%
We can augment the original system with these:
%
\begin{align*}%
\underbrace{\bmat[c]{
	Ïƒğ•€â‚˜ & -A
\\ -A^âŠ¤ & Ïƒğ•€â‚™
\\ u^âŠ¤ & ğŸâ‚™^âŠ¤
\\ ğŸâ‚˜^âŠ¤ & v^âŠ¤
}}_{â‰•\Tilde{K}}â‹…\bmat{âˆ†u\\âˆ†v}
= \underbrace{\bmat{{âˆ†A}v - {âˆ†Ïƒ}u \\ {âˆ†A}^âŠ¤u - {âˆ†Ïƒ}v \\ 0 \\ 0}}_{â‰•\Tilde{c}}%
\end{align*}%
%


\section{VJP with modified K matrix}
\begin{align*}%
\Bigl{âŸ¨}\bmat{Ï•\\Ïˆ}\Bigm{\vert}\bmat{âˆ†u\\âˆ†v}\Bigr{âŸ©}
&= \Bigl{âŸ¨}\bmat{Ï•\\Ïˆ}\Bigm{\vert} \Tilde{K}^{-1}\Tilde{c}\Bigr{âŸ©}
\\&= \Bigl{âŸ¨}\Tilde{K}^{-âŠ¤}\bmat{Ï•\\Ïˆ}\Bigm{\vert}\Tilde{c}\Bigr{âŸ©}%
\\&= \Bigl{âŸ¨}
\bmat[c]{
	Ïƒğ•€â‚˜ & -A & u & ğŸâ‚˜
\\ -A^âŠ¤ & Ïƒğ•€â‚™ & ğŸâ‚™ & v
}^{-1}\bmat{Ï•\\Ïˆ}
\Bigm{\vert}\bsmat{{âˆ†A}v - {âˆ†Ïƒ}u \\ {âˆ†A}^âŠ¤u - {âˆ†Ïƒ}v \\ 0 \\ 0}\Bigr{âŸ©}%
\\&= \Bigl{âŸ¨}\bmat[c]{
	Ïƒğ•€â‚˜ & -A & u & ğŸâ‚˜
\\ -A^âŠ¤ & Ïƒğ•€â‚™ & ğŸâ‚™ & v
}\bsmat{p\\q\\Î»\\Î¼} = \bmat{Ï•\\Ïˆ} \Bigm{\vert}\bsmat{{âˆ†A}v - {âˆ†Ïƒ}u \\ {âˆ†A}^âŠ¤u - {âˆ†Ïƒ}v \\ 0 \\ 0}\Bigr{âŸ©}%
\end{align*}%
\subsection{Augmented block inversion}

\textbf{NOTE: Tested this and the issue is that it vastly increases the condition number!}

We use the technique \href{https://en.wikipedia.org/wiki/Block_matrix_pseudoinverse#Column-wise_partitioning_in_over-determined_least_squares}{\underline{Column-wise partitioning in over-determined least squares}}.
%
\begin{align*}%
\bmat{A & B} \bmat{xâ‚ \\ xâ‚‚} = d âŸ¸ x = \bmat{A & B}âºd = \bmat{(P_B^âŸ‚ A)âº \\ (P_A^âŸ‚ B)âº}d%
\end{align*}%
%
In particular, in our case this means that the relevant part of the solution is
%
\begin{align*}%
\bmat{p\\q} = (P_B^âŸ‚K)âº\bmat{Ï•\\Ïˆ}%
\end{align*}%
%
Here
%
\begin{align*}%
P_B^âŸ‚  &= ğ•€ - BBâº
\\&= ğ•€ - B(Báµ€B)^{-1}B^âŠ¤%
\\&= ğ•€ - \bmat{u & 0 \\ 0 & v}(\bmat{u^âŠ¤ & 0^âŠ¤ \\ 0^âŠ¤ & v^âŠ¤} \bmat{u & 0 \\ 0 & v})^{-1} \bmat{u^âŠ¤ & 0^âŠ¤ \\ 0^âŠ¤ & v^âŠ¤}
\\&= ğ•€ - \bmat{u & 0 \\ 0 & v}(\bmat{u^âŠ¤ & 0^âŠ¤ \\ 0^âŠ¤ & v^âŠ¤} \bmat{u & 0 \\ 0 & v})^{-1} \bmat{u^âŠ¤ & 0^âŠ¤ \\ 0^âŠ¤ & v^âŠ¤}
\\&= ğ•€ - \bmat{u & 0 \\ 0 & v}\bmat{1/â€–uâ€–Â² & 0 \\ 0& 1/â€–vâ€–Â²}\bmat{u^âŠ¤ & 0^âŠ¤ \\ 0^âŠ¤ & v^âŠ¤}
\\&= ğ•€ - \bmat{u & 0 \\ 0 & v}\bmat{1/â€–uâ€–Â² & 0 \\ 0& 1/â€–vâ€–Â²}\bmat{u^âŠ¤ & 0^âŠ¤ \\ 0^âŠ¤ & v^âŠ¤}
\\&= \bmat{ğ•€â‚˜ - uu^âŠ¤ & ğŸ_{mÃ—n} \\ ğŸ_{nÃ—m} & ğ•€â‚™ - vv^âŠ¤}
\end{align*}%
%
So
%
\begin{align*}%
P_B^âŸ‚ K
&= \bmat{ğ•€â‚˜ - uu^âŠ¤ & ğŸ_{mÃ—n} \\ ğŸ_{nÃ—m} & ğ•€â‚™ - vv^âŠ¤} \bmat[c]{Ïƒğ•€â‚˜ & -A \\ -A^âŠ¤ & Ïƒğ•€â‚™}%
\\&= \bmat{Ïƒ(ğ•€â‚˜ - uu^âŠ¤) & -A + Ïƒuv^âŠ¤ \\ -A^âŠ¤ + Ïƒvu^âŠ¤ & Ïƒ(ğ•€â‚™ - vv^âŠ¤)}
\\&= K - Ïƒ\bmat{uu^âŠ¤ & -uv^âŠ¤ \\ -vu^âŠ¤ & \sigma vv^âŠ¤ }
\\&= K - Ïƒzz^âŠ¤  \qquad z = \bsmat{u \\ -v}
\end{align*}%
%
In particular, we see that effectively this is a low rank update of the original matrix!
%
We can use the inversion formula for 2Ã—2 block matrices, combined with the inverse of rank-1 update formulas:
%
\begin{align*}%
Î² â‰” 1 - Ïƒ z^âŠ¤ Kâº z = ? \text{[.. proof ...]} = 0
\end{align*}%
%
Also $zâˆˆ\Im(K)$, so, in particular, the case (vi) of the paper \href{https://epubs.siam.org/doi/abs/10.1137/0124033}{Generalized Inversion of Modified Matrices} holds:

%
\begin{align*}%
(A + cdáµ€)âº &= Aâº - kkâºAâº - Aâºhâºh + (kâºAâºhâº)kh  \qq{with} k = Aâºcï¼Œ h = d^âŠ¤Aâº
\end{align*}
%
Assuming $A$ is symmetric, $c=d=x$ and noting that $vâº = \frac{1}{â€–vâ€–Â²}v^âŠ¤$ we can simplify since
%
\begin{align*}%
k = h^âŠ¤ &= Aâºx    &  kâº &= \frac{1}{â€–Aâºxâ€–Â²}x^âŠ¤Aâº
\\ h = k^âŠ¤ &= x^âŠ¤Aâº  &  hâº &= \frac{1}{â€–Aâºxâ€–Â²}Aâºx
\end{align*}%
%
\begin{align*}%
(A + xxáµ€)âº &= Aâº - \frac{Aâºxx^âŠ¤AâºAâº}{â€–Aâºxâ€–Â²} - \frac{AâºAâºxx^âŠ¤Aâº}{â€–Aâºxâ€–Â²} + \frac{(x^âŠ¤(Aâº)Â³x)Aâºxx^âŠ¤Aâº}{â€–Aâºxâ€–â´}
\end{align*}
%
With an additional scalar $Î³$:
%
\begin{align*}%
(A + Î³xxáµ€)âº &= Aâº - \frac{Aâºxx^âŠ¤AâºAâº}{â€–Aâºxâ€–Â²} - \frac{AâºAâºxx^âŠ¤Aâº}{â€–Aâºxâ€–Â²} + \frac{(x^âŠ¤(Aâº)Â³x)Aâºxx^âŠ¤Aâº}{â€–Aâºxâ€–â´}
\end{align*}
%
Now, in our case $x$ happens to be an eigenvector: $Kz = 2Ïƒz$, $Kâºz = \tfrac{1}{2Ïƒ}z$, $â€–zâ€–Â² = 2$, hence $â€–Kâºzâ€–Â² = \tfrac{1}{2ÏƒÂ²}$ and $Kâºzz^âŠ¤Kâº = \tfrac{1}{4ÏƒÂ²}zz^âŠ¤$.
%
\begin{align*}%
(K - Ïƒzzáµ€)âº &= Kâº - \tfrac{1/(2Ïƒ)Â³}{1/2ÏƒÂ²}zz^âŠ¤  - \tfrac{1/(2Ïƒ)Â³}{1/2ÏƒÂ²}zz^âŠ¤ +
\tfrac{2/(2Ïƒ)Â³}{1/2ÏƒÂ²}\tfrac{1/(2Ïƒ)Â²}{1/2ÏƒÂ²}zz^âŠ¤%
\\&= Kâº - \tfrac{1}{4Ïƒ}zz^âŠ¤ - \tfrac{1}{4Ïƒ}zz^âŠ¤ + \tfrac{1}{4Ïƒ}zz^âŠ¤
\\&= Kâº - \tfrac{1}{4Ïƒ}zz^âŠ¤
\end{align*}%
%


%
\begin{align*}%
âŸ¹(A + xxáµ€)âº &=
%\\&= Aâº - Aâºxx^âŠ¤(Aâº)^âŠ¤Aâº - Aâº(Aâº)^âŠ¤ xx^âŠ¤Aâº + (x^âŠ¤(Aâº)^âŠ¤Aâº(Aâº)^âŠ¤ x)Aâºxx^âŠ¤Aâº
%\\(K - Ïƒzz^âŠ¤)âº &= Kâº + ÏƒKâº zzáµ€KKâº + ÏƒKâºKzzáµ€Kâº + ÏƒÂ²(záµ€KKâºKz)Kâºzzáµ€Kâº
%\\&=Kâº + ÏƒKâº(zzáµ€K + Kzzáµ€ + Ïƒzzáµ€Kzzáµ€)Kâº
%\\&=Kâº + 4Ïƒ(Ïƒ+ÏƒÂ²)Kâºzzáµ€Kâº   \qq{using} Kz = 2Ïƒ \qq{and} â€–zâ€–Â² = 2
\end{align*}%
%
Note that $vâº = \frac{1}{â€–vâ€–Â²}v^âŠ¤$.






%
\section{The VJP}
The last equation allows us to compute the VJP at ease:
%
\begin{align*}%
\Bigl{âŸ¨}\bmat{Ï•\\Ïˆ}\Bigm{\vert}\bmat{âˆ†u\\âˆ†v}\Bigr{âŸ©}
&= \Bigl{âŸ¨}\bmat{Ï•\\Ïˆ}\Bigm{\vert} K^{-1}\bmat{{âˆ†A}v - {âˆ†Ïƒ}u \\ {âˆ†A}^âŠ¤u - {âˆ†Ïƒ}v}\Bigr{âŸ©}
\\&= \Bigl{âŸ¨}K^{-âŠ¤}\bmat{Ï•\\Ïˆ}\Bigm{\vert}\bmat{{âˆ†A}v - {âˆ†Ïƒ}u \\ {âˆ†A}^âŠ¤u - {âˆ†Ïƒ}v}\Bigr{âŸ©}%
\\&= \Bigl{âŸ¨}\bmat{\tilde{Ï•}\\\tilde{Ïˆ}}\Bigm{\vert}\bmat{{âˆ†A}v - {âˆ†Ïƒ}u \\ {âˆ†A}^âŠ¤u - {âˆ†Ïƒ}v}\Bigr{âŸ©}%
\end{align*}%
%
Now, we compute the terms individually:
%
\begin{align*}%
âŸ¨\tilde{Ï•}âˆ£ {âˆ†A}v - {âˆ†Ïƒ}uâŸ©
&= âŸ¨\tilde{Ï•}v^âŠ¤âˆ£ {âˆ†A}âŸ© - âŸ¨u^âŠ¤\tilde{Ï•}âˆ£{âˆ†Ïƒ}âŸ©%
\\&= âŸ¨\tilde{Ï•}v^âŠ¤âˆ£ {âˆ†A}âŸ© - âŸ¨u^âŠ¤\tilde{Ï•}âˆ£u^âŠ¤{âˆ†A}vâŸ©%
\\&= âŸ¨(ğ•€â‚˜ - uu^âŠ¤)\tilde{Ï•}v^âŠ¤âˆ£{âˆ†A}âŸ©
\end{align*}%
%
And for the second term we get
%
\begin{align*}%
âŸ¨\tilde{Ïˆ} âˆ£ {âˆ†A}^âŠ¤ u - {âˆ†Ïƒ}vâŸ©
&= âŸ¨\tilde{Ïˆ}u^âŠ¤âˆ£ {âˆ†A}^âŠ¤âŸ© - âŸ¨v^âŠ¤\tilde{Ïˆ}âˆ£{âˆ†Ïƒ}âŸ©%
\\&= âŸ¨u\tilde{Ïˆ}^âŠ¤âˆ£ {âˆ†A}âŸ© - âŸ¨\tilde{Ïˆ}^âŠ¤vâˆ£u^âŠ¤{âˆ†A}vâŸ©%
\\&= âŸ¨u\tilde{Ïˆ}(ğ•€â‚™ - vv^âŠ¤)âˆ£{âˆ†A}âŸ©
\end{align*}%
%
Using the formula for inverting a 2Ã—2 block-matrix, we can give an explicit solution to $K^{-âŠ¤}\bsmat{Ï•\\Ïˆ}$:
%
\begin{align*}%
K^{-1} = \bmat[c]{Ïƒğ•€â‚˜ & -A \\ -A^âŠ¤ & Ïƒğ•€â‚™}^{-1}
&= \bmat{
	(Ïƒğ•€â‚˜ - \frac{1}{Ïƒ}AA^âŠ¤)^{-1} & ğŸ_{mÃ—n} \\ ğŸ_{nÃ—m} & (Ïƒğ•€â‚™ - \frac{1}{Ïƒ}A^âŠ¤A)^{-1}}
	â‹…\bmat{ğ•€â‚˜ & \frac{1}{Ïƒ}A \\ \frac{1}{Ïƒ}A^âŠ¤ & ğ•€â‚™}%
\\&= \bmat{
	Ïƒ(ÏƒÂ²ğ•€â‚˜ - AA^âŠ¤)^{-1} & (ÏƒÂ²ğ•€â‚˜ - AA^âŠ¤)^{-1}A
\\  (ÏƒÂ²ğ•€â‚™ - A^âŠ¤A)^{-1}A^âŠ¤ & Ïƒ(ÏƒÂ²ğ•€â‚™ - A^âŠ¤A)^{-1}
}
\end{align*}%
%
And we see it's basically projection operators with respect to the image/kernel of $\tilde{A} = \frac{1}{Ïƒ}A$.
%
In summary, we obtain the following formula for the VJP:

%
\begin{align*}%
K \bmat{p \\Â q} = \bmat{Ï•\\Ïˆ}
&âŸº
\bmat{p \\Â q} =
\bmat{
	Ïƒ(ÏƒÂ²ğ•€â‚˜ - AA^âŠ¤)^{-1} & (ÏƒÂ²ğ•€â‚˜ - AA^âŠ¤)^{-1}A
\\  (ÏƒÂ²ğ•€â‚™ - A^âŠ¤A)^{-1}A^âŠ¤ & Ïƒ(ÏƒÂ²ğ•€â‚™ - A^âŠ¤A)^{-1}
}
\bmat{Ï•\\Ïˆ}%
%\\&\bmat{\tilde{Ï•}\\Â \tilde{Ïˆ}} = \frac{1}{Ïƒ}\bmat{
%		(ğ•€â‚˜ - \tilde{A}\tilde{A}^âŠ¤)^{-1} & (ğ•€â‚˜ - \tilde{A}\tilde{A}^âŠ¤)^{-1}\tilde{A}
%	\\  (ğ•€â‚™ - \tilde{A}^âŠ¤\tilde{A})^{-1}\tilde{A}^âŠ¤ & (ğ•€â‚™ - \tilde{A}^âŠ¤\tilde{A})^{-1}
%}^âŠ¤ \bmat{Ï•\\Ïˆ}%
%\\&âŸº
%\bmat{\tilde{Ï•}\\Â \tilde{Ïˆ}} = \frac{1}{Ïƒ}\bmat{
%		(ğ•€â‚˜ - \tilde{A}\tilde{A}^âŠ¤)^{-1} & \tilde{A}(ğ•€â‚™ - \tilde{A}^âŠ¤\tilde{A})^{-1}
%	\\  \tilde{A}^âŠ¤(ğ•€â‚˜ - \tilde{A}\tilde{A}^âŠ¤)^{-1}  & (ğ•€â‚™ - \tilde{A}^âŠ¤\tilde{A})^{-1}
%} \bmat{Ï•\\Ïˆ}%
\end{align*}%
%
In particular, we can find the solution by solving 4 smaller linear systems:
%
\begin{align*}%
	Ïƒ(ÏƒÂ²ğ•€â‚˜ - AA^âŠ¤)^{-1} Ï•  &= x & (ÏƒÂ²ğ•€â‚˜ - AA^âŠ¤)^{-1}AÏˆ &= y%
\\  (ÏƒÂ²ğ•€â‚™ - A^âŠ¤A)^{-1}A^âŠ¤Ï• &= w & Ïƒ(ÏƒÂ²ğ•€â‚™ - A^âŠ¤A)^{-1}Ïˆ &= z
\end{align*}%
%
Or, equivalently:
%
\begin{align*}%
	(ÏƒÂ²ğ•€â‚˜ - AA^âŠ¤)x &=  ÏƒÏ•  & (ÏƒÂ²ğ•€â‚˜ - AA^âŠ¤)y  &= AÏˆ%
\\  (ÏƒÂ²ğ•€â‚™ - A^âŠ¤A)w &= A^âŠ¤Ï• &  (ÏƒÂ²ğ•€â‚™ - A^âŠ¤A)z &= ÏƒÏˆ
\end{align*}%
%


%
Note how this shows that the off-diagonal entries are solutions to regularized least squares problems!
%
However, we really do not want to compute the matrices $AA^âŠ¤$ and $A^âŠ¤A$ since this leads to numerical stability (squared condition number!)
%
To circumvent this issue, we do a reformulation
%

%
\begin{alignat*}{3}%
(ÏƒÂ²ğ•€â‚˜ - AA^âŠ¤)y &= AÏˆ &âŸº y &=\argmin_y â€–-A^âŠ¤y - Ïˆâ€–â‚‚Â² - ÏƒÂ²â€–yâ€–â‚‚Â²
\\  &&âŸº y &= \argmin_y \Bigl{â€–} \bmat{A^âŠ¤ \\ ÏƒÂ²ğ•€â‚˜} y - \bmat{-Ïˆ \\ ğŸâ‚˜}\Bigr{â€–}â‚‚Â²%
\\  (ÏƒÂ²ğ•€â‚™ - A^âŠ¤A)w   &= A^âŠ¤Ï•  &âŸº  w &= \argmin_w â€–Aw + Ï•â€–â‚‚Â²   -ÏƒÂ²â€–wâ€–â‚‚Â²
\\  &&âŸº w &= \argmin_w \Bigl{â€–} \bmat{A \\ ÏƒÂ²ğ•€â‚™} w - \bmat{-Ï• \\ ğŸâ‚™}\Bigr{â€–}â‚‚Â²%
\end{alignat*}%
%

%
\begin{remark}[When is Ridge Regression unconstrained?]%
\label{rem: label}%
Consider the problem
%
\begin{align*}%
Î²^* = \argmin_Î² â€–XÎ²-yâ€–Â² + Î»â€–Î²â€–Â²%
\end{align*}%
%
Question: When is there an unconstrained solution?
%
The solution satisfies the normal equation
%
\begin{align*}%
(Xáµ€X  + Î»ğ•€)Î² = X^âŠ¤y%
\end{align*}%
%
If $Î»>0$, then $(Xáµ€X  + Î»ğ•€)$ is positive definite and hence invertible. If $Î»<0$, then $(Xáµ€X  + Î»ğ•€)$ is singular whenever $Î»$ is an eigenvalue of $Xáµ€X$.
%
In particular, the 4 systems listed before are all ill-conditioned! The central issue is that the constraint is missing! $â€–uâ€–Â² = 1$ and $â€–vâ€–Â²=1$ translate to $uâŸ‚âˆ†u$ and $vâŸ‚âˆ†v$. Since $u$, $v$ are singular vectors, this means we avoid the singular subspace when solving these equations!

What we should do is use \textbf{Riemannian Optimization}.
%
\end{remark}%


\subsection{What happens if Ï• or Ïˆ are zero?}
%
In this case we want to fast track the calculation, meaning skip half of the necessary inversions.
Looking at the equations we find that if $Ï•=0$ then $x=0$ and $w=0$, and if $Ïˆ=0$ then $y=0$ and $z=0$.
This suggests that backward substitution is better than forward substitution, since it allows decoupling of the two gradient contributions.



\subsection{Via Forward Substitution}
Now, the diagonal entries we have a problem: the RHS lacks the $A$ matrix. Thus, we solve in two steps instead:
%
%
\begin{alignat*}{3}%
	AÎ¼ &= ÏƒÏ• &âŸ¹ x &= \argmin_x \Bigl{â€–} \bmat{A^âŠ¤ \\ ÏƒÂ²ğ•€â‚˜} x - \bmat{-Î¼\\ğŸâ‚˜}\Bigr{â€–}â‚‚Â² %%
\\  A^âŠ¤Î½ &= ÏƒÏˆ &âŸ¹ z &= \argmin_z \Bigl{â€–} \bmat{A \\ ÏƒÂ²ğ•€â‚™} z - \bmat{-Î½\\ğŸâ‚™}\Bigr{â€–}â‚‚Â² %%
\end{alignat*}%
%
We can optimize further by performing a simultaneous solve:
%
\begin{align*}%
	\bmat{xï¼Œy} &= \argmin_{x,y} \Bigl{â€–} \bmat{A^âŠ¤ \\ ÏƒÂ²ğ•€â‚˜} \bmat{xï¼Œy} - \bmat{-Î¼ & -Ïˆ \\ ğŸâ‚˜ & ğŸâ‚˜}\Bigr{â€–}â‚‚Â²%
& Î¼ &= \argmin_Î¼ â€–AÎ¼ - ÏƒÏ•â€–â‚‚Â²
\\  \bmat{wï¼Œz} &= \argmin_{w,z} \Bigl{â€–} \bmat{A   \\ ÏƒÂ²ğ•€â‚™} \bmat{wï¼Œz} - \bmat{-Ï• &-Î½ \\ ğŸâ‚™ & ğŸâ‚™}\Bigr{â€–}â‚‚Â²%
& Î½ &= \argmin_Î½ â€–A^âŠ¤Î½ - ÏƒÏˆâ€–â‚‚Â²
\end{align*}%
%
\subsection{Via Backward Substitution}

We need to introduce an additional modification:

If $AÎ¼ = ÏƒÏ•$ not solveable, we instead can multiply the equation by $A^âŠ¤$ to obtain:
%
\begin{align*}%
	(ÏƒÂ²ğ•€â‚˜ - AA^âŠ¤)x &= ÏƒÏ•  &&âŸ¹&  (ÏƒÂ²ğ•€â‚™ - A^âŠ¤A)Î¼ &= ÏƒA^âŠ¤Ï•  & A^âŠ¤x &= Î¼%
\\  (ÏƒÂ²ğ•€â‚™ - A^âŠ¤A)z &= ÏƒÏˆ  &&âŸ¹&  (ÏƒÂ²ğ•€â‚™ - A^âŠ¤A)AÎ½ &= ÏƒAÏˆ    & Az &= Î½%
\end{align*}%
%
So:
%
\begin{align*}%
	Î¼ &= \argmin_Î¼ \Bigl{â€–} \bmat{A \\ ÏƒÂ²ğ•€â‚™} Î¼ - \bmat{-ÏƒÏ•\\ğŸâ‚™}\Bigr{â€–}â‚‚Â²  & A^âŠ¤x &= Î¼%
\\  Î½ &= \argmin_Î¼ \Bigl{â€–} \bmat{A^âŠ¤ \\ ÏƒÂ²ğ•€â‚˜} Î½ - \bmat{-ÏƒÏˆ\\ğŸâ‚˜}\Bigr{â€–}â‚‚Â²  & Az &= Î½%
\end{align*}%
%
So
%
\begin{align*}%
	\bmat{Î¼ & w} &= \bmat{A \\ ÏƒÂ²ğ•€â‚™} \bmat{-ÏƒÏ• & -Ï•\\ğŸâ‚™ & ğŸâ‚™} %
\\  \bmat{y & Î½}   &= \bmat{A^âŠ¤ \\ ÏƒÂ²ğ•€â‚˜} \bmat{-Ïˆ & -ÏƒÏˆ\\ğŸâ‚˜ & ğŸâ‚˜} %
\end{align*}%
%





%
In principle, one could try to rephrase these as smaller problems, but for now, it's better to just stick to the bigger system.
%
We can use the \textbf{push-through identity} to convert these into 4 linear systems:
%
\begin{align*}%
	Px &= Ï•             & Py &= \tilde{A}Ïˆ%
\\  Qz &= \tilde{A}^âŠ¤Ï•  & Qw &= Ïˆ
\end{align*}%
%
Then $\tilde{Ï•} = x+y$ and $\tilde{Ïˆ} = z+w$, and the VJP are given by the previous equations:
%
\begin{align*}%
	Î¾^âŠ¤\frac{âˆ‚Ïƒ}{âˆ‚A} &= Î¾uv^âŠ¤
\\  Ï•^âŠ¤\frac{âˆ‚u}{âˆ‚A} &= (ğ•€â‚˜ - uu^âŠ¤)\tilde{Ï•}v^âŠ¤ = (\tilde{Ï•} - (u^âŠ¤\tilde{Ï•})u)v^âŠ¤%
\\  Ïˆ^âŠ¤\frac{âˆ‚v}{âˆ‚A} &= u\tilde{Ïˆ}^âŠ¤(ğ•€â‚™ - vv^âŠ¤) = u(\tilde{Ïˆ} - (v^âŠ¤\tilde{Ïˆ})v)^âŠ¤%
\end{align*}%
%
\section{Spectral Normalization}
The VJP of spectral normalization can be computed as follows: let $g(A) = â€–Aâ€–â‚‚$ and $V$ be the vector in the VJP. then
%
\begin{align*}%
âˆ‡_AâŸ¨V âˆ£ \frac{A}{â€–Aâ€–â‚‚}âŸ©
	&= âŸ¨V | \frac{A+âˆ†A}{g(A+âˆ†A)} - \frac{A}{g(A)}âŸ©
\\  &= âŸ¨V | \frac{A+âˆ†A}{g(A)+âˆ‡g(A)âˆ†A} - \frac{A}{g(A)}âŸ©
\\  &= âŸ¨V | \frac{(A+âˆ†A)(g(A)-âˆ‡g(A)âˆ†A)}{(g(A)+âˆ‡g(A)âˆ†A)(g(A) - âˆ‡g(A)âˆ†A)} - \frac{A}{g(A)}âŸ©
\\  &= âŸ¨V | \frac{ âˆ†Ag(A)- Aâˆ‡g(A)âˆ†A}{g(A)Â²}âŸ©
\\  &= âŸ¨\tfrac{1}{g(A)}V - \tfrac{âŸ¨Vâˆ£AâŸ©}{g(A)}âˆ‡g(A) | âˆ†AâŸ©
\end{align*}%
%
%
\begin{align*}%
g(A)=1 âŸ¹ âˆ‡_AâŸ¨V âˆ£ \frac{A}{â€–Aâ€–â‚‚}âŸ© = âŸ¨V - âŸ¨Vâˆ£AâŸ©âˆ‡g(A)âˆ£âˆ†AâŸ©%
\end{align*}%
%

\section{Projected gradient}

When using spectral normalization we want to do the following:

%
\begin{alignat*}{3}%
   &\text{update:}\quad& A' &= A - âˆ‡_A ğ“›(\frac{A}{â€–Aâ€–â‚‚})%
\\ &\text{project:}\quad& A &= \frac{A'}{â€–A'â€–â‚‚}
\end{alignat*}%
%
Moreover, we want:
%
\begin{outline}%
%\renewcommand{\outlineii}{enumerate}
\1 During forward, compute $\frac{A}{â€–Aâ€–â‚‚}$ only once and then reuse this node.
\1 Compute $â€–Aâ€–â‚‚$ effectively between gradient updates.
	\2 Avoid built-in torch algos, as they make use of full SVD algos.
\1 After gradient update, perform projection step. (maybe unnecessary)
\end{outline}%
%
NOTE: gradients are different if we include normalization!

\end{document}
