\RequirePackage{iftex}
\RequirePDFTeX
\NeedsTeXFormat{LaTeX2e}
\documentclass[10pt]{article}
\usepackage{ismll-packages}
\usepackage{ismll-mathoperators}
\usepackage{ismll-style}
\usepackage{unicode-symbols}

\title{Derivative of the first order SVD}
\author{Randolf Scholz}
\begin{document}

\maketitle


Consider computing the first order SVD expansion. By the Eckartâ€“Youngâ€“Mirsky theorem, this is equivalent to solving


%
\begin{align*}%
\minimize_{Ïƒï¼Œuï¼Œv} Â½â€–A - Ïƒuv^âŠ¤â€–Â²_F \qq{s.t.}  â€–uâ€–=1 \qq{and} â€–vâ€–=1 \qq{and} Ïƒâ‰¥0%
\end{align*}%
%
Equivalently this may be formalized as
%
\begin{align*}%
Ïƒ = \max_{uï¼Œv} u^âŠ¤ A v \qq{s.t.}  â€–uâ€–=1 \qq{and} â€–vâ€–=1%%
\end{align*}%
%
Which is a non-convex quadratically constrained quadratic program (QCQP)
%
\begin{align*}%
Ïƒ = \max_{u,v}
\frac{1}{2}\bmat{u \\ v}^âŠ¤
â‹…\bmat{ğŸ_{mÃ—m} & A \\  A^âŠ¤ & ğŸ_{nÃ—n}}
â‹…\bmat{u \\ v}
\;\text{s.t.}\;
\begin{aligned}
\bmat{u \\ v}^âŠ¤
â‹…\,\bmat{ğ•€_m & ğŸ_{mÃ—n} \\  ğŸ_{nÃ—m} & ğŸ_{nÃ—n}}
â‹…\bmat{u \\ v}
&= 1
\\
\bmat{u \\ v}^âŠ¤
â‹…\bmat{ğŸ_{mÃ—m} & ğŸ_{mÃ—n} \\  ğŸ_{nÃ—m} & ğ•€_n}
â‹…\bmat{u \\ v}
&= 1
\end{aligned}
\end{align*}%
%



\paragraph{The Jacobian and Lagrangian}
%
The derivative of the objective function is

\begin{align*}%
ğ‰_f(Aï¼Œ\bsmat{Ïƒ \\ u \\ v}) = \bmat{ A-Ïƒuv^âŠ¤ &  \mat{ Ïƒ - u^âŠ¤Av \\ ÏƒÂ²u - ÏƒAv \\ ÏƒÂ²v - ÏƒA^âŠ¤u}}
âŸ¹ ğ‡_f(\bsmat{Ïƒ \\ u \\ v}) = \bmat{ 1 & 2Ïƒu - Av & 2Ïƒv - A^âŠ¤u \\ -Av & ÏƒÂ²ğ•€â‚˜  & -ÏƒA \\ -A^âŠ¤u & -ÏƒA^âŠ¤ & ÏƒÂ²ğ•€â‚™ }
\end{align*}%
%
Consider the function
%
\begin{align*}%
f(Aï¼Œ\bsmat{Ïƒ \\ u \\ v}) = \pmat{ Ïƒ - u^âŠ¤Av \\ ÏƒÂ²u - ÏƒAv \\ ÏƒÂ²v - ÏƒA^âŠ¤u} â‰¡ ğŸ%
âŸ¹ ğ‰_f(Aï¼Œ\bsmat{Ïƒ \\ u \\ v}) = \barr{c|ccc}{-Î¾vu^âŠ¤ & 1 & 2Ïƒu - Av & 2Ïƒv - A^âŠ¤u \\ -ÏƒvÏ•^âŠ¤ & -Av & ÏƒÂ²ğ•€â‚˜  & -ÏƒA \\ -ÏƒuÏˆ^âŠ¤ & -A^âŠ¤u & -ÏƒA^âŠ¤ & ÏƒÂ²ğ•€â‚™ }
\end{align*}%
%
Thus, gradient descent schema is

%
\begin{align*}%
Ïƒ' &= Ïƒ - Î·_Ïƒ(Ïƒ - u^âŠ¤Av) \\
u' &= u - Î·_u(ÏƒÂ²u - ÏƒAv) \\
v' &= v - Î·_v(ÏƒÂ²v - ÏƒA^âŠ¤u)
\end{align*}%
%
And the newton step with diagonal approximation of the hessian:
%
\begin{align*}%
\begin{aligned}
Ïƒ' &= Ïƒ - 1(Ïƒ - u^âŠ¤Av)              &&= u^âŠ¤Av \\
u' &= u - \tfrac{1}{ÏƒÂ²}(ÏƒÂ²u - ÏƒAv)  &&= \tfrac{1}{Ïƒ}Av \\
v' &= v - \tfrac{1}{ÏƒÂ²}(ÏƒÂ²v - ÏƒA^âŠ¤u)&&= \tfrac{1}{Ïƒ}A^âŠ¤u
\end{aligned}
\end{align*}%
%

\section{Analysis of the backward}

At the equilibrium point, we have:
%
\begin{align*}%
	Ïƒ &= u^âŠ¤ A v &  Av &= Ïƒu & A^âŠ¤u &= Ïƒv%
\end{align*}%
%
Note that this states that $Ïƒ$ is an eigenvalue:
%
\begin{align*}%
\bmat{0 & A \\ A^âŠ¤ & 0}\bmat{u\\v} = Ïƒ\bmat{u\\v}%
\end{align*}%
%
In particular, Rayleigh iteration could be useful.
%
from this we can derive
%
\begin{alignat*}{3}%
âˆ†Ïƒ &= {âˆ†u}^âŠ¤ A v + u^âŠ¤{âˆ†A}v + u^âŠ¤A{âˆ†v}
&&= {âˆ†u}^âŠ¤ u + u^âŠ¤ {âˆ†A}v + v^âŠ¤ {âˆ†v}
&&= u^âŠ¤{âˆ†A}v%
\end{alignat*}%
%
Where in the last step we used $âˆ†uâŸ‚u$ and $âˆ†vâŸ‚v$, which follows from the side condition. Further we have:
%
\begin{align*}%
\begin{aligned}
   {âˆ†Ïƒ}u + Ïƒ{âˆ†u} &= {âˆ†A}v  + A{âˆ†v}
\\  {âˆ†Ïƒ}v + Ïƒ{âˆ†v} &= {âˆ†A}^âŠ¤u + A^âŠ¤{âˆ†u}%
\end{aligned}
\iff
\underbrace{\bmat[c]{Ïƒğ•€â‚˜ & -A \\ -A^âŠ¤ & Ïƒğ•€â‚™}}_{â‰•K}â‹…\bmat{âˆ†u\\âˆ†v} = \bmat{{âˆ†A}v - {âˆ†Ïƒ}u \\ {âˆ†A}^âŠ¤u - {âˆ†Ïƒ}v}
\end{align*}%
%
which allows us to express $âˆ†u$ and $âˆ†v$ in terms of $âˆ†A$.
%
\section{The VJP}
The last equation allows us to compute the VJP at ease:
%
\begin{align*}%
\Bigl{âŸ¨}\bmat{Ï•\\Ïˆ}\Bigm{\vert}\bmat{âˆ†u\\âˆ†v}\Bigr{âŸ©}
&= \Bigl{âŸ¨}\bmat{Ï•\\Ïˆ}\Bigm{\vert} K^{-1}\bmat{{âˆ†A}v - {âˆ†Ïƒ}u \\ {âˆ†A}^âŠ¤u - {âˆ†Ïƒ}v}\Bigr{âŸ©}
\\&= \Bigl{âŸ¨}K^{-âŠ¤}\bmat{Ï•\\Ïˆ}\Bigm{\vert}\bmat{{âˆ†A}v - {âˆ†Ïƒ}u \\ {âˆ†A}^âŠ¤u - {âˆ†Ïƒ}v}\Bigr{âŸ©}%
\\&= \Bigl{âŸ¨}\bmat{\tilde{Ï•}\\\tilde{Ïˆ}}\Bigm{\vert}\bmat{{âˆ†A}v - {âˆ†Ïƒ}u \\ {âˆ†A}^âŠ¤u - {âˆ†Ïƒ}v}\Bigr{âŸ©}%
\end{align*}%
%
Now, we compute the terms individually:
%
\begin{align*}%
âŸ¨\tilde{Ï•}âˆ£ {âˆ†A}v - {âˆ†Ïƒ}uâŸ©
&= âŸ¨\tilde{Ï•}v^âŠ¤âˆ£ {âˆ†A}âŸ© - âŸ¨u^âŠ¤\tilde{Ï•}âˆ£{âˆ†Ïƒ}âŸ©%
\\&= âŸ¨\tilde{Ï•}v^âŠ¤âˆ£ {âˆ†A}âŸ© - âŸ¨u^âŠ¤\tilde{Ï•}âˆ£u^âŠ¤{âˆ†A}vâŸ©%
\\&= âŸ¨(ğ•€â‚˜ - uu^âŠ¤)\tilde{Ï•}v^âŠ¤âˆ£{âˆ†A}âŸ©
\end{align*}%
%
And for the second term we get
%
\begin{align*}%
âŸ¨\tilde{Ïˆ} âˆ£ {âˆ†A}^âŠ¤ u - {âˆ†Ïƒ}vâŸ©
&= âŸ¨\tilde{Ïˆ}u^âŠ¤âˆ£ {âˆ†A}^âŠ¤âŸ© - âŸ¨v^âŠ¤\tilde{Ïˆ}âˆ£{âˆ†Ïƒ}âŸ©%
\\&= âŸ¨u\tilde{Ïˆ}^âŠ¤âˆ£ {âˆ†A}âŸ© - âŸ¨\tilde{Ïˆ}^âŠ¤vâˆ£u^âŠ¤{âˆ†A}vâŸ©%
\\&= âŸ¨u\tilde{Ïˆ}(ğ•€â‚™ - vv^âŠ¤)âˆ£{âˆ†A}âŸ©
\end{align*}%
%
Using the formula for inverting a 2Ã—2 block-matrix, we can give an explicit solution to $K^{-âŠ¤}\bsmat{Ï•\\Ïˆ}$:
%
\begin{align*}%
K^{-âŠ¤} = \bmat[c]{Ïƒğ•€â‚˜ & -A \\ -A^âŠ¤ & Ïƒğ•€â‚™}^{-1}
&= \bmat{
	(Ïƒğ•€â‚˜ - \frac{1}{Ïƒ}AA^âŠ¤)^{-1} & ğŸ_{mÃ—n} \\ ğŸ_{nÃ—m} & (Ïƒğ•€â‚™ - \frac{1}{Ïƒ}A^âŠ¤A)^{-1}}
	â‹…\bmat{ğ•€â‚˜ & \frac{1}{Ïƒ}A \\ \frac{1}{Ïƒ}A^âŠ¤ & ğ•€â‚™}%
\\&= \bmat{
	\tfrac{1}{Ïƒ}(ğ•€â‚˜ - \frac{1}{ÏƒÂ²}AA^âŠ¤)^{-1} & \tfrac{1}{ÏƒÂ²}(ğ•€â‚˜ - \frac{1}{ÏƒÂ²}AA^âŠ¤)^{-1}A
\\  \frac{1}{ÏƒÂ²}(ğ•€â‚™ - \frac{1}{ÏƒÂ²}A^âŠ¤A)^{-1}A^âŠ¤ & \tfrac{1}{Ïƒ}(ğ•€â‚™ - \frac{1}{ÏƒÂ²}A^âŠ¤A)^{-1}
}
\\&= \frac{1}{Ïƒ}\bmat{
	(ğ•€â‚˜ - \tilde{A}\tilde{A}^âŠ¤)^{-1} & (ğ•€â‚˜ - \tilde{A}\tilde{A}^âŠ¤)^{-1}\tilde{A}
\\  (ğ•€â‚™ - \tilde{A}^âŠ¤\tilde{A})^{-1}\tilde{A}^âŠ¤ & (ğ•€â‚™ - \tilde{A}^âŠ¤\tilde{A})^{-1}
}
\\&= \frac{1}{Ïƒ}\bmat{
	(ğ•€â‚˜ - \tilde{A}\tilde{A}^âŠ¤)^{-1} & (ğ•€â‚˜ - \tilde{A}\tilde{A}^âŠ¤)^{-1}\tilde{A}
\\  (ğ•€â‚™ - \tilde{A}^âŠ¤\tilde{A})^{-1}\tilde{A}^âŠ¤ & (ğ•€â‚™ - \tilde{A}^âŠ¤\tilde{A})^{-1} }
\end{align*}%
%
And we see it's basically projection operators with respect to the image/kernel of $\tilde{A} = \frac{1}{Ïƒ}A$.
%
In summary, we obtain the following formula for the VJP:

%
\begin{align*}%
K \bmat{\tilde{Ï•}\\Â \tilde{Ïˆ}} = \bmat{Ï•\\Ïˆ}
&âŸº
\frac{1}{Ïƒ}\bmat{
	(ğ•€â‚˜ - \tilde{A}\tilde{A}^âŠ¤)^{-1} & (ğ•€â‚˜ - \tilde{A}\tilde{A}^âŠ¤)^{-1}\tilde{A}
\\  (ğ•€â‚™ - \tilde{A}^âŠ¤\tilde{A})^{-1}\tilde{A}^âŠ¤ & (ğ•€â‚™ - \tilde{A}^âŠ¤\tilde{A})^{-1}
} \bmat{Ï•\\Ïˆ}%
%\\&\bmat{\tilde{Ï•}\\Â \tilde{Ïˆ}} = \frac{1}{Ïƒ}\bmat{
%		(ğ•€â‚˜ - \tilde{A}\tilde{A}^âŠ¤)^{-1} & (ğ•€â‚˜ - \tilde{A}\tilde{A}^âŠ¤)^{-1}\tilde{A}
%	\\  (ğ•€â‚™ - \tilde{A}^âŠ¤\tilde{A})^{-1}\tilde{A}^âŠ¤ & (ğ•€â‚™ - \tilde{A}^âŠ¤\tilde{A})^{-1}
%}^âŠ¤ \bmat{Ï•\\Ïˆ}%
%\\&âŸº
%\bmat{\tilde{Ï•}\\Â \tilde{Ïˆ}} = \frac{1}{Ïƒ}\bmat{
%		(ğ•€â‚˜ - \tilde{A}\tilde{A}^âŠ¤)^{-1} & \tilde{A}(ğ•€â‚™ - \tilde{A}^âŠ¤\tilde{A})^{-1}
%	\\  \tilde{A}^âŠ¤(ğ•€â‚˜ - \tilde{A}\tilde{A}^âŠ¤)^{-1}  & (ğ•€â‚™ - \tilde{A}^âŠ¤\tilde{A})^{-1}
%} \bmat{Ï•\\Ïˆ}%
\end{align*}%
%
In particular, we can find the solution by solving 4 smaller linear systems:
%
\begin{align*}%
	(ğ•€â‚˜ - \tilde{A}\tilde{A}^âŠ¤)x &= \tfrac{1}{Ïƒ}Ï•             & (ğ•€â‚˜ - \tilde{A}\tilde{A}^âŠ¤)y &= \tfrac{1}{Ïƒ}\tilde{A}Ïˆ%
\\  (ğ•€â‚™ - \tilde{A}^âŠ¤\tilde{A})w &= \tfrac{1}{Ïƒ}\tilde{A}^âŠ¤Ï•  & (ğ•€â‚™ - \tilde{A}^âŠ¤\tilde{A})z &= \tfrac{1}{Ïƒ}Ïˆ
\end{align*}%
%
Or, equivalently
%
\begin{align*}%
	(ÏƒÂ²ğ•€â‚˜ - AA^âŠ¤)x &= ÏƒÏ•      & (ÏƒÂ²ğ•€â‚˜ - AA^âŠ¤)y &= AÏˆ%
\\  (ÏƒÂ²ğ•€â‚™ - A^âŠ¤A)w   &= A^âŠ¤Ï•  & (ÏƒÂ²ğ•€â‚™ - A^âŠ¤A)z &= ÏƒÏˆ
\end{align*}%
%
Note how this shows that the off-diagonal entries are obviously regularized least squares problems!
%
However, we really do not want to compute the matrices $AA^âŠ¤$ and $A^âŠ¤A$ since this leads to numerical stability (squared condition number!)
%
To circumvent this issue, we do a reformulation
%

%
\begin{alignat*}{3}%
(ÏƒÂ²ğ•€â‚˜ - AA^âŠ¤)y &= AÏˆ &âŸº y &=\argmin_y â€–-A^âŠ¤y - Ïˆâ€–â‚‚Â² - ÏƒÂ²â€–yâ€–â‚‚Â²
\\  &&âŸº y &= \argmin_y \Bigl{â€–} \bmat{A^âŠ¤ \\ ÏƒÂ²ğ•€â‚˜} y - \bmat{-Ïˆ \\ ğŸâ‚˜}\Bigr{â€–}â‚‚Â²%
\\  (ÏƒÂ²ğ•€â‚™ - A^âŠ¤A)w   &= A^âŠ¤Ï•  &âŸº  w &= \argmin_w â€–-Aw - Ï•â€–â‚‚Â² - ÏƒÂ²â€–wâ€–â‚‚Â²
\\  &&âŸº w &= \argmin_w \Bigl{â€–} \bmat{A \\ ÏƒÂ²ğ•€â‚™} w - \bmat{-Ï• \\ ğŸâ‚™}\Bigr{â€–}â‚‚Â²%
\end{alignat*}%
%
\subsection{What happens if Ï• or Ïˆ are zero?}
%
In this case we want to fast track the calculation, meaning skip half of the necessary inversions.
Looking at the equations we find that if $Ï•=0$ then $x=0$ and $w=0$, and if $Ïˆ=0$ then $y=0$ and $z=0$.
This suggests that backward substitution is better than forward substitution, since it allows decoupling of the two gradient contributions.



\subsection{Via Forward Substitution}
Now, the diagonal entries we have a problem: the RHS lacks the $A$ matrix. Thus, we solve in two steps instead:
%
%
\begin{alignat*}{3}%
	AÎ¼ &= ÏƒÏ• &âŸ¹ x &= \argmin_x \Bigl{â€–} \bmat{A^âŠ¤ \\ ÏƒÂ²ğ•€â‚˜} x - \bmat{-Î¼\\ğŸâ‚˜}\Bigr{â€–}â‚‚Â² %%
\\  A^âŠ¤Î½ &= ÏƒÏˆ &âŸ¹ z &= \argmin_z \Bigl{â€–} \bmat{A \\ ÏƒÂ²ğ•€â‚™} z - \bmat{-Î½\\ğŸâ‚™}\Bigr{â€–}â‚‚Â² %%
\end{alignat*}%
%
We can optimize further by performing a simultaneous solve:
%
\begin{align*}%
	\bmat{xï¼Œy} &= \argmin_{x,y} \Bigl{â€–} \bmat{A^âŠ¤ \\ ÏƒÂ²ğ•€â‚˜} \bmat{xï¼Œy} - \bmat{-Î¼ & -Ïˆ \\ ğŸâ‚˜ & ğŸâ‚˜}\Bigr{â€–}â‚‚Â²%
& Î¼ &= \argmin_Î¼ â€–AÎ¼ - ÏƒÏ•â€–â‚‚Â²
\\  \bmat{wï¼Œz} &= \argmin_{w,z} \Bigl{â€–} \bmat{A   \\ ÏƒÂ²ğ•€â‚™} \bmat{wï¼Œz} - \bmat{-Ï• &-Î½ \\ ğŸâ‚™ & ğŸâ‚™}\Bigr{â€–}â‚‚Â²%
& Î½ &= \argmin_Î½ â€–A^âŠ¤Î½ - ÏƒÏˆâ€–â‚‚Â²
\end{align*}%
%
\subsection{Via Backward Substitution}

We need to introduce an additional modification:

If $AÎ¼ = ÏƒÏ•$ not solveable, we instead can multiply the equation by $A^âŠ¤$ to obtain:
%
\begin{align*}%
	(ÏƒÂ²ğ•€â‚˜ - AA^âŠ¤)x &= ÏƒÏ•  &&âŸ¹&  (ÏƒÂ²ğ•€â‚™ - A^âŠ¤A)Î¼ &= ÏƒA^âŠ¤Ï•  & A^âŠ¤x &= Î¼%
\\  (ÏƒÂ²ğ•€â‚™ - A^âŠ¤A)z &= ÏƒÏˆ  &&âŸ¹&  (ÏƒÂ²ğ•€â‚™ - A^âŠ¤A)AÎ½ &= ÏƒAÏˆ    & Az &= Î½%
\end{align*}%
%
So:
%
\begin{align*}%
	Î¼ &= \argmin_Î¼ \Bigl{â€–} \bmat{A \\ ÏƒÂ²ğ•€â‚™} Î¼ - \bmat{-ÏƒÏ•\\ğŸâ‚™}\Bigr{â€–}â‚‚Â²  & A^âŠ¤x &= Î¼%
\\  Î½ &= \argmin_Î¼ \Bigl{â€–} \bmat{A^âŠ¤ \\ ÏƒÂ²ğ•€â‚˜} Î½ - \bmat{-ÏƒÏˆ\\ğŸâ‚˜}\Bigr{â€–}â‚‚Â²  & Az &= Î½%
\end{align*}%
%
So
%
\begin{align*}%
	\bmat{Î¼ & w} &= \bmat{A \\ ÏƒÂ²ğ•€â‚™} \bmat{-ÏƒÏ• & -Ï•\\ğŸâ‚™ & ğŸâ‚™} %
\\  \bmat{y & Î½}   &= \bmat{A^âŠ¤ \\ ÏƒÂ²ğ•€â‚˜} \bmat{-Ïˆ & -ÏƒÏˆ\\ğŸâ‚˜ & ğŸâ‚˜} %
\end{align*}%
%





%
In principle, one could try to rephrase these as smaller problems, but for now, it's better to just stick to the bigger system.
%
We can use the \textbf{push-through identity} to convert these into 4 linear systems:
%
\begin{align*}%
	Px &= Ï•             & Py &= \tilde{A}Ïˆ%
\\  Qz &= \tilde{A}^âŠ¤Ï•  & Qw &= Ïˆ
\end{align*}%
%
Then $\tilde{Ï•} = x+y$ and $\tilde{Ïˆ} = z+w$, and the VJP are given by the previous equations:
%
\begin{align*}%
	Î¾^âŠ¤\frac{âˆ‚Ïƒ}{âˆ‚A} &= Î¾uv^âŠ¤
\\  Ï•^âŠ¤\frac{âˆ‚u}{âˆ‚A} &= (ğ•€â‚˜ - uu^âŠ¤)\tilde{Ï•}v^âŠ¤ = (\tilde{Ï•} - (u^âŠ¤\tilde{Ï•})u)v^âŠ¤%
\\  Ïˆ^âŠ¤\frac{âˆ‚v}{âˆ‚A} &= u\tilde{Ïˆ}^âŠ¤(ğ•€â‚™ - vv^âŠ¤) = u(\tilde{Ïˆ} - (v^âŠ¤\tilde{Ïˆ})v)^âŠ¤%
\end{align*}%
%
\section{Spectral Normalization}
The VJP of spectral normalization can be computed as follows: let $g(A) = â€–Aâ€–â‚‚$ and $V$ be the vector in the VJP. then
%
\begin{align*}%
âˆ‡_AâŸ¨V âˆ£ \frac{A}{â€–Aâ€–â‚‚}âŸ©
	&= âŸ¨V | \frac{A+âˆ†A}{g(A+âˆ†A)} - \frac{A}{g(A)}âŸ©
\\  &= âŸ¨V | \frac{A+âˆ†A}{g(A)+âˆ‡g(A)âˆ†A} - \frac{A}{g(A)}âŸ©
\\  &= âŸ¨V | \frac{(A+âˆ†A)(g(A)-âˆ‡g(A)âˆ†A)}{(g(A)+âˆ‡g(A)âˆ†A)(g(A) - âˆ‡g(A)âˆ†A)} - \frac{A}{g(A)}âŸ©
\\  &= âŸ¨V | \frac{ âˆ†Ag(A)- Aâˆ‡g(A)âˆ†A}{g(A)Â²}âŸ©
\\  &= âŸ¨\tfrac{1}{g(A)}V - \tfrac{âŸ¨Vâˆ£AâŸ©}{g(A)}âˆ‡g(A) | âˆ†AâŸ©
\end{align*}%
%
%
\begin{align*}%
g(A)=1 âŸ¹ âˆ‡_AâŸ¨V âˆ£ \frac{A}{â€–Aâ€–â‚‚}âŸ© = âŸ¨V - âŸ¨Vâˆ£AâŸ©âˆ‡g(A)âˆ£âˆ†AâŸ©%
\end{align*}%
%

\section{Projected gradient}

When using spectral normalization we want to do the following:

%
\begin{alignat*}{3}%
   &\text{update:}\quad& A' &= A - âˆ‡_A ğ“›(\frac{A}{â€–Aâ€–â‚‚})%
\\ &\text{project:}\quad& A &= \frac{A'}{â€–A'â€–â‚‚}
\end{alignat*}%
%
Moreover, we want:
%
\begin{outline}%
%\renewcommand{\outlineii}{enumerate}
\1 During forward, compute $\frac{A}{â€–Aâ€–â‚‚}$ only once and then reuse this node.
\1 Compute $â€–Aâ€–â‚‚$ effectively between gradient updates.
	\2 Avoid built-in torch algos, as they make use of full SVD algos.
\1 After gradient update, perform projection step. (maybe unecessary)
\end{outline}%
%
NOTE: gradients are different if we include normalization!

\end{document}
